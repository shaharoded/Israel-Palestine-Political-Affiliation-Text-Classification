{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi5VNBSOv-kT"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UD9uGUUSvuxq"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEqukEMawVMb"
      },
      "source": [
        "# Load dataset and preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjn5Fg5Mwat2"
      },
      "source": [
        "For test the code, I used the iris dataset. Replace with our data when it is ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kO7c6Y5FwU1d"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75rhsfaiwuOx"
      },
      "source": [
        "# Define optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7ekXAYOkw0w_"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Optimization\n",
        "def objective_logreg(trial):\n",
        "    \"\"\"Logistic Regression Optimization\"\"\"\n",
        "    # Define l1_ration if penalty is 'elasticnet'.\n",
        "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2', None, 'elasticnet'])\n",
        "    if penalty == 'elasticnet':\n",
        "        l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0, step=0.25)\n",
        "    else:\n",
        "        l1_ratio = None\n",
        "\n",
        "    param = {\n",
        "        'C': trial.suggest_categorical('C', [0.5, 1.0, 1.5]),\n",
        "        'penalty': penalty,\n",
        "        'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
        "        'l1_ratio': l1_ratio\n",
        "    }\n",
        "\n",
        "    clf = LogisticRegression(**param, solver='saga', random_state=42)\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', clf)\n",
        "    ])\n",
        "    score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Dvg-ZLIiyunH"
      },
      "outputs": [],
      "source": [
        "# SVM Optimization\n",
        "def objective_svm(trial):\n",
        "    param = {\n",
        "        'C': trial.suggest_categorical('C', [0.5, 1.0, 1.5]),\n",
        "        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
        "        'degree': trial.suggest_int('degree', 2, 5, step=1),\n",
        "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "    }\n",
        "    clf = SVC(**param, random_state=42)\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', clf)\n",
        "    ])\n",
        "    score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ze5Iu-hQyvkX"
      },
      "outputs": [],
      "source": [
        "# XGBoost Optimization\n",
        "def objective_xgb(trial):\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_categorical('n_estimators', [5, 10, 25, 50, 100]),\n",
        "        'learning_rate': trial.suggest_categorical('learning_rate', [0.001, 0.01, 0.1, 1.0]),\n",
        "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart'])\n",
        "    }\n",
        "\n",
        "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
        "        # maximum depth of the tree, signifies complexity of the tree.\n",
        "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
        "        # minimum child weight, larger the term more conservative the tree.\n",
        "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
        "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
        "        # defines how selective algorithm is.\n",
        "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
        "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "\n",
        "    if param[\"booster\"] == \"dart\":\n",
        "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
        "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
        "\n",
        "    clf = XGBClassifier(**param, use_label_encoder=False, objective='binary:logistic', random_state=42)\n",
        "    clf.fit(X_train, y_train, verbose=False)\n",
        "    preds = clf.predict(X_test)\n",
        "    preds_labels = np.rint(preds)\n",
        "    score = accuracy_score(y_test, preds_labels)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKkf4G7y2cK"
      },
      "source": [
        "# Run trails and find optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j6Ht5JJzA6u"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKDPafM2y8YG",
        "outputId": "c7b85b20-6e2a-48ac-ecf8-f15f0c0584ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-11 20:22:57,681] A new study created in memory with name: no-name-82f9cdec-02a8-4701-a2a9-cf9652334acd\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:57,776] Trial 0 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 1.5, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:57,874] Trial 1 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 0.5, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:57,967] Trial 2 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 1.5, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,059] Trial 3 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 1.0, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,166] Trial 4 finished with value: 0.9736263736263737 and parameters: {'penalty': 'elasticnet', 'l1_ratio': 0.5, 'C': 1.0, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,285] Trial 5 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 1.5, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,412] Trial 6 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 1.0, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,535] Trial 7 finished with value: 0.9736263736263737 and parameters: {'penalty': 'elasticnet', 'l1_ratio': 0.0, 'C': 1.5, 'max_iter': 50}. Best is trial 0 with value: 0.9736263736263737.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,635] Trial 8 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,746] Trial 9 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 1.0, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,859] Trial 10 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:58,967] Trial 11 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,079] Trial 12 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,192] Trial 13 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,316] Trial 14 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,453] Trial 15 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,594] Trial 16 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,721] Trial 17 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,828] Trial 18 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:22:59,937] Trial 19 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,057] Trial 20 finished with value: 0.9736263736263737 and parameters: {'penalty': 'elasticnet', 'l1_ratio': 1.0, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,171] Trial 21 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,280] Trial 22 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,434] Trial 23 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,558] Trial 24 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,670] Trial 25 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,782] Trial 26 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:00,904] Trial 27 finished with value: 0.9736263736263737 and parameters: {'penalty': 'elasticnet', 'l1_ratio': 1.0, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,010] Trial 28 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 1.0, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,119] Trial 29 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,233] Trial 30 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,364] Trial 31 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,503] Trial 32 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,643] Trial 33 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,768] Trial 34 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:01,875] Trial 35 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,001] Trial 36 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,104] Trial 37 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.0, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,224] Trial 38 finished with value: 0.9736263736263737 and parameters: {'penalty': 'elasticnet', 'l1_ratio': 0.0, 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,351] Trial 39 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 1.0, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,503] Trial 40 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l2', 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,628] Trial 41 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,739] Trial 42 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,855] Trial 43 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:02,963] Trial 44 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:03,079] Trial 45 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:03,204] Trial 46 finished with value: 0.9736263736263737 and parameters: {'penalty': 'l1', 'C': 1.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:03,328] Trial 47 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:03,478] Trial 48 finished with value: 0.9758241758241759 and parameters: {'penalty': None, 'C': 1.0, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n",
            "<ipython-input-38-691f3761cd92>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  'max_iter': trial.suggest_int('max_iter', 50, 100, 150),\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [50, 100] and step=150, but the range is not divisible by `step`. It will be replaced by [50, 50].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-01-11 20:23:03,643] Trial 49 finished with value: 0.9736263736263737 and parameters: {'penalty': 'elasticnet', 'l1_ratio': 0.5, 'C': 0.5, 'max_iter': 50}. Best is trial 8 with value: 0.9758241758241759.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression: {'penalty': None, 'C': 1.5, 'max_iter': 50}\n"
          ]
        }
      ],
      "source": [
        "study_logreg = optuna.create_study(direction='maximize')\n",
        "study_logreg.optimize(objective_logreg, n_trials=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOo2fE08CTHT",
        "outputId": "5da70e59-0223-40f2-fe55-abc7b0a75b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression: {'penalty': None, 'C': 1.5, 'max_iter': 50}\n"
          ]
        }
      ],
      "source": [
        "print(\"Best parameters for Logistic Regression:\", study_logreg.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrPApzDzEiC"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5oqLnmazGeQ",
        "outputId": "2067edf2-0231-4b3d-93c6-d4d8e29a8cd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-11 20:27:45,020] A new study created in memory with name: no-name-259e2b59-f434-474a-81e1-1873ee4e2535\n",
            "[I 2025-01-11 20:27:45,069] Trial 0 finished with value: 0.9736263736263737 and parameters: {'C': 1.0, 'kernel': 'rbf', 'degree': 5, 'gamma': 'auto'}. Best is trial 0 with value: 0.9736263736263737.\n",
            "[I 2025-01-11 20:27:45,113] Trial 1 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,154] Trial 2 finished with value: 0.9692307692307693 and parameters: {'C': 1.5, 'kernel': 'linear', 'degree': 2, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,199] Trial 3 finished with value: 0.9736263736263737 and parameters: {'C': 1.0, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,247] Trial 4 finished with value: 0.9692307692307693 and parameters: {'C': 0.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,286] Trial 5 finished with value: 0.9714285714285715 and parameters: {'C': 1.0, 'kernel': 'linear', 'degree': 3, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,335] Trial 6 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,388] Trial 7 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,433] Trial 8 finished with value: 0.9494505494505494 and parameters: {'C': 1.5, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,491] Trial 9 finished with value: 0.8043956043956044 and parameters: {'C': 1.0, 'kernel': 'poly', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,547] Trial 10 finished with value: 0.9626373626373628 and parameters: {'C': 0.5, 'kernel': 'sigmoid', 'degree': 5, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,599] Trial 11 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,656] Trial 12 finished with value: 0.9098901098901099 and parameters: {'C': 1.5, 'kernel': 'poly', 'degree': 3, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,710] Trial 13 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,762] Trial 14 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,827] Trial 15 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 5, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,877] Trial 16 finished with value: 0.9758241758241759 and parameters: {'C': 0.5, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,933] Trial 17 finished with value: 0.9098901098901099 and parameters: {'C': 1.5, 'kernel': 'poly', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:45,986] Trial 18 finished with value: 0.9494505494505494 and parameters: {'C': 1.5, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,042] Trial 19 finished with value: 0.9692307692307693 and parameters: {'C': 0.5, 'kernel': 'rbf', 'degree': 5, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,096] Trial 20 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,150] Trial 21 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,202] Trial 22 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,259] Trial 23 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,311] Trial 24 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,377] Trial 25 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,434] Trial 26 finished with value: 0.9692307692307693 and parameters: {'C': 1.5, 'kernel': 'linear', 'degree': 5, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,487] Trial 27 finished with value: 0.9494505494505494 and parameters: {'C': 1.5, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,556] Trial 28 finished with value: 0.8043956043956044 and parameters: {'C': 1.0, 'kernel': 'poly', 'degree': 4, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,612] Trial 29 finished with value: 0.9692307692307693 and parameters: {'C': 0.5, 'kernel': 'rbf', 'degree': 5, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,668] Trial 30 finished with value: 0.9736263736263737 and parameters: {'C': 1.0, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,723] Trial 31 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,776] Trial 32 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,842] Trial 33 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,900] Trial 34 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:46,954] Trial 35 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,007] Trial 36 finished with value: 0.9714285714285715 and parameters: {'C': 1.0, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,061] Trial 37 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,120] Trial 38 finished with value: 0.9626373626373628 and parameters: {'C': 0.5, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,176] Trial 39 finished with value: 0.9098901098901099 and parameters: {'C': 1.5, 'kernel': 'poly', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,225] Trial 40 finished with value: 0.9714285714285715 and parameters: {'C': 1.0, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,279] Trial 41 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,337] Trial 42 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,406] Trial 43 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,461] Trial 44 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,518] Trial 45 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,572] Trial 46 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,650] Trial 47 finished with value: 0.9626373626373628 and parameters: {'C': 0.5, 'kernel': 'sigmoid', 'degree': 2, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,762] Trial 48 finished with value: 0.8241758241758242 and parameters: {'C': 1.5, 'kernel': 'poly', 'degree': 4, 'gamma': 'scale'}. Best is trial 1 with value: 0.9758241758241759.\n",
            "[I 2025-01-11 20:27:47,863] Trial 49 finished with value: 0.9758241758241759 and parameters: {'C': 1.5, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 1 with value: 0.9758241758241759.\n"
          ]
        }
      ],
      "source": [
        "study_svm = optuna.create_study(direction='maximize')\n",
        "study_svm.optimize(objective_svm, n_trials=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Liydo6kZDUld",
        "outputId": "bc8e5720-fb05-408d-8e04-f3f65a241128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for SVM: {'C': 1.5, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Best parameters for SVM:\", study_svm.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFLuV6uLzM-9"
      },
      "source": [
        "## XGboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1TYrkZjzP5H",
        "outputId": "dbfd1f4b-225d-4c8f-cafc-a624c850e02b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-11 20:37:08,230] A new study created in memory with name: no-name-da828f77-59c8-4fff-8a6a-fb26401cb2d1\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:10,613] Trial 0 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 10, 'eta': 4.207008942642503e-08, 'gamma': 0.0011319596186514975, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.9193282672923407e-07, 'skip_drop': 1.3001259631241898e-06}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:10,851] Trial 1 finished with value: 0.956140350877193 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 8, 'eta': 0.0015297363455298346, 'gamma': 3.818152506548063e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 5.93177747666441e-08, 'skip_drop': 2.1585541797247382e-07}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:10,931] Trial 2 finished with value: 0.6228070175438597 and parameters: {'n_estimators': 10, 'learning_rate': 0.001, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 7, 'eta': 8.836789219574613e-06, 'gamma': 2.558992535526232e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 4.491499184014054e-05, 'skip_drop': 8.74759291055332e-05}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:11,788] Trial 3 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.01656138527620358, 'gamma': 0.018812914815776408, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:11,816] Trial 4 finished with value: 0.8157894736842105 and parameters: {'n_estimators': 100, 'learning_rate': 0.001, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:11,837] Trial 5 finished with value: 0.9298245614035088 and parameters: {'n_estimators': 5, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.0011042538600923174, 'gamma': 0.19747574205852203, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:11,860] Trial 6 finished with value: 0.6228070175438597 and parameters: {'n_estimators': 5, 'learning_rate': 0.01, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.0001109908220208511, 'gamma': 8.228697102733568e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 8.731198216826599e-07, 'skip_drop': 0.001258292033462347}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:11,886] Trial 7 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.5358406682944129, 'gamma': 1.5742822669199062e-05, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:11,911] Trial 8 finished with value: 0.9298245614035088 and parameters: {'n_estimators': 50, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 10, 'eta': 4.706167851019924e-07, 'gamma': 2.5816141774079894e-05, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:12,008] Trial 9 finished with value: 0.8859649122807017 and parameters: {'n_estimators': 25, 'learning_rate': 0.01, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 10, 'eta': 1.8485827292792296e-05, 'gamma': 0.09920931371334994, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 3.2631861811559156e-07, 'skip_drop': 3.537564810874624e-08}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:12,041] Trial 10 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:12,148] Trial 11 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 3, 'min_child_weight': 3, 'eta': 1.581782121647566e-08, 'gamma': 0.003314870215341171, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:13,033] Trial 12 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.30159873386223285, 'gamma': 0.0028578123724616374, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.8818756704052261, 'skip_drop': 0.6589846134083969}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:13,130] Trial 13 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.021591059975452343, 'gamma': 0.0017143641742066002, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:13,147] Trial 14 finished with value: 0.9473684210526315 and parameters: {'n_estimators': 10, 'learning_rate': 0.1, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:13,465] Trial 15 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 50, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 10, 'eta': 1.4802014192671844e-08, 'gamma': 0.01838729838960446, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.001132920403479312, 'skip_drop': 4.9952756474694575e-06}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:13,592] Trial 16 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 2, 'eta': 1.1350015758455968e-06, 'gamma': 0.0002958103205463961, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:14,674] Trial 17 finished with value: 0.6228070175438597 and parameters: {'n_estimators': 100, 'learning_rate': 0.001, 'booster': 'dart', 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.014784879124585695, 'gamma': 0.43942055950589987, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.739913289531303e-05, 'skip_drop': 0.0030944964478917063}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:14,829] Trial 18 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 8, 'eta': 1.554088333930391e-07, 'gamma': 3.1399562881610206e-07, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:14,844] Trial 19 finished with value: 0.9298245614035088 and parameters: {'n_estimators': 5, 'learning_rate': 0.1, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,186] Trial 20 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 50, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.0007919522083688838, 'gamma': 0.00022273876702220798, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.284491592681116e-08, 'skip_drop': 4.7343168834384896e-06}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,235] Trial 21 finished with value: 0.956140350877193 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.17613005037082458, 'gamma': 0.024227636648116172, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,282] Trial 22 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.7507948350570055, 'gamma': 9.025094904176309e-05, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,342] Trial 23 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 7, 'eta': 0.02038425105542684, 'gamma': 1.2359591862941039e-06, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,394] Trial 24 finished with value: 0.956140350877193 and parameters: {'n_estimators': 10, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.07747623867592249, 'gamma': 0.000720716619794898, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,442] Trial 25 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 6, 'eta': 0.005473997948044477, 'gamma': 6.320330155445836e-05, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,583] Trial 26 finished with value: 0.6228070175438597 and parameters: {'n_estimators': 100, 'learning_rate': 0.001, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 10, 'eta': 0.8460303814245391, 'gamma': 0.01658157629784181, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,728] Trial 27 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 8, 'eta': 0.00010425504501036598, 'gamma': 1.8134717262553156e-07, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,772] Trial 28 finished with value: 0.9473684210526315 and parameters: {'n_estimators': 25, 'learning_rate': 0.1, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:15,899] Trial 29 finished with value: 0.956140350877193 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 5, 'eta': 0.05585952701373424, 'gamma': 1.9015266485350744e-05, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.006160702907253007, 'skip_drop': 1.3318614068906199e-06}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:16,193] Trial 30 finished with value: 0.956140350877193 and parameters: {'n_estimators': 50, 'learning_rate': 1.0, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 9, 'eta': 6.832672100931664e-06, 'gamma': 0.0061610283559093626, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 3.3328911159168966e-06, 'skip_drop': 1.2944981731988878e-08}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:17,075] Trial 31 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.2762814708329606, 'gamma': 0.0008408278732979296, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.39778440622963596, 'skip_drop': 0.3529760704213937}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:18,094] Trial 32 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.1941491561353955, 'gamma': 0.0677551002434396, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.6898970046907776, 'skip_drop': 0.8217585404752731}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:19,218] Trial 33 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.00434390004481629, 'gamma': 0.005607118308162141, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.02476855295191855, 'skip_drop': 0.06071667838042718}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:19,294] Trial 34 finished with value: 0.6228070175438597 and parameters: {'n_estimators': 10, 'learning_rate': 0.001, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 6, 'eta': 0.7424424228207781, 'gamma': 0.00029823136379233305, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.00037867355169723515, 'skip_drop': 8.424669007501855e-05}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:19,361] Trial 35 finished with value: 0.9298245614035088 and parameters: {'n_estimators': 5, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 8, 'eta': 0.0004966020735242884, 'gamma': 3.4376658294632522e-06, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.025888068044170234, 'skip_drop': 0.007002350584004409}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:20,443] Trial 36 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.05587529215808268, 'gamma': 0.8332588676518715, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 7.073857914105643e-06, 'skip_drop': 2.6316569647748358e-05}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:20,585] Trial 37 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.00425597753798414, 'gamma': 0.0016468486168173266, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:20,600] Trial 38 finished with value: 0.9298245614035088 and parameters: {'n_estimators': 5, 'learning_rate': 0.1, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:21,750] Trial 39 finished with value: 0.6228070175438597 and parameters: {'n_estimators': 100, 'learning_rate': 0.001, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.22434601604894433, 'gamma': 1.3158285902694683e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.4302472347378912e-07, 'skip_drop': 0.0009481715739402791}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:21,896] Trial 40 finished with value: 0.9385964912280702 and parameters: {'n_estimators': 10, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 10, 'eta': 1.576226960056379e-05, 'gamma': 0.03615943103000588, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:22,143] Trial 41 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.01762033377556181, 'gamma': 0.001366535179668973, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:22,638] Trial 42 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 6, 'eta': 0.03977706921454278, 'gamma': 0.0033513528368642135, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:23,264] Trial 43 finished with value: 0.956140350877193 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.00028592483813007647, 'gamma': 0.01073823672515993, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:23,553] Trial 44 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.002046314238740601, 'gamma': 0.13700769081097838, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:23,773] Trial 45 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 9, 'eta': 0.11995832830540104, 'gamma': 2.0056693769481597e-05, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:23,830] Trial 46 finished with value: 0.9473684210526315 and parameters: {'n_estimators': 50, 'learning_rate': 0.1, 'booster': 'gblinear'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:25,695] Trial 47 finished with value: 0.9473684210526315 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 3, 'eta': 3.78675216506424e-08, 'gamma': 0.00048798285316025954, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.4663086325524232e-08, 'skip_drop': 2.5952361198881356e-07}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:25,751] Trial 48 finished with value: 0.9298245614035088 and parameters: {'n_estimators': 25, 'learning_rate': 1.0, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 10, 'eta': 2.658947534509829e-06, 'gamma': 0.0021166438579204717, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9736842105263158.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:37:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-01-11 20:37:26,782] Trial 49 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.40751314061973337, 'gamma': 0.00013743921319243912, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0003170525913587908, 'skip_drop': 0.02969353540915604}. Best is trial 0 with value: 0.9736842105263158.\n"
          ]
        }
      ],
      "source": [
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N-rMKH4FhBK",
        "outputId": "61387ca5-19d7-4834-df7a-54c69e8a7027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBoost: {'n_estimators': 100, 'learning_rate': 0.1, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 10, 'eta': 4.207008942642503e-08, 'gamma': 0.0011319596186514975, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.9193282672923407e-07, 'skip_drop': 1.3001259631241898e-06}\n"
          ]
        }
      ],
      "source": [
        "print(\"Best parameters for XGBoost:\", study_xgb.best_params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
