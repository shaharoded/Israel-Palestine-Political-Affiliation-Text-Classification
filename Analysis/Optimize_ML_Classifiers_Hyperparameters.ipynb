{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5VNBSOv-kT"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UD9uGUUSvuxq"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "from classifiers import Classifier, DNN\n",
    "from dataset import TextDataset, get_dataloader\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEqukEMawVMb"
   },
   "source": [
    "# Load dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe ="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75rhsfaiwuOx"
   },
   "source": [
    "# Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_hyperparameters = {   # TODO: Add neural network hyperparameters\n",
    "    'logistic_regression': {\n",
    "        'C': (1e-4, 1e2, 'loguniform'),\n",
    "        'max_iter': ([50, 200], 'uniform'),\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': (1e-4, 1e2, 'loguniform'),\n",
    "        'kernel': (['linear', 'poly', 'rbf', 'sigmoid'], 'categorical'),\n",
    "        'degree': (2, 5, 'int'),\n",
    "        'gamma': (['scale', 'auto'], 'categorical')\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': ([5, 100], 'int'),\n",
    "        'learning_rate': ([1e-3, 1.0], 'loguniform'),\n",
    "        'booster': (['gbtree', 'gblinear', 'dart'], 'categorical'),\n",
    "    },\n",
    "    'dnn': {\n",
    "        \"num_epochs\": ([10, 50], 'uniform'),  # Adjust after trial and error\n",
    "        \"learning_rate\": ([1e-3, 0.1], 'loguniform'),\n",
    "        \"batch_norm\": ([True, False], 'categorical'),\n",
    "        \"drop_out\": ([0.0, 1.0], 'uniform'),\n",
    "        \"layers\": [768, 128, 64, 3]  # Layer dimentions, including an input and an output layer.\n",
    "    }\n",
    "}\n",
    "\n",
    "def suggest_hyperparameters(trial, hyperparams):\n",
    "    params = {}\n",
    "    for key, value in hyperparams.items():\n",
    "        if len(value) == 2 and value[1] == 'categorical':\n",
    "            params[key] = trial.suggest_categorical(key, value[0])\n",
    "        elif len(value) == 3:\n",
    "            if value[2] == 'loguniform':\n",
    "                params[key] = trial.suggest_float(key, value[0], value[1], log=True)\n",
    "            elif value[2] == 'uniform':\n",
    "                params[key] = trial.suggest_float(key, value[0], value[1])\n",
    "            elif value[2] == 'int':\n",
    "                params[key] = trial.suggest_int(key, value[0], value[1])\n",
    "            elif value[2] == 'categorical':\n",
    "                params[key] = trial.suggest_categorical(key, value[0])\n",
    "            elif value[1] == 'custom':\n",
    "                hidden_dims = params['hidden_dims']\n",
    "                layer_count = len(hidden_dims)\n",
    "                params[key] = trial.suggest_categorical(key, value[0][layer_count])\n",
    "            else:\n",
    "                raise ValueError(f\"Hyperparameter tuple for {key} is not in the expected format: {value}\")\n",
    "    return params\n",
    "\n",
    "# Define objective function for optuna. The function include all models, and should be called with the model name. The function optimize the Classifier class hyperparameters.\n",
    "def objective(trial, model_name, data, folds_scores):\n",
    "    params = suggest_hyperparameters(trial, model_hyperparameters[model_name])\n",
    "\n",
    "    # Add some more parameters for Logistic Regression\n",
    "    if model_name == 'Logistic Regression':\n",
    "        penalty = trial.suggest_categorical('penalty', ['l1', 'l2', None, 'elasticnet'])\n",
    "        if penalty == 'elasticnet':\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0, step=0.25)\n",
    "        else:\n",
    "            l1_ratio = None\n",
    "        params['penalty'] = penalty\n",
    "        params['l1_ratio'] = l1_ratio\n",
    "\n",
    "    # Add some more parameters for XGBoost\n",
    "    elif model_name == 'xgboost':\n",
    "        if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            # defines how selective algorithm is.\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "\n",
    "    model = Classifier(**params, model_type=model_name)\n",
    "    model.fit(data)\n",
    "\n",
    "    # Create a pipeline with just the classifier since feature prep is external\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation and return the mean F1 score\n",
    "    scores = cross_val_score(pipeline, data, cv=cv, scoring='accuracy')\n",
    "    folds_scores.append(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def optimize_model(model_name, data, n_trials=50, timout=1200):\n",
    "    \"\"\"\n",
    "    The actual optimization.\n",
    "    \"\"\"\n",
    "    folds_scores = []   # create a list to store the scores from each trial folds\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, model_name, data, folds_scores), n_trials=n_trials, timeout=timout)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "\n",
    "    print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best F1 score for {model_name}: {best_value}\")\n",
    "\n",
    "    return best_params, best_value, folds_scores\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimize models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7ekXAYOkw0w_"
   },
   "outputs": [],
   "source": [
    "# Create 4 different datasets: embedding with and without augmentation, and tf-idf with and without augmentation.\n",
    "embedder = Embedder()\n",
    "data_without_augmentation = TextDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO\n",
    ")\n",
    "\n",
    "data_with_augmentation = TextDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=AUGMENTATION_RATIO,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO\n",
    ")\n",
    "\n",
    "bert_embedding_no_augmentation_loader = get_dataloader(\n",
    "    dataset=data_without_augmentation,\n",
    "    embedder=embedder,\n",
    "    datashape='embedding',\n",
    "    embedding_method=EMBEDDING_METHOD,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "bert_embedding_with_augmentation_loader = get_dataloader(\n",
    "    dataset=data_with_augmentation,\n",
    "    embedder=embedder,\n",
    "    datashape='embedding',\n",
    "    embedding_method=EMBEDDING_METHOD,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "tfidf_embedding_no_augmentation_loader = get_dataloader(\n",
    "    dataset=data_without_augmentation,\n",
    "    embedder=embedder,\n",
    "    datashape='embedding',\n",
    "    embedding_method='tf-idf',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "tfidf_embedding_with_augmentation_loader = get_dataloader(\n",
    "    dataset=data_with_augmentation,\n",
    "    embedder=embedder,\n",
    "    datashape='embedding',\n",
    "    embedding_method='tf-idf',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "lr_results['bert_without_augmentation'] = optimize_model('logistic_regression', bert_embedding_no_augmentation_loader)\n",
    "lr_results['bert_with_augmentation'] = optimize_model('logistic_regression', bert_embedding_with_augmentation_loader)\n",
    "lr_results['tfidf_without_augmentation'] = optimize_model('logistic_regression', tfidf_embedding_no_augmentation_loader)\n",
    "lr_results['tfidf_with_augmentation'] = optimize_model('logistic_regression', tfidf_embedding_with_augmentation_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Logistic Regression results:\\n\")\n",
    "print(lr_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Dvg-ZLIiyunH"
   },
   "outputs": [],
   "source": [
    "svm_results = {}\n",
    "svm_results['bert_without_augmentation'] = optimize_model('svm', bert_embedding_no_augmentation_loader)\n",
    "svm_results['bert_with_augmentation'] = optimize_model('svm', bert_embedding_with_augmentation_loader)\n",
    "svm_results['tfidf_without_augmentation'] = optimize_model('svm', tfidf_embedding_no_augmentation_loader)\n",
    "svm_results['tfidf_with_augmentation'] = optimize_model('svm', tfidf_embedding_with_augmentation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ze5Iu-hQyvkX"
   },
   "outputs": [],
   "source": [
    "xgb_results = {}\n",
    "xgb_results['bert_without_augmentation'] = optimize_model('xgboost', bert_embedding_no_augmentation_loader)\n",
    "xgb_results['bert_with_augmentation'] = optimize_model('xgboost', bert_embedding_with_augmentation_loader)\n",
    "xgb_results['tfidf_without_augmentation'] = optimize_model('xgboost', tfidf_embedding_no_augmentation_loader)\n",
    "xgb_results['tfidf_with_augmentation'] = optimize_model('xgboost', tfidf_embedding_with_augmentation_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
