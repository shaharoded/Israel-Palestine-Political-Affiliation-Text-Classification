{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5VNBSOv-kT"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UD9uGUUSvuxq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\amita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\amita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from classifiers import *\n",
    "from dataset import EmbeddingDataset, get_dataloader\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75rhsfaiwuOx"
   },
   "source": [
    "# Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Helper dataset\n",
    "class HelperDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(HelperDataset).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.y[item]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Custom tqdm callback\n",
    "class TqdmCallback:\n",
    "    def __init__(self, n_trials):\n",
    "        self.pbar = tqdm(total=n_trials)\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model_hyperparameters = {   # TODO: Add neural network hyperparameters\n",
    "    'logistic_regression': {\n",
    "        'learning_rate': (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform')\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': (1e-4, 1e2, 'loguniform'),\n",
    "        'kernel': (['linear', 'poly', 'rbf', 'sigmoid'], 'categorical'),\n",
    "        'degree': (2, 5, 'int'),\n",
    "        'gamma': (['scale', 'auto'], 'categorical')\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': ([5, 100], 'int'),\n",
    "        'learning_rate': ([1e-3, 1.0], 'loguniform'),\n",
    "        'booster': (['gbtree', 'gblinear', 'dart'], 'categorical')\n",
    "    },\n",
    "    'dnn': {\n",
    "        \"num_epochs\": ([2, 15], 'int'),  # Adjust after trial and error\n",
    "        \"learning_rate\": ([1e-5, 1e-3], 'loguniform'),\n",
    "        \"batch_norm\": ([True, False], 'categorical'),\n",
    "        \"drop_out\": ([0.0, 0.5], 'uniform'),\n",
    "        \"layers\": ([[768, 64, 3],\n",
    "                    [768, 128, 3],\n",
    "                    [768, 64, 64, 3],\n",
    "                    [768, 128, 64, 3],\n",
    "                    [768, 512, 32, 3],\n",
    "                    [768, 512, 128, 3],\n",
    "                    [768, 512, 128, 64, 3]], 'custom')  # Layer dimensions, including an input and output layer.\n",
    "    }\n",
    "}\n",
    "\n",
    "def suggest_hyperparameters(trial, hyperparams):\n",
    "    params = {}\n",
    "    for key, value in hyperparams.items():\n",
    "        if len(value) == 2 and value[1] == 'categorical':\n",
    "            params[key] = trial.suggest_categorical(key, value[0])\n",
    "        elif len(value) == 3:\n",
    "            if value[2] == 'loguniform':\n",
    "                params[key] = trial.suggest_float(key, value[0], value[1], log=True)\n",
    "            elif value[2] == 'uniform':\n",
    "                params[key] = trial.suggest_float(key, value[0], value[1])\n",
    "            elif value[2] == 'int':\n",
    "                params[key] = trial.suggest_int(key, value[0], value[1])\n",
    "            elif value[2] == 'categorical':\n",
    "                params[key] = trial.suggest_categorical(key, value[0])\n",
    "            elif value[1] == 'custom':\n",
    "                hidden_dims = params['hidden_dims']\n",
    "                layer_count = len(hidden_dims)\n",
    "                params[key] = trial.suggest_categorical(key, value[0][layer_count])\n",
    "            else:\n",
    "                raise ValueError(f\"Hyperparameter tuple for {key} is not in the expected format: {value}\")\n",
    "    return params\n",
    "\n",
    "def cross_validation(estimator, X, y, n_splits=10):\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for i, (train_index, val_index) in enumerate(cv.split(X, y)):\n",
    "        # Split to train and validation sets\n",
    "        x_train, x_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        help_train_dataset = HelperDataset(x_train, y_train)\n",
    "        help_val_dataset = HelperDataset(x_val, y_val)\n",
    "\n",
    "        train_dataloader = DataLoader(help_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_dataloader = DataLoader(help_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # Fit to the Classifier train and predict data type\n",
    "        train = (train_dataloader, (x_train, y_train))\n",
    "        val = (val_dataloader, (x_val, y_val))\n",
    "\n",
    "        estimator.fit(train)\n",
    "        pred = estimator.predict(val)\n",
    "        score = f1_score(y_val, pred, average='micro')\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Define objective function for optuna. The function include all models, and should be called with the model name. The function optimize the Classifier class hyperparameters.\n",
    "def objective(trial, model_name, X, y, folds_scores):\n",
    "    params = suggest_hyperparameters(trial, model_hyperparameters[model_name])\n",
    "\n",
    "    if model_name == 'logistic_regression':\n",
    "        params['num_epochs'] = 1\n",
    "        params['batch_norm'] = False\n",
    "        params['drop_out'] = 0.0\n",
    "        params['layers'] = [768, 3]\n",
    "\n",
    "    # Add some more parameters for XGBoost\n",
    "    if model_name == 'xgboost':\n",
    "        if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            # defines how selective algorithm is.\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "\n",
    "    model = Classifier(params, model_type=model_name, log=False)\n",
    "\n",
    "    # Create a pipeline with just the classifier since feature prep is external\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Perform cross validation\n",
    "    scores = cross_validation(model, X, y, n_splits=10)\n",
    "\n",
    "    folds_scores.append(scores)     # Save scores for statistic tests\n",
    "    return np.mean(scores)\n",
    "\n",
    "def optimize_model(model_name, X, y, n_trials=50, timout=1200):\n",
    "    \"\"\"\n",
    "    The actual optimization.\n",
    "    \"\"\"\n",
    "    folds_scores = []   # create a list to store the scores from each trial folds\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    progress_bar = TqdmCallback(n_trials)\n",
    "    study.optimize(lambda trial: objective(trial, model_name, X, y, folds_scores), n_trials=n_trials, timeout=timout, callbacks=[progress_bar])\n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "\n",
    "    print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best F1 score for {model_name}: {best_value}\")\n",
    "\n",
    "    return best_params, best_value, folds_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimize models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ekXAYOkw0w_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Status]: Loading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing comments: 100%|██████████| 6637/6637 [00:00<00:00, 27607.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Status]: No Augmentation was chosen (augmentation/ adversation ratio == 0 or no augmented_classes). Moving on...\n",
      "[EmbeddingDataset]: Loading precomputed embeddings from C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\Data\\subset B_augmentation=0_embeddings_distilbert.pkl...\n",
      "[Dataset Status]: Loading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing comments: 100%|██████████| 6637/6637 [00:00<00:00, 29965.96it/s]\n",
      "Augmenting data: 100%|██████████| 6537/6537 [00:16<00:00, 398.76row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EmbeddingDataset]: Loading precomputed embeddings from C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\Data\\subset B_augmentation=5_embeddings_distilbert.pkl...\n",
      "[Dataset Status]: Loading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing comments: 100%|██████████| 6637/6637 [00:00<00:00, 37508.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Status]: No Augmentation was chosen (augmentation/ adversation ratio == 0 or no augmented_classes). Moving on...\n",
      "[EmbeddingDataset]: Loading precomputed embeddings from C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\Data\\subset B_augmentation=0_embeddings_tf-idf.pkl...\n",
      "[Dataset Status]: Loading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing comments: 100%|██████████| 6637/6637 [00:00<00:00, 37395.36it/s]\n",
      "Augmenting data: 100%|██████████| 6537/6537 [00:12<00:00, 505.05row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EmbeddingDataset]: Loading precomputed embeddings from C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\Data\\subset B_augmentation=5_embeddings_tf-idf.pkl...\n"
     ]
    }
   ],
   "source": [
    "# Create 4 different datasets: embedding with and without augmentation, and tf-idf with and without augmentation.\n",
    "embedder = Embedder()\n",
    "bert_embedding_no_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    embedder=embedder,\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "X_bert_no_augmentation, y_bert_no_augmentation = bert_embedding_no_augmentation_data.embeddings, bert_embedding_no_augmentation_data.labels\n",
    "\n",
    "bert_embedding_with_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=5,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    embedder=embedder,\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "X_bert_with_augmentation, y_bert_with_augmentation = bert_embedding_with_augmentation_data.embeddings, bert_embedding_with_augmentation_data.labels\n",
    "\n",
    "tfidf_embedding_no_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    embedder=embedder,\n",
    "    embedding_method='tf-idf'\n",
    ")\n",
    "\n",
    "X_tfidf_no_augmentation, y_tfidf_no_augmentation = tfidf_embedding_no_augmentation_data.embeddings, tfidf_embedding_no_augmentation_data.labels\n",
    "\n",
    "tfidf_embedding_with_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=5,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    embedder=embedder,\n",
    "    embedding_method='tf-idf'\n",
    ")\n",
    "\n",
    "X_tfidf_with_augmentation, y_tfidf_with_augmentation = tfidf_embedding_with_augmentation_data.embeddings, tfidf_embedding_with_augmentation_data.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 21:49:16,949] A new study created in memory with name: no-name-3bdc8213-9c68-424d-84e9-53a4649ba7c5\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 21:49:18,579] Trial 0 finished with value: 0.5276163648369556 and parameters: {'learning_rate': 0.0003002489113552147, 'weight_decay': 0.0002848277827833069}. Best is trial 0 with value: 0.5276163648369556.\n",
      "\n",
      "  2%|▏         | 1/50 [00:01<01:19,  1.63s/it]\u001B[A[I 2025-01-21 21:49:19,656] Trial 1 finished with value: 0.5340346366569726 and parameters: {'learning_rate': 0.0004935960792804718, 'weight_decay': 1.4086721855697989e-05}. Best is trial 1 with value: 0.5340346366569726.\n",
      "\n",
      "  4%|▍         | 2/50 [00:02<01:02,  1.30s/it]\u001B[A[I 2025-01-21 21:49:20,514] Trial 2 finished with value: 0.5046833949168973 and parameters: {'learning_rate': 1.536356067988419e-05, 'weight_decay': 0.0001960894451980013}. Best is trial 1 with value: 0.5340346366569726.\n",
      "\n",
      "  6%|▌         | 3/50 [00:03<00:51,  1.10s/it]\u001B[A[I 2025-01-21 21:49:21,369] Trial 3 finished with value: 0.5331230594152605 and parameters: {'learning_rate': 1.652790936411696e-05, 'weight_decay': 1.8733269920304056e-05}. Best is trial 1 with value: 0.5340346366569726.\n",
      "\n",
      "  8%|▊         | 4/50 [00:04<00:46,  1.00s/it]\u001B[A[I 2025-01-21 21:49:22,295] Trial 4 finished with value: 0.5354058661271666 and parameters: {'learning_rate': 0.0001778845662606258, 'weight_decay': 7.052772578193963e-05}. Best is trial 4 with value: 0.5354058661271666.\n",
      "\n",
      " 10%|█         | 5/50 [00:05<00:43,  1.03it/s]\u001B[A[I 2025-01-21 21:49:23,196] Trial 5 finished with value: 0.58649142279107 and parameters: {'learning_rate': 2.9253142977884583e-05, 'weight_decay': 4.170667709309629e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 12%|█▏        | 6/50 [00:06<00:41,  1.05it/s]\u001B[A[I 2025-01-21 21:49:24,038] Trial 6 finished with value: 0.539394514145487 and parameters: {'learning_rate': 3.5663930470554386e-05, 'weight_decay': 1.2929931604855265e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 14%|█▍        | 7/50 [00:07<00:39,  1.09it/s]\u001B[A[I 2025-01-21 21:49:24,852] Trial 7 finished with value: 0.4907603579808084 and parameters: {'learning_rate': 1.7575304042113724e-05, 'weight_decay': 0.00014727317567380006}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 16%|█▌        | 8/50 [00:07<00:37,  1.13it/s]\u001B[A[I 2025-01-21 21:49:25,689] Trial 8 finished with value: 0.5447490059991289 and parameters: {'learning_rate': 1.8880135934830733e-05, 'weight_decay': 0.0003930583617148313}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 18%|█▊        | 9/50 [00:08<00:35,  1.15it/s]\u001B[A[I 2025-01-21 21:49:26,517] Trial 9 finished with value: 0.5496354159349228 and parameters: {'learning_rate': 4.330941869129407e-05, 'weight_decay': 0.0005180102601616541}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 20%|██        | 10/50 [00:09<00:34,  1.17it/s]\u001B[A[I 2025-01-21 21:49:27,383] Trial 10 finished with value: 0.5247041413190591 and parameters: {'learning_rate': 6.886871715510408e-05, 'weight_decay': 4.8910251090433965e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 22%|██▏       | 11/50 [00:10<00:33,  1.16it/s]\u001B[A[I 2025-01-21 21:49:28,263] Trial 11 finished with value: 0.5325032430888255 and parameters: {'learning_rate': 6.988728339533856e-05, 'weight_decay': 0.0008263202388873436}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 24%|██▍       | 12/50 [00:11<00:32,  1.16it/s]\u001B[A[I 2025-01-21 21:49:29,125] Trial 12 finished with value: 0.5344886690925439 and parameters: {'learning_rate': 3.967541438579903e-05, 'weight_decay': 3.905135148733988e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 26%|██▌       | 13/50 [00:12<00:31,  1.16it/s]\u001B[A[I 2025-01-21 21:49:29,979] Trial 13 finished with value: 0.5363338344315345 and parameters: {'learning_rate': 3.84817745047107e-05, 'weight_decay': 0.0009669481128591329}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 28%|██▊       | 14/50 [00:13<00:31,  1.16it/s]\u001B[A[I 2025-01-21 21:49:30,820] Trial 14 finished with value: 0.5351016948358785 and parameters: {'learning_rate': 0.00017433971557892462, 'weight_decay': 3.0384786917300337e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 30%|███       | 15/50 [00:13<00:29,  1.17it/s]\u001B[A[I 2025-01-21 21:49:31,706] Trial 15 finished with value: 0.5190447757000154 and parameters: {'learning_rate': 0.0008496566796338511, 'weight_decay': 0.00010333791285910759}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 32%|███▏      | 16/50 [00:14<00:29,  1.16it/s]\u001B[A[I 2025-01-21 21:49:32,577] Trial 16 finished with value: 0.5448864567674014 and parameters: {'learning_rate': 9.56082766864914e-05, 'weight_decay': 0.00048522254828151074}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 34%|███▍      | 17/50 [00:15<00:28,  1.15it/s]\u001B[A[I 2025-01-21 21:49:33,454] Trial 17 finished with value: 0.5328275519713765 and parameters: {'learning_rate': 1.0185630973801216e-05, 'weight_decay': 7.832401252888578e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 36%|███▌      | 18/50 [00:16<00:27,  1.15it/s]\u001B[A[I 2025-01-21 21:49:34,402] Trial 18 finished with value: 0.567843779123406 and parameters: {'learning_rate': 3.2392158145239984e-05, 'weight_decay': 2.427290706979105e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 38%|███▊      | 19/50 [00:17<00:27,  1.12it/s]\u001B[A[I 2025-01-21 21:49:35,306] Trial 19 finished with value: 0.5274597131095717 and parameters: {'learning_rate': 2.528308922532836e-05, 'weight_decay': 2.3677168782024642e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 40%|████      | 20/50 [00:18<00:26,  1.12it/s]\u001B[A[I 2025-01-21 21:49:36,197] Trial 20 finished with value: 0.49627899461904834 and parameters: {'learning_rate': 1.0184805740103347e-05, 'weight_decay': 4.948572832742056e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 42%|████▏     | 21/50 [00:19<00:25,  1.12it/s]\u001B[A[I 2025-01-21 21:49:37,083] Trial 21 finished with value: 0.5341861369075216 and parameters: {'learning_rate': 5.682476938565351e-05, 'weight_decay': 1.0636827691892344e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 44%|████▍     | 22/50 [00:20<00:24,  1.12it/s]\u001B[A[I 2025-01-21 21:49:37,938] Trial 22 finished with value: 0.55253733649915 and parameters: {'learning_rate': 2.8525379718773096e-05, 'weight_decay': 2.6989585639404327e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 46%|████▌     | 23/50 [00:20<00:23,  1.13it/s]\u001B[A[I 2025-01-21 21:49:38,826] Trial 23 finished with value: 0.5533091213922099 and parameters: {'learning_rate': 2.4976277667920177e-05, 'weight_decay': 2.2300649794187543e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 48%|████▊     | 24/50 [00:21<00:22,  1.13it/s]\u001B[A[I 2025-01-21 21:49:39,689] Trial 24 finished with value: 0.5516320815244626 and parameters: {'learning_rate': 2.6049207430239015e-05, 'weight_decay': 1.689510199680115e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 50%|█████     | 25/50 [00:22<00:21,  1.14it/s]\u001B[A[I 2025-01-21 21:49:40,544] Trial 25 finished with value: 0.537551456228838 and parameters: {'learning_rate': 0.000109243860426099, 'weight_decay': 4.3614304118817605e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 52%|█████▏    | 26/50 [00:23<00:20,  1.15it/s]\u001B[A[I 2025-01-21 21:49:41,385] Trial 26 finished with value: 0.5692138378034102 and parameters: {'learning_rate': 4.9161596908777335e-05, 'weight_decay': 2.1561206672514696e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 54%|█████▍    | 27/50 [00:24<00:19,  1.16it/s]\u001B[A[I 2025-01-21 21:49:42,262] Trial 27 finished with value: 0.540610028520449 and parameters: {'learning_rate': 0.00010191337049304405, 'weight_decay': 3.2080746836855343e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 56%|█████▌    | 28/50 [00:25<00:19,  1.15it/s]\u001B[A[I 2025-01-21 21:49:43,165] Trial 28 finished with value: 0.5478052367103606 and parameters: {'learning_rate': 5.19611495239132e-05, 'weight_decay': 5.818558128647654e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 58%|█████▊    | 29/50 [00:26<00:18,  1.14it/s]\u001B[A[I 2025-01-21 21:49:44,004] Trial 29 finished with value: 0.5341882443298631 and parameters: {'learning_rate': 0.0001783656318656105, 'weight_decay': 0.00010069962726380171}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 60%|██████    | 30/50 [00:27<00:17,  1.16it/s]\u001B[A[I 2025-01-21 21:49:44,843] Trial 30 finished with value: 0.5384630334705499 and parameters: {'learning_rate': 7.337616834226939e-05, 'weight_decay': 3.6383067750555634e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 62%|██████▏   | 31/50 [00:27<00:16,  1.17it/s]\u001B[A[I 2025-01-21 21:49:45,737] Trial 31 finished with value: 0.5522343359980518 and parameters: {'learning_rate': 2.4151885346985645e-05, 'weight_decay': 1.9997531520814265e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 64%|██████▍   | 32/50 [00:28<00:15,  1.15it/s]\u001B[A[I 2025-01-21 21:49:46,781] Trial 32 finished with value: 0.5675367979356627 and parameters: {'learning_rate': 1.3295258473600917e-05, 'weight_decay': 2.3142282602142103e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 66%|██████▌   | 33/50 [00:29<00:15,  1.09it/s]\u001B[A[I 2025-01-21 21:49:47,640] Trial 33 finished with value: 0.5326589581840576 and parameters: {'learning_rate': 3.256629202720878e-05, 'weight_decay': 1.4247460322687678e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 68%|██████▊   | 34/50 [00:30<00:14,  1.11it/s]\u001B[A[I 2025-01-21 21:49:48,487] Trial 34 finished with value: 0.5375554369154829 and parameters: {'learning_rate': 1.4756138599038144e-05, 'weight_decay': 1.0548211594597038e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 70%|███████   | 35/50 [00:31<00:13,  1.13it/s]\u001B[A[I 2025-01-21 21:49:49,349] Trial 35 finished with value: 0.5777788705152881 and parameters: {'learning_rate': 1.276927781936162e-05, 'weight_decay': 2.7118688938916526e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 72%|███████▏  | 36/50 [00:32<00:12,  1.14it/s]\u001B[A[I 2025-01-21 21:49:50,192] Trial 36 finished with value: 0.5415267572389958 and parameters: {'learning_rate': 2.0671763371964432e-05, 'weight_decay': 1.3932475371410741e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 74%|███████▍  | 37/50 [00:33<00:11,  1.15it/s]\u001B[A[I 2025-01-21 21:49:51,057] Trial 37 finished with value: 0.5453524312629079 and parameters: {'learning_rate': 4.950355833582023e-05, 'weight_decay': 6.653518267211397e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 76%|███████▌  | 38/50 [00:34<00:10,  1.15it/s]\u001B[A[I 2025-01-21 21:49:51,915] Trial 38 finished with value: 0.5095812785965503 and parameters: {'learning_rate': 1.240331447267155e-05, 'weight_decay': 1.7873200794723686e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 78%|███████▊  | 39/50 [00:34<00:09,  1.16it/s]\u001B[A[I 2025-01-21 21:49:52,770] Trial 39 finished with value: 0.5464248750766868 and parameters: {'learning_rate': 3.296992523563946e-05, 'weight_decay': 0.00020881631044517753}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 80%|████████  | 40/50 [00:35<00:08,  1.16it/s]\u001B[A[I 2025-01-21 21:49:53,618] Trial 40 finished with value: 0.561105881581597 and parameters: {'learning_rate': 1.960570892264714e-05, 'weight_decay': 0.00014151058909000187}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 82%|████████▏ | 41/50 [00:36<00:07,  1.17it/s]\u001B[A[I 2025-01-21 21:49:54,463] Trial 41 finished with value: 0.5655469229292234 and parameters: {'learning_rate': 1.4168842091629155e-05, 'weight_decay': 2.658856650777598e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 84%|████████▍ | 42/50 [00:37<00:06,  1.17it/s]\u001B[A[I 2025-01-21 21:49:55,320] Trial 42 finished with value: 0.5369569289705007 and parameters: {'learning_rate': 1.3544426844358437e-05, 'weight_decay': 3.212924870327706e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 86%|████████▌ | 43/50 [00:38<00:05,  1.17it/s]\u001B[A[I 2025-01-21 21:49:56,168] Trial 43 finished with value: 0.5487238386932107 and parameters: {'learning_rate': 1.775797580571606e-05, 'weight_decay': 1.7317439626177246e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 88%|████████▊ | 44/50 [00:39<00:05,  1.17it/s]\u001B[A[I 2025-01-21 21:49:56,999] Trial 44 finished with value: 0.5658555432232322 and parameters: {'learning_rate': 2.1073315931606027e-05, 'weight_decay': 2.2930314124284397e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 90%|█████████ | 45/50 [00:40<00:04,  1.18it/s]\u001B[A[I 2025-01-21 21:49:57,864] Trial 45 finished with value: 0.5627911169806726 and parameters: {'learning_rate': 1.1683754679051998e-05, 'weight_decay': 3.84834298769348e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 92%|█████████▏| 46/50 [00:40<00:03,  1.17it/s]\u001B[A[I 2025-01-21 21:49:58,725] Trial 46 finished with value: 0.547187527806267 and parameters: {'learning_rate': 4.473871407581101e-05, 'weight_decay': 5.358704218371904e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 94%|█████████▍| 47/50 [00:41<00:02,  1.17it/s]\u001B[A[I 2025-01-21 21:49:59,569] Trial 47 finished with value: 0.5286855304382033 and parameters: {'learning_rate': 3.143273641220436e-05, 'weight_decay': 1.4469867838422796e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 96%|█████████▌| 48/50 [00:42<00:01,  1.17it/s]\u001B[A[I 2025-01-21 21:50:00,524] Trial 48 finished with value: 0.5182720541748036 and parameters: {'learning_rate': 0.00037996626438459156, 'weight_decay': 2.6233287320666412e-05}. Best is trial 5 with value: 0.58649142279107.\n",
      "\n",
      " 98%|█████████▊| 49/50 [00:43<00:00,  1.13it/s]\u001B[A[I 2025-01-21 21:50:01,458] Trial 49 finished with value: 0.5893959190937148 and parameters: {'learning_rate': 1.639303640706943e-05, 'weight_decay': 8.202639343676631e-05}. Best is trial 49 with value: 0.5893959190937148.\n",
      "\n",
      "100%|██████████| 50/50 [00:44<00:00,  1.12it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for logistic_regression: {'learning_rate': 1.639303640706943e-05, 'weight_decay': 8.202639343676631e-05}\n",
      "Best F1 score for logistic_regression: 0.5893959190937148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr_results = {}\n",
    "lr_results['bert_without_augmentation'] = optimize_model('logistic_regression', X_bert_no_augmentation, y_bert_no_augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 21:58:20,985] A new study created in memory with name: no-name-7d3004a2-0ed9-447e-95d1-30247402c1e2\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 21:58:24,483] Trial 0 finished with value: 0.3528695426375376 and parameters: {'learning_rate': 0.00015693529527601566, 'weight_decay': 5.552633583208091e-05}. Best is trial 0 with value: 0.3528695426375376.\n",
      "\n",
      "  2%|▏         | 1/50 [00:03<02:51,  3.50s/it]\u001B[A[I 2025-01-21 21:58:26,720] Trial 1 finished with value: 0.3463980195963023 and parameters: {'learning_rate': 0.0007350279741325184, 'weight_decay': 0.000456344870848001}. Best is trial 0 with value: 0.3528695426375376.\n",
      "\n",
      "  4%|▍         | 2/50 [00:05<02:12,  2.76s/it]\u001B[A[I 2025-01-21 21:58:28,877] Trial 2 finished with value: 0.35646065159436346 and parameters: {'learning_rate': 0.0009300364309336016, 'weight_decay': 0.0004383435616569376}. Best is trial 2 with value: 0.35646065159436346.\n",
      "\n",
      "  6%|▌         | 3/50 [00:07<01:56,  2.48s/it]\u001B[A[I 2025-01-21 21:58:31,248] Trial 3 finished with value: 0.35299134438943325 and parameters: {'learning_rate': 5.0421814322505434e-05, 'weight_decay': 1.0873426723895398e-05}. Best is trial 2 with value: 0.35646065159436346.\n",
      "\n",
      "  8%|▊         | 4/50 [00:10<01:52,  2.44s/it]\u001B[A[I 2025-01-21 21:58:33,622] Trial 4 finished with value: 0.35204511304227404 and parameters: {'learning_rate': 1.0626813615730687e-05, 'weight_decay': 0.0001707235156918579}. Best is trial 2 with value: 0.35646065159436346.\n",
      "\n",
      " 10%|█         | 5/50 [00:12<01:48,  2.41s/it]\u001B[A[I 2025-01-21 21:58:35,944] Trial 5 finished with value: 0.3524020357996053 and parameters: {'learning_rate': 3.1119995285675224e-05, 'weight_decay': 2.1184597906356498e-05}. Best is trial 2 with value: 0.35646065159436346.\n",
      "\n",
      " 12%|█▏        | 6/50 [00:14<01:44,  2.38s/it]\u001B[A[I 2025-01-21 21:58:38,212] Trial 6 finished with value: 0.3517527957622131 and parameters: {'learning_rate': 1.2124662856961034e-05, 'weight_decay': 6.404278363919789e-05}. Best is trial 2 with value: 0.35646065159436346.\n",
      "\n",
      " 14%|█▍        | 7/50 [00:17<01:40,  2.35s/it]\u001B[A[I 2025-01-21 21:58:40,464] Trial 7 finished with value: 0.34128040715992103 and parameters: {'learning_rate': 1.1563791842305465e-05, 'weight_decay': 0.0006173156860127466}. Best is trial 2 with value: 0.35646065159436346.\n",
      "\n",
      " 16%|█▌        | 8/50 [00:19<01:37,  2.32s/it]\u001B[A[I 2025-01-21 21:58:42,744] Trial 8 finished with value: 0.35669757296679705 and parameters: {'learning_rate': 9.000064273228029e-05, 'weight_decay': 1.6532731031711897e-05}. Best is trial 8 with value: 0.35669757296679705.\n",
      "\n",
      " 18%|█▊        | 9/50 [00:21<01:34,  2.30s/it]\u001B[A[I 2025-01-21 21:58:44,986] Trial 9 finished with value: 0.357110064743967 and parameters: {'learning_rate': 1.901273829727439e-05, 'weight_decay': 2.0489775000286897e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 20%|██        | 10/50 [00:24<01:31,  2.29s/it]\u001B[A[I 2025-01-21 21:58:47,224] Trial 10 finished with value: 0.35051663608350936 and parameters: {'learning_rate': 0.0002587772992400545, 'weight_decay': 3.41174056784941e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 22%|██▏       | 11/50 [00:26<01:28,  2.27s/it]\u001B[A[I 2025-01-21 21:58:49,488] Trial 11 finished with value: 0.34792770834054637 and parameters: {'learning_rate': 5.272412912104504e-05, 'weight_decay': 1.006229055245734e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 24%|██▍       | 12/50 [00:28<01:26,  2.27s/it]\u001B[A[I 2025-01-21 21:58:51,792] Trial 12 finished with value: 0.35204944084755735 and parameters: {'learning_rate': 2.656910159170524e-05, 'weight_decay': 2.2566246157917397e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 26%|██▌       | 13/50 [00:30<01:24,  2.28s/it]\u001B[A[I 2025-01-21 21:58:55,587] Trial 13 finished with value: 0.3541082643769692 and parameters: {'learning_rate': 9.903220888725113e-05, 'weight_decay': 0.00019749657134439106}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 28%|██▊       | 14/50 [00:34<01:38,  2.74s/it]\u001B[A[I 2025-01-21 21:58:58,382] Trial 14 finished with value: 0.35104597860333064 and parameters: {'learning_rate': 0.0003568290546571376, 'weight_decay': 2.0594297763724315e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 30%|███       | 15/50 [00:37<01:36,  2.76s/it]\u001B[A[I 2025-01-21 21:59:04,645] Trial 15 finished with value: 0.3449259425959907 and parameters: {'learning_rate': 9.587580875475849e-05, 'weight_decay': 4.241783221622661e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 32%|███▏      | 16/50 [00:43<02:09,  3.81s/it]\u001B[A[I 2025-01-21 21:59:11,595] Trial 16 finished with value: 0.348987432053457 and parameters: {'learning_rate': 2.337053837254946e-05, 'weight_decay': 9.916373408268068e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 34%|███▍      | 17/50 [00:50<02:36,  4.76s/it]\u001B[A[I 2025-01-21 21:59:18,502] Trial 17 finished with value: 0.3450441782363328 and parameters: {'learning_rate': 5.168154187629216e-05, 'weight_decay': 1.5140236241266125e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 36%|███▌      | 18/50 [00:57<02:52,  5.40s/it]\u001B[A[I 2025-01-21 21:59:25,354] Trial 18 finished with value: 0.354461621022747 and parameters: {'learning_rate': 0.00017856561213583776, 'weight_decay': 3.2882865983868336e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 38%|███▊      | 19/50 [01:04<03:00,  5.84s/it]\u001B[A[I 2025-01-21 21:59:32,395] Trial 19 finished with value: 0.355284804210089 and parameters: {'learning_rate': 1.7612696165439793e-05, 'weight_decay': 9.207922951674453e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 40%|████      | 20/50 [01:11<03:05,  6.20s/it]\u001B[A[I 2025-01-21 21:59:39,220] Trial 20 finished with value: 0.35446182875740057 and parameters: {'learning_rate': 0.00042402933542710393, 'weight_decay': 0.00021973844761170137}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 42%|████▏     | 21/50 [01:18<03:05,  6.39s/it]\u001B[A[I 2025-01-21 21:59:46,204] Trial 21 finished with value: 0.34227857217048097 and parameters: {'learning_rate': 0.0006914932462057439, 'weight_decay': 0.0003409595085235656}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 44%|████▍     | 22/50 [01:25<03:03,  6.57s/it]\u001B[A[I 2025-01-21 21:59:52,874] Trial 22 finished with value: 0.3485748364089603 and parameters: {'learning_rate': 0.0009302946533800134, 'weight_decay': 0.0008008154559753934}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 46%|████▌     | 23/50 [01:31<02:58,  6.60s/it]\u001B[A[I 2025-01-21 21:59:59,930] Trial 23 finished with value: 0.3477518263338296 and parameters: {'learning_rate': 0.00013768918325167157, 'weight_decay': 0.0009501717833611173}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 48%|████▊     | 24/50 [01:38<02:55,  6.74s/it]\u001B[A[I 2025-01-21 22:00:03,640] Trial 24 finished with value: 0.3491635217948274 and parameters: {'learning_rate': 8.245756116707714e-05, 'weight_decay': 0.00013824875232743358}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 50%|█████     | 25/50 [01:42<02:25,  5.83s/it]\u001B[A[I 2025-01-21 22:00:05,941] Trial 25 finished with value: 0.3440426894713153 and parameters: {'learning_rate': 0.00023567346793994232, 'weight_decay': 0.0003427039872864213}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 52%|█████▏    | 26/50 [01:44<01:54,  4.77s/it]\u001B[A[I 2025-01-21 22:00:08,494] Trial 26 finished with value: 0.3528137312606031 and parameters: {'learning_rate': 3.715420756982773e-05, 'weight_decay': 1.5832740174447013e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 54%|█████▍    | 27/50 [01:47<01:34,  4.10s/it]\u001B[A[I 2025-01-21 22:00:10,808] Trial 27 finished with value: 0.3518711352698819 and parameters: {'learning_rate': 0.0004630571094219102, 'weight_decay': 6.885092010800353e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 56%|█████▌    | 28/50 [01:49<01:18,  3.57s/it]\u001B[A[I 2025-01-21 22:00:13,075] Trial 28 finished with value: 0.3524618980022851 and parameters: {'learning_rate': 1.734722545459672e-05, 'weight_decay': 2.853721085343366e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 58%|█████▊    | 29/50 [01:52<01:06,  3.18s/it]\u001B[A[I 2025-01-21 22:00:15,344] Trial 29 finished with value: 0.34945964754353775 and parameters: {'learning_rate': 6.683321083840187e-05, 'weight_decay': 4.658969520365281e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 60%|██████    | 30/50 [01:54<00:58,  2.90s/it]\u001B[A[I 2025-01-21 22:00:17,521] Trial 30 finished with value: 0.3455733130215005 and parameters: {'learning_rate': 0.00014576936932823612, 'weight_decay': 1.4422102346099034e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 62%|██████▏   | 31/50 [01:56<00:51,  2.69s/it]\u001B[A[I 2025-01-21 22:00:19,698] Trial 31 finished with value: 0.3519882976145137 and parameters: {'learning_rate': 1.7794083057040008e-05, 'weight_decay': 8.475093575052965e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 64%|██████▍   | 32/50 [01:58<00:45,  2.53s/it]\u001B[A[I 2025-01-21 22:00:21,931] Trial 32 finished with value: 0.3566982654156424 and parameters: {'learning_rate': 1.779846110620698e-05, 'weight_decay': 0.00012759674863136922}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 66%|██████▌   | 33/50 [02:00<00:41,  2.44s/it]\u001B[A[I 2025-01-21 22:00:24,163] Trial 33 finished with value: 0.35128227677180346 and parameters: {'learning_rate': 4.140064006527846e-05, 'weight_decay': 0.0001365719214941254}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 68%|██████▊   | 34/50 [02:03<00:38,  2.38s/it]\u001B[A[I 2025-01-21 22:00:26,369] Trial 34 finished with value: 0.34675186095627186 and parameters: {'learning_rate': 2.1184449382426964e-05, 'weight_decay': 0.00031318408175270453}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 70%|███████   | 35/50 [02:05<00:34,  2.33s/it]\u001B[A[I 2025-01-21 22:00:28,580] Trial 35 finished with value: 0.35169445694699303 and parameters: {'learning_rate': 1.332055847538953e-05, 'weight_decay': 0.0002638122031504257}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 72%|███████▏  | 36/50 [02:07<00:32,  2.29s/it]\u001B[A[I 2025-01-21 22:00:30,876] Trial 36 finished with value: 0.3529882283696292 and parameters: {'learning_rate': 3.2010259969996715e-05, 'weight_decay': 0.0004717782925170177}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 74%|███████▍  | 37/50 [02:09<00:29,  2.29s/it]\u001B[A[I 2025-01-21 22:00:33,167] Trial 37 finished with value: 0.3441619984073677 and parameters: {'learning_rate': 0.000640177558980046, 'weight_decay': 0.00013124207463446322}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 76%|███████▌  | 38/50 [02:12<00:27,  2.29s/it]\u001B[A[I 2025-01-21 22:00:35,600] Trial 38 finished with value: 0.35263559879513906 and parameters: {'learning_rate': 7.347745874037108e-05, 'weight_decay': 0.0005028602418312897}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 78%|███████▊  | 39/50 [02:14<00:25,  2.33s/it]\u001B[A[I 2025-01-21 22:00:38,021] Trial 39 finished with value: 0.35134106567877293 and parameters: {'learning_rate': 1.4419994965298746e-05, 'weight_decay': 5.3541928554925985e-05}. Best is trial 9 with value: 0.357110064743967.\n",
      "\n",
      " 80%|████████  | 40/50 [02:17<00:23,  2.36s/it]\u001B[A[I 2025-01-21 22:00:40,253] Trial 40 finished with value: 0.35763694907038746 and parameters: {'learning_rate': 0.0002738246363978017, 'weight_decay': 1.3118316853159452e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 82%|████████▏ | 41/50 [02:19<00:20,  2.32s/it]\u001B[A[I 2025-01-21 22:00:42,559] Trial 41 finished with value: 0.346573243776616 and parameters: {'learning_rate': 0.00027815507963392747, 'weight_decay': 1.3224365914358835e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 84%|████████▍ | 42/50 [02:21<00:18,  2.32s/it]\u001B[A[I 2025-01-21 22:00:44,807] Trial 42 finished with value: 0.348222068344701 and parameters: {'learning_rate': 0.000862444197439563, 'weight_decay': 1.860714878787053e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 86%|████████▌ | 43/50 [02:23<00:16,  2.30s/it]\u001B[A[I 2025-01-21 22:00:47,006] Trial 43 finished with value: 0.3517543537721151 and parameters: {'learning_rate': 1.0466526384277212e-05, 'weight_decay': 1.1951978599768295e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 88%|████████▊ | 44/50 [02:26<00:13,  2.27s/it]\u001B[A[I 2025-01-21 22:00:49,201] Trial 44 finished with value: 0.35375421528234596 and parameters: {'learning_rate': 0.00018942257270611592, 'weight_decay': 2.6102222549190673e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 90%|█████████ | 45/50 [02:28<00:11,  2.25s/it]\u001B[A[I 2025-01-21 22:00:51,417] Trial 45 finished with value: 0.34581019977149186 and parameters: {'learning_rate': 0.0005088920505161798, 'weight_decay': 1.8054893281855668e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 92%|█████████▏| 46/50 [02:30<00:08,  2.24s/it]\u001B[A[I 2025-01-21 22:00:53,659] Trial 46 finished with value: 0.35451954436865984 and parameters: {'learning_rate': 0.0001140615838181215, 'weight_decay': 1.013572038480615e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 94%|█████████▍| 47/50 [02:32<00:06,  2.24s/it]\u001B[A[I 2025-01-21 22:00:55,928] Trial 47 finished with value: 0.3482238340892566 and parameters: {'learning_rate': 0.00032146029717754123, 'weight_decay': 3.7472196569385975e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 96%|█████████▌| 48/50 [02:34<00:04,  2.25s/it]\u001B[A[I 2025-01-21 22:00:58,146] Trial 48 finished with value: 0.3522838694041478 and parameters: {'learning_rate': 2.674515895269442e-05, 'weight_decay': 2.4662679260432778e-05}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      " 98%|█████████▊| 49/50 [02:37<00:02,  2.24s/it]\u001B[A[I 2025-01-21 22:01:00,651] Trial 49 finished with value: 0.35410781428521965 and parameters: {'learning_rate': 0.0005883338016473956, 'weight_decay': 0.0006989061763830691}. Best is trial 40 with value: 0.35763694907038746.\n",
      "\n",
      "100%|██████████| 50/50 [02:39<00:00,  3.19s/it]\u001B[A\n",
      "[I 2025-01-21 22:01:00,654] A new study created in memory with name: no-name-83a34e26-482e-481a-8fb6-a57d5646c4e3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for logistic_regression: {'learning_rate': 0.0002738246363978017, 'weight_decay': 1.3118316853159452e-05}\n",
      "Best F1 score for logistic_regression: 0.35763694907038746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 22:01:01,519] Trial 0 finished with value: 0.5004160988334246 and parameters: {'learning_rate': 1.4760397445003843e-05, 'weight_decay': 2.072476988515809e-05}. Best is trial 0 with value: 0.5004160988334246.\n",
      "\n",
      "  2%|▏         | 1/50 [00:00<00:42,  1.16it/s]\u001B[A[I 2025-01-21 22:01:02,389] Trial 1 finished with value: 0.6796696966716776 and parameters: {'learning_rate': 0.0006196821549443317, 'weight_decay': 1.2644084276175046e-05}. Best is trial 1 with value: 0.6796696966716776.\n",
      "\n",
      "  4%|▍         | 2/50 [00:01<00:41,  1.15it/s]\u001B[A[I 2025-01-21 22:01:03,276] Trial 2 finished with value: 0.5689660985992667 and parameters: {'learning_rate': 0.00013239681433701453, 'weight_decay': 0.0003621976558031507}. Best is trial 1 with value: 0.6796696966716776.\n",
      "\n",
      "  6%|▌         | 3/50 [00:02<00:41,  1.14it/s]\u001B[A[I 2025-01-21 22:01:04,158] Trial 3 finished with value: 0.41955547438076907 and parameters: {'learning_rate': 7.693301785032692e-05, 'weight_decay': 0.0002427105320404121}. Best is trial 1 with value: 0.6796696966716776.\n",
      "\n",
      "  8%|▊         | 4/50 [00:03<00:40,  1.14it/s]\u001B[A[I 2025-01-21 22:01:05,025] Trial 4 finished with value: 0.6720246708908777 and parameters: {'learning_rate': 0.0004991818939491546, 'weight_decay': 6.675001651923065e-05}. Best is trial 1 with value: 0.6796696966716776.\n",
      "\n",
      " 10%|█         | 5/50 [00:04<00:39,  1.14it/s]\u001B[A[I 2025-01-21 22:01:05,915] Trial 5 finished with value: 0.6568865878959027 and parameters: {'learning_rate': 5.654773696183038e-05, 'weight_decay': 0.0005605407665972589}. Best is trial 1 with value: 0.6796696966716776.\n",
      "\n",
      " 12%|█▏        | 6/50 [00:05<00:38,  1.14it/s]\u001B[A[I 2025-01-21 22:01:06,786] Trial 6 finished with value: 0.6769174030936961 and parameters: {'learning_rate': 0.00048289229266187435, 'weight_decay': 5.334929161498627e-05}. Best is trial 1 with value: 0.6796696966716776.\n",
      "\n",
      " 14%|█▍        | 7/50 [00:06<00:37,  1.14it/s]\u001B[A[I 2025-01-21 22:01:07,714] Trial 7 finished with value: 0.6796699308297156 and parameters: {'learning_rate': 0.0009303688160507927, 'weight_decay': 0.0009002867193664499}. Best is trial 7 with value: 0.6796699308297156.\n",
      "\n",
      " 16%|█▌        | 8/50 [00:07<00:37,  1.12it/s]\u001B[A[I 2025-01-21 22:01:08,602] Trial 8 finished with value: 0.6657555577410307 and parameters: {'learning_rate': 0.00020405609332227234, 'weight_decay': 0.00016713313675553311}. Best is trial 7 with value: 0.6796699308297156.\n",
      "\n",
      " 18%|█▊        | 9/50 [00:07<00:36,  1.12it/s]\u001B[A[I 2025-01-21 22:01:09,534] Trial 9 finished with value: 0.6191190037980434 and parameters: {'learning_rate': 0.00012620285568315637, 'weight_decay': 7.308714502887674e-05}. Best is trial 7 with value: 0.6796699308297156.\n",
      "\n",
      " 20%|██        | 10/50 [00:08<00:36,  1.11it/s]\u001B[A[I 2025-01-21 22:01:10,409] Trial 10 finished with value: 0.4614308929382619 and parameters: {'learning_rate': 2.498466904439765e-05, 'weight_decay': 0.0009833468764538882}. Best is trial 7 with value: 0.6796699308297156.\n",
      "\n",
      " 22%|██▏       | 11/50 [00:09<00:34,  1.12it/s]\u001B[A[I 2025-01-21 22:01:11,340] Trial 11 finished with value: 0.6799755070692312 and parameters: {'learning_rate': 0.0008989164748916521, 'weight_decay': 1.3288082813706007e-05}. Best is trial 11 with value: 0.6799755070692312.\n",
      "\n",
      " 24%|██▍       | 12/50 [00:10<00:34,  1.10it/s]\u001B[A[I 2025-01-21 22:01:12,176] Trial 12 finished with value: 0.6795160889987871 and parameters: {'learning_rate': 0.0009339697460753594, 'weight_decay': 2.508975936248799e-05}. Best is trial 11 with value: 0.6799755070692312.\n",
      "\n",
      " 26%|██▌       | 13/50 [00:11<00:32,  1.13it/s]\u001B[A[I 2025-01-21 22:01:13,025] Trial 13 finished with value: 0.6732476783230539 and parameters: {'learning_rate': 0.00027901370729243333, 'weight_decay': 2.8312883100652608e-05}. Best is trial 11 with value: 0.6799755070692312.\n",
      "\n",
      " 28%|██▊       | 14/50 [00:12<00:31,  1.14it/s]\u001B[A[I 2025-01-21 22:01:13,875] Trial 14 finished with value: 0.6801286464260459 and parameters: {'learning_rate': 0.0008808431619617066, 'weight_decay': 0.0001362224078468555}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 30%|███       | 15/50 [00:13<00:30,  1.15it/s]\u001B[A[I 2025-01-21 22:01:14,722] Trial 15 finished with value: 0.6772234476492874 and parameters: {'learning_rate': 0.00025310764170180186, 'weight_decay': 0.00014278608126230903}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 32%|███▏      | 16/50 [00:14<00:29,  1.16it/s]\u001B[A[I 2025-01-21 22:01:15,599] Trial 16 finished with value: 0.6769178714097719 and parameters: {'learning_rate': 0.0003981134912362852, 'weight_decay': 1.2101598331583055e-05}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 34%|███▍      | 17/50 [00:14<00:28,  1.15it/s]\u001B[A[I 2025-01-21 22:01:16,462] Trial 17 finished with value: 0.4060560293353189 and parameters: {'learning_rate': 4.133885822662142e-05, 'weight_decay': 4.268320323137853e-05}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 36%|███▌      | 18/50 [00:15<00:27,  1.16it/s]\u001B[A[I 2025-01-21 22:01:17,338] Trial 18 finished with value: 0.6799755070692312 and parameters: {'learning_rate': 0.0009910131166761692, 'weight_decay': 0.00010778903052926066}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 38%|███▊      | 19/50 [00:16<00:26,  1.15it/s]\u001B[A[I 2025-01-21 22:01:18,197] Trial 19 finished with value: 0.6238595332761989 and parameters: {'learning_rate': 0.0001826569336678234, 'weight_decay': 0.00030073837655009394}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 40%|████      | 20/50 [00:17<00:25,  1.16it/s]\u001B[A[I 2025-01-21 22:01:19,060] Trial 20 finished with value: 0.6541347626339971 and parameters: {'learning_rate': 0.0003725312422793498, 'weight_decay': 3.753811428864607e-05}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 42%|████▏     | 21/50 [00:18<00:25,  1.16it/s]\u001B[A[I 2025-01-21 22:01:19,938] Trial 21 finished with value: 0.6799755070692312 and parameters: {'learning_rate': 0.000764000112823612, 'weight_decay': 0.00012741079528071056}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 44%|████▍     | 22/50 [00:19<00:24,  1.15it/s]\u001B[A[I 2025-01-21 22:01:20,796] Trial 22 finished with value: 0.6798223677124164 and parameters: {'learning_rate': 0.0009781447235815172, 'weight_decay': 9.295498044072717e-05}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 46%|████▌     | 23/50 [00:20<00:23,  1.16it/s]\u001B[A[I 2025-01-21 22:01:21,646] Trial 23 finished with value: 0.6756948639775958 and parameters: {'learning_rate': 0.000539491996184679, 'weight_decay': 0.00020385378077772106}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 48%|████▊     | 24/50 [00:20<00:22,  1.16it/s]\u001B[A[I 2025-01-21 22:01:22,500] Trial 24 finished with value: 0.6798228360284924 and parameters: {'learning_rate': 0.0006346827303337988, 'weight_decay': 0.0001025779223138141}. Best is trial 14 with value: 0.6801286464260459.\n",
      "\n",
      " 50%|█████     | 25/50 [00:21<00:21,  1.16it/s]\u001B[A[I 2025-01-21 22:01:23,390] Trial 25 finished with value: 0.6805873620223762 and parameters: {'learning_rate': 0.00035128291989658535, 'weight_decay': 0.0004388332527791302}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 52%|█████▏    | 26/50 [00:22<00:20,  1.15it/s]\u001B[A[I 2025-01-21 22:01:24,270] Trial 26 finished with value: 0.6538282497623296 and parameters: {'learning_rate': 0.0003386904506610976, 'weight_decay': 0.0005192774189405124}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 54%|█████▍    | 27/50 [00:23<00:20,  1.15it/s]\u001B[A[I 2025-01-21 22:01:25,147] Trial 27 finished with value: 0.6792112152333853 and parameters: {'learning_rate': 0.0005846160535089755, 'weight_decay': 0.0004254337726603098}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 56%|█████▌    | 28/50 [00:24<00:19,  1.15it/s]\u001B[A[I 2025-01-21 22:01:26,001] Trial 28 finished with value: 0.6426668727257401 and parameters: {'learning_rate': 0.00017717477647069402, 'weight_decay': 0.0006877795031879829}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 58%|█████▊    | 29/50 [00:25<00:18,  1.15it/s]\u001B[A[I 2025-01-21 22:01:26,883] Trial 29 finished with value: 0.20408418449780125 and parameters: {'learning_rate': 1.2685414707777318e-05, 'weight_decay': 1.517087236356458e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 60%|██████    | 30/50 [00:26<00:17,  1.15it/s]\u001B[A[I 2025-01-21 22:01:27,735] Trial 30 finished with value: 0.6497000435533952 and parameters: {'learning_rate': 0.00028509679068319696, 'weight_decay': 1.946642351619496e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 62%|██████▏   | 31/50 [00:27<00:16,  1.15it/s]\u001B[A[I 2025-01-21 22:01:28,588] Trial 31 finished with value: 0.6796699308297156 and parameters: {'learning_rate': 0.0007669335780099893, 'weight_decay': 0.0002340926415723132}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 64%|██████▍   | 32/50 [00:27<00:15,  1.16it/s]\u001B[A[I 2025-01-21 22:01:29,627] Trial 32 finished with value: 0.6801286464260459 and parameters: {'learning_rate': 0.0006404240923334316, 'weight_decay': 0.00010848438225409635}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 66%|██████▌   | 33/50 [00:28<00:15,  1.09it/s]\u001B[A[I 2025-01-21 22:01:30,594] Trial 33 finished with value: 0.6793641204321621 and parameters: {'learning_rate': 0.0006905472999896048, 'weight_decay': 0.0003164705863511086}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 68%|██████▊   | 34/50 [00:29<00:14,  1.07it/s]\u001B[A[I 2025-01-21 22:01:31,465] Trial 34 finished with value: 0.6600980653862906 and parameters: {'learning_rate': 0.0003846826106955905, 'weight_decay': 0.000179225648521001}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 70%|███████   | 35/50 [00:30<00:13,  1.10it/s]\u001B[A[I 2025-01-21 22:01:32,346] Trial 35 finished with value: 0.6763060164566269 and parameters: {'learning_rate': 0.0004578860026388312, 'weight_decay': 5.9406118835715526e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 72%|███████▏  | 36/50 [00:31<00:12,  1.11it/s]\u001B[A[I 2025-01-21 22:01:33,223] Trial 36 finished with value: 0.6801286464260459 and parameters: {'learning_rate': 0.0006994034810921563, 'weight_decay': 7.884063948381047e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 74%|███████▍  | 37/50 [00:32<00:11,  1.12it/s]\u001B[A[I 2025-01-21 22:01:34,185] Trial 37 finished with value: 0.5406751244549971 and parameters: {'learning_rate': 7.969042969465989e-05, 'weight_decay': 8.765663678700598e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 76%|███████▌  | 38/50 [00:33<00:10,  1.09it/s]\u001B[A[I 2025-01-21 22:01:35,084] Trial 38 finished with value: 0.6796699308297156 and parameters: {'learning_rate': 0.0005784729771127856, 'weight_decay': 7.67609171540863e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 78%|███████▊  | 39/50 [00:34<00:10,  1.10it/s]\u001B[A[I 2025-01-21 22:01:36,074] Trial 39 finished with value: 0.6721773419316165 and parameters: {'learning_rate': 0.00044930094384648504, 'weight_decay': 4.714785718160569e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 80%|████████  | 40/50 [00:35<00:09,  1.07it/s]\u001B[A[I 2025-01-21 22:01:37,080] Trial 40 finished with value: 0.6799757412272691 and parameters: {'learning_rate': 0.00014277841341419378, 'weight_decay': 0.0001350128840841146}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 82%|████████▏ | 41/50 [00:36<00:08,  1.05it/s]\u001B[A[I 2025-01-21 22:01:38,080] Trial 41 finished with value: 0.6347158023893487 and parameters: {'learning_rate': 0.00014538846904500218, 'weight_decay': 0.0001300484494223399}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 84%|████████▍ | 42/50 [00:37<00:07,  1.03it/s]\u001B[A[I 2025-01-21 22:01:39,028] Trial 42 finished with value: 0.6255414904627432 and parameters: {'learning_rate': 8.691605458877154e-05, 'weight_decay': 0.0001610652290197618}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 86%|████████▌ | 43/50 [00:38<00:06,  1.04it/s]\u001B[A[I 2025-01-21 22:01:39,979] Trial 43 finished with value: 0.6798228360284924 and parameters: {'learning_rate': 0.0007398900380491519, 'weight_decay': 0.00025331555743640075}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 88%|████████▊ | 44/50 [00:39<00:05,  1.04it/s]\u001B[A[I 2025-01-21 22:01:40,945] Trial 44 finished with value: 0.6337969662484605 and parameters: {'learning_rate': 5.50534440227629e-05, 'weight_decay': 6.774702663645194e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 90%|█████████ | 45/50 [00:40<00:04,  1.04it/s]\u001B[A[I 2025-01-21 22:01:41,925] Trial 45 finished with value: 0.6767647320529572 and parameters: {'learning_rate': 0.00010879721256688889, 'weight_decay': 0.00011030918339996}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 92%|█████████▏| 46/50 [00:41<00:03,  1.03it/s]\u001B[A[I 2025-01-21 22:01:42,905] Trial 46 finished with value: 0.6752356800651896 and parameters: {'learning_rate': 0.0002278804789998781, 'weight_decay': 3.4262125954745337e-05}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 94%|█████████▍| 47/50 [00:42<00:02,  1.03it/s]\u001B[A[I 2025-01-21 22:01:43,787] Trial 47 finished with value: 0.6798228360284924 and parameters: {'learning_rate': 0.000308525415674263, 'weight_decay': 0.00042055556618002466}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 96%|█████████▌| 48/50 [00:43<00:01,  1.06it/s]\u001B[A[I 2025-01-21 22:01:44,712] Trial 48 finished with value: 0.6789054048358318 and parameters: {'learning_rate': 0.00047160270294090525, 'weight_decay': 0.0002002699486137508}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      " 98%|█████████▊| 49/50 [00:44<00:00,  1.07it/s]\u001B[A[I 2025-01-21 22:01:45,586] Trial 49 finished with value: 0.6801286464260459 and parameters: {'learning_rate': 0.0008201888270685744, 'weight_decay': 0.0007537318803000671}. Best is trial 25 with value: 0.6805873620223762.\n",
      "\n",
      "100%|██████████| 50/50 [00:44<00:00,  1.11it/s]\u001B[A\n",
      "[I 2025-01-21 22:01:45,590] A new study created in memory with name: no-name-35369abe-d8bd-4319-8d3d-79307bfef3c2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for logistic_regression: {'learning_rate': 0.00035128291989658535, 'weight_decay': 0.0004388332527791302}\n",
      "Best F1 score for logistic_regression: 0.6805873620223762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 22:01:47,808] Trial 0 finished with value: 0.43273330332721666 and parameters: {'learning_rate': 2.765886029653541e-05, 'weight_decay': 0.0003162455167593957}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      "  2%|▏         | 1/50 [00:02<01:48,  2.22s/it]\u001B[A[I 2025-01-21 22:01:50,043] Trial 1 finished with value: 0.4297309836235848 and parameters: {'learning_rate': 0.0006489292919329492, 'weight_decay': 0.00012014646053284168}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      "  4%|▍         | 2/50 [00:04<01:46,  2.23s/it]\u001B[A[I 2025-01-21 22:01:52,285] Trial 2 finished with value: 0.37206266662050347 and parameters: {'learning_rate': 1.9496940669960404e-05, 'weight_decay': 0.00023508800862769862}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      "  6%|▌         | 3/50 [00:06<01:44,  2.23s/it]\u001B[A[I 2025-01-21 22:01:54,525] Trial 3 finished with value: 0.4316733026347679 and parameters: {'learning_rate': 0.0004668006214031272, 'weight_decay': 0.0001895096920497403}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      "  8%|▊         | 4/50 [00:08<01:42,  2.24s/it]\u001B[A[I 2025-01-21 22:01:56,768] Trial 4 finished with value: 0.42902762870892913 and parameters: {'learning_rate': 3.0380692030636013e-05, 'weight_decay': 9.419001786061866e-05}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      " 10%|█         | 5/50 [00:11<01:40,  2.24s/it]\u001B[A[I 2025-01-21 22:01:58,978] Trial 5 finished with value: 0.33874988747706264 and parameters: {'learning_rate': 2.0789666838295615e-05, 'weight_decay': 5.595441186258284e-05}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      " 12%|█▏        | 6/50 [00:13<01:38,  2.23s/it]\u001B[A[I 2025-01-21 22:02:01,158] Trial 6 finished with value: 0.42331987674410554 and parameters: {'learning_rate': 3.743447575386872e-05, 'weight_decay': 0.0003924096103211198}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      " 14%|█▍        | 7/50 [00:15<01:35,  2.21s/it]\u001B[A[I 2025-01-21 22:02:03,402] Trial 7 finished with value: 0.33675289962953986 and parameters: {'learning_rate': 2.0631394770291526e-05, 'weight_decay': 7.487622270132263e-05}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      " 16%|█▌        | 8/50 [00:17<01:33,  2.22s/it]\u001B[A[I 2025-01-21 22:02:05,785] Trial 8 finished with value: 0.43237963507945854 and parameters: {'learning_rate': 0.0003938721708275912, 'weight_decay': 2.79452887708093e-05}. Best is trial 0 with value: 0.43273330332721666.\n",
      "\n",
      " 18%|█▊        | 9/50 [00:20<01:33,  2.27s/it]\u001B[A[I 2025-01-21 22:02:08,257] Trial 9 finished with value: 0.43408662535055226 and parameters: {'learning_rate': 0.00031459779339776633, 'weight_decay': 0.00015646618791604155}. Best is trial 9 with value: 0.43408662535055226.\n",
      "\n",
      " 20%|██        | 10/50 [00:22<01:33,  2.33s/it]\u001B[A[I 2025-01-21 22:02:10,629] Trial 10 finished with value: 0.4306752761139771 and parameters: {'learning_rate': 0.00015443703291688515, 'weight_decay': 0.0009885922543420723}. Best is trial 9 with value: 0.43408662535055226.\n",
      "\n",
      " 22%|██▏       | 11/50 [00:25<01:31,  2.35s/it]\u001B[A[I 2025-01-21 22:02:14,194] Trial 11 finished with value: 0.4349104663642973 and parameters: {'learning_rate': 8.907800526083353e-05, 'weight_decay': 0.0005285481392211958}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 24%|██▍       | 12/50 [00:28<01:43,  2.72s/it]\u001B[A[I 2025-01-21 22:02:16,721] Trial 12 finished with value: 0.43438084686493783 and parameters: {'learning_rate': 0.00012066440250280766, 'weight_decay': 0.0007157339259628808}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 26%|██▌       | 13/50 [00:31<01:38,  2.66s/it]\u001B[A[I 2025-01-21 22:02:19,027] Trial 13 finished with value: 0.4332632690509988 and parameters: {'learning_rate': 8.210478190568813e-05, 'weight_decay': 0.0007616151612882437}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 28%|██▊       | 14/50 [00:33<01:31,  2.55s/it]\u001B[A[I 2025-01-21 22:02:21,242] Trial 14 finished with value: 0.42473403039850427 and parameters: {'learning_rate': 8.744927479512353e-05, 'weight_decay': 0.0005151231655710017}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 30%|███       | 15/50 [00:35<01:25,  2.45s/it]\u001B[A[I 2025-01-21 22:02:23,467] Trial 15 finished with value: 0.4316158293806045 and parameters: {'learning_rate': 0.00016914868693501968, 'weight_decay': 0.0006068014685222412}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 32%|███▏      | 16/50 [00:37<01:21,  2.38s/it]\u001B[A[I 2025-01-21 22:02:25,735] Trial 16 finished with value: 0.40690703874251294 and parameters: {'learning_rate': 5.788792572235515e-05, 'weight_decay': 1.3932295499983879e-05}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 34%|███▍      | 17/50 [00:40<01:17,  2.35s/it]\u001B[A[I 2025-01-21 22:02:27,952] Trial 17 finished with value: 0.43332195409064156 and parameters: {'learning_rate': 0.00019324422517125688, 'weight_decay': 0.0003208787630358822}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 36%|███▌      | 18/50 [00:42<01:13,  2.31s/it]\u001B[A[I 2025-01-21 22:02:30,194] Trial 18 finished with value: 0.3215682581449295 and parameters: {'learning_rate': 1.0881283944204639e-05, 'weight_decay': 0.0008878789234478986}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 38%|███▊      | 19/50 [00:44<01:10,  2.29s/it]\u001B[A[I 2025-01-21 22:02:32,406] Trial 19 finished with value: 0.4249089429768376 and parameters: {'learning_rate': 5.109655530928627e-05, 'weight_decay': 0.0004710141328318703}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 40%|████      | 20/50 [00:46<01:07,  2.27s/it]\u001B[A[I 2025-01-21 22:02:34,627] Trial 20 finished with value: 0.4312631305612298 and parameters: {'learning_rate': 0.00011712172896521949, 'weight_decay': 4.026692561007142e-05}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 42%|████▏     | 21/50 [00:49<01:05,  2.25s/it]\u001B[A[I 2025-01-21 22:02:36,816] Trial 21 finished with value: 0.4327921268566285 and parameters: {'learning_rate': 0.00028177931481972515, 'weight_decay': 0.00015454631659140327}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 44%|████▍     | 22/50 [00:51<01:02,  2.23s/it]\u001B[A[I 2025-01-21 22:02:39,059] Trial 22 finished with value: 0.4343219540906415 and parameters: {'learning_rate': 0.0002628954555929842, 'weight_decay': 0.00023049690539170053}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 46%|████▌     | 23/50 [00:53<01:00,  2.24s/it]\u001B[A[I 2025-01-21 22:02:41,338] Trial 23 finished with value: 0.4229623654052557 and parameters: {'learning_rate': 0.0007983819635707609, 'weight_decay': 0.0002603835418169051}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 48%|████▊     | 24/50 [00:55<00:58,  2.25s/it]\u001B[A[I 2025-01-21 22:02:43,685] Trial 24 finished with value: 0.4346751030017657 and parameters: {'learning_rate': 0.0002163144663898241, 'weight_decay': 0.0006254028087702579}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 50%|█████     | 25/50 [00:58<00:56,  2.28s/it]\u001B[A[I 2025-01-21 22:02:46,175] Trial 25 finished with value: 0.43373378804140844 and parameters: {'learning_rate': 0.00011126767352090108, 'weight_decay': 0.0005732565275327353}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 52%|█████▏    | 26/50 [01:00<00:56,  2.34s/it]\u001B[A[I 2025-01-21 22:02:48,703] Trial 26 finished with value: 0.43308693695253264 and parameters: {'learning_rate': 5.9686065040259234e-05, 'weight_decay': 0.0007037403577896994}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 54%|█████▍    | 27/50 [01:03<00:55,  2.40s/it]\u001B[A[I 2025-01-21 22:02:51,155] Trial 27 finished with value: 0.4329685281999792 and parameters: {'learning_rate': 0.00019931241061629053, 'weight_decay': 0.00040483577312245243}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 56%|█████▌    | 28/50 [01:05<00:53,  2.41s/it]\u001B[A[I 2025-01-21 22:02:53,500] Trial 28 finished with value: 0.43355780216736484 and parameters: {'learning_rate': 0.00012965853305884123, 'weight_decay': 0.000667163529141041}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 58%|█████▊    | 29/50 [01:07<00:50,  2.39s/it]\u001B[A[I 2025-01-21 22:02:55,713] Trial 29 finished with value: 0.42596811273067203 and parameters: {'learning_rate': 7.091401946717756e-05, 'weight_decay': 0.0003358855201879135}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 60%|██████    | 30/50 [01:10<00:46,  2.34s/it]\u001B[A[I 2025-01-21 22:02:57,927] Trial 30 finished with value: 0.4061397361769899 and parameters: {'learning_rate': 4.620725851089549e-05, 'weight_decay': 0.000460243294577832}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 62%|██████▏   | 31/50 [01:12<00:43,  2.30s/it]\u001B[A[I 2025-01-21 22:03:00,337] Trial 31 finished with value: 0.43361558702350866 and parameters: {'learning_rate': 0.000267144521079405, 'weight_decay': 0.00022442786594526355}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 64%|██████▍   | 32/50 [01:14<00:42,  2.33s/it]\u001B[A[I 2025-01-21 22:03:03,079] Trial 32 finished with value: 0.4342633729183256 and parameters: {'learning_rate': 0.00023138383935311042, 'weight_decay': 0.00029908734534020627}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 66%|██████▌   | 33/50 [01:17<00:41,  2.46s/it]\u001B[A[I 2025-01-21 22:03:05,374] Trial 33 finished with value: 0.43073157220510333 and parameters: {'learning_rate': 0.0004942180756685263, 'weight_decay': 0.0009429169716292906}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 68%|██████▊   | 34/50 [01:19<00:38,  2.41s/it]\u001B[A[I 2025-01-21 22:03:07,719] Trial 34 finished with value: 0.4287923692137244 and parameters: {'learning_rate': 0.00014050448548477077, 'weight_decay': 0.0002030825753430492}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 70%|███████   | 35/50 [01:22<00:35,  2.39s/it]\u001B[A[I 2025-01-21 22:03:09,957] Trial 35 finished with value: 0.4323214347540076 and parameters: {'learning_rate': 0.0003609567178292953, 'weight_decay': 0.00038143781068898045}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 72%|███████▏  | 36/50 [01:24<00:32,  2.34s/it]\u001B[A[I 2025-01-21 22:03:12,225] Trial 36 finished with value: 0.43073150296021884 and parameters: {'learning_rate': 0.0005885469186161942, 'weight_decay': 0.00012895661647631314}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 74%|███████▍  | 37/50 [01:26<00:30,  2.32s/it]\u001B[A[I 2025-01-21 22:03:14,505] Trial 37 finished with value: 0.43226319980611433 and parameters: {'learning_rate': 8.724758239352564e-05, 'weight_decay': 0.0007322169840473391}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 76%|███████▌  | 38/50 [01:28<00:27,  2.31s/it]\u001B[A[I 2025-01-21 22:03:16,772] Trial 38 finished with value: 0.4337923692137243 and parameters: {'learning_rate': 0.00022834866242080057, 'weight_decay': 9.490494773375408e-05}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 78%|███████▊  | 39/50 [01:31<00:25,  2.30s/it]\u001B[A[I 2025-01-21 22:03:19,043] Trial 39 finished with value: 0.43467527611397705 and parameters: {'learning_rate': 9.765833728486732e-05, 'weight_decay': 0.00025761677370011657}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 80%|████████  | 40/50 [01:33<00:22,  2.29s/it]\u001B[A[I 2025-01-21 22:03:21,346] Trial 40 finished with value: 0.36924097219817886 and parameters: {'learning_rate': 2.833938255509133e-05, 'weight_decay': 0.0005217323737596078}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 82%|████████▏ | 41/50 [01:35<00:20,  2.29s/it]\u001B[A[I 2025-01-21 22:03:23,842] Trial 41 finished with value: 0.4302627843368071 and parameters: {'learning_rate': 9.67117254923833e-05, 'weight_decay': 0.00026778681953361675}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 84%|████████▍ | 42/50 [01:38<00:18,  2.35s/it]\u001B[A[I 2025-01-21 22:03:26,198] Trial 42 finished with value: 0.42796807810822973 and parameters: {'learning_rate': 7.207268733305958e-05, 'weight_decay': 0.00018850647262903675}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 86%|████████▌ | 43/50 [01:40<00:16,  2.35s/it]\u001B[A[I 2025-01-21 22:03:28,514] Trial 43 finished with value: 0.4336753107364194 and parameters: {'learning_rate': 0.00017092753602121716, 'weight_decay': 7.025535508327895e-05}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 88%|████████▊ | 44/50 [01:42<00:14,  2.34s/it]\u001B[A[I 2025-01-21 22:03:30,771] Trial 44 finished with value: 0.4171993214001315 and parameters: {'learning_rate': 3.578140509385224e-05, 'weight_decay': 0.0003888590108262663}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 90%|█████████ | 45/50 [01:45<00:11,  2.32s/it]\u001B[A[I 2025-01-21 22:03:33,046] Trial 45 finished with value: 0.43479278468303156 and parameters: {'learning_rate': 0.00013597659805370985, 'weight_decay': 0.00012262349113553308}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 92%|█████████▏| 46/50 [01:47<00:09,  2.30s/it]\u001B[A[I 2025-01-21 22:03:35,296] Trial 46 finished with value: 0.4279687705570751 and parameters: {'learning_rate': 0.00010428299313384441, 'weight_decay': 0.00012103565956637149}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 94%|█████████▍| 47/50 [01:49<00:06,  2.29s/it]\u001B[A[I 2025-01-21 22:03:37,530] Trial 47 finished with value: 0.43314565661461757 and parameters: {'learning_rate': 0.00013965312984150829, 'weight_decay': 0.000816249820743334}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 96%|█████████▌| 48/50 [01:51<00:04,  2.27s/it]\u001B[A[I 2025-01-21 22:03:39,816] Trial 48 finished with value: 0.42420371845029947 and parameters: {'learning_rate': 7.076000029133539e-05, 'weight_decay': 0.00016668519260117052}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      " 98%|█████████▊| 49/50 [01:54<00:02,  2.28s/it]\u001B[A[I 2025-01-21 22:03:42,103] Trial 49 finished with value: 0.43291005089499013 and parameters: {'learning_rate': 0.00015981348004880016, 'weight_decay': 1.184809807991198e-05}. Best is trial 11 with value: 0.4349104663642973.\n",
      "\n",
      "100%|██████████| 50/50 [01:56<00:00,  2.33s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for logistic_regression: {'learning_rate': 8.907800526083353e-05, 'weight_decay': 0.0005285481392211958}\n",
      "Best F1 score for logistic_regression: 0.4349104663642973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr_results['bert_with_augmentation'] = optimize_model('logistic_regression', X_bert_with_augmentation, y_bert_with_augmentation)\n",
    "lr_results['tfidf_without_augmentation'] = optimize_model('logistic_regression', X_tfidf_no_augmentation, y_tfidf_no_augmentation)\n",
    "lr_results['tfidf_with_augmentation'] = optimize_model('logistic_regression', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression results:\n",
      "\n",
      "\n",
      "Using BERT embeddings without augmentation scores: [[0.5412844036697247, 0.5244648318042814, 0.5198776758409785, 0.5229357798165137, 0.5275229357798165, 0.5152905198776758, 0.5137614678899083, 0.5084226646248086, 0.5451761102603369, 0.557427258805513], [0.5214067278287462, 0.5351681957186545, 0.5305810397553516, 0.5458715596330275, 0.5764525993883792, 0.5305810397553516, 0.5137614678899083, 0.5176110260336907, 0.5084226646248086, 0.5604900459418071], [0.30428134556574926, 0.45718654434250766, 0.4877675840978593, 0.5275229357798165, 0.5504587155963303, 0.5535168195718655, 0.536697247706422, 0.5191424196018377, 0.5313935681470138, 0.5788667687595712], [0.45718654434250766, 0.5275229357798165, 0.5336391437308868, 0.5458715596330275, 0.5642201834862385, 0.5565749235474006, 0.5214067278287462, 0.5329249617151608, 0.5436447166921899, 0.5482388973966309], [0.5259938837920489, 0.5688073394495413, 0.5489296636085627, 0.5290519877675841, 0.5489296636085627, 0.5474006116207951, 0.5305810397553516, 0.5053598774885145, 0.49770290964777947, 0.5513016845329249], [0.6681957186544343, 0.6269113149847095, 0.6024464831804281, 0.5902140672782875, 0.5932721712538226, 0.5749235474006116, 0.555045871559633, 0.5497702909647779, 0.5375191424196019, 0.5666156202143952], [0.5382262996941896, 0.5489296636085627, 0.5397553516819572, 0.5351681957186545, 0.5412844036697247, 0.5244648318042814, 0.5259938837920489, 0.5467075038284839, 0.5482388973966309, 0.5451761102603369], [0.25535168195718655, 0.4694189602446483, 0.5091743119266054, 0.5259938837920489, 0.5229357798165137, 0.5229357798165137, 0.5290519877675841, 0.4992343032159265, 0.5375191424196019, 0.5359877488514548], [0.4847094801223242, 0.5504587155963303, 0.5749235474006116, 0.5474006116207951, 0.5443425076452599, 0.5474006116207951, 0.5397553516819572, 0.5436447166921899, 0.5666156202143952, 0.5482388973966309], [0.5351681957186545, 0.5672782874617737, 0.5672782874617737, 0.5581039755351682, 0.5642201834862385, 0.5489296636085627, 0.5397553516819572, 0.5467075038284839, 0.5405819295558959, 0.5283307810107197], [0.5091743119266054, 0.5107033639143731, 0.5397553516819572, 0.5152905198776758, 0.5489296636085627, 0.5351681957186545, 0.5229357798165137, 0.5114854517611026, 0.5130168453292496, 0.5405819295558959], [0.5474006116207951, 0.5305810397553516, 0.5275229357798165, 0.5428134556574924, 0.5458715596330275, 0.5321100917431193, 0.5275229357798165, 0.5145482388973966, 0.5283307810107197, 0.5283307810107197], [0.5244648318042814, 0.5259938837920489, 0.5657492354740061, 0.5611620795107034, 0.5428134556574924, 0.5382262996941896, 0.5305810397553516, 0.5344563552833078, 0.5053598774885145, 0.5160796324655437], [0.5076452599388379, 0.5397553516819572, 0.5107033639143731, 0.5489296636085627, 0.5703363914373089, 0.5168195718654435, 0.5458715596330275, 0.5252679938744257, 0.5451761102603369, 0.552833078101072], [0.5840978593272171, 0.555045871559633, 0.555045871559633, 0.518348623853211, 0.518348623853211, 0.5275229357798165, 0.5275229357798165, 0.5099540581929556, 0.5298621745788668, 0.5252679938744257], [0.4984709480122324, 0.5137614678899083, 0.5076452599388379, 0.5076452599388379, 0.5412844036697247, 0.5244648318042814, 0.5443425076452599, 0.5222052067381318, 0.5298621745788668, 0.5007656967840735], [0.599388379204893, 0.5535168195718655, 0.5749235474006116, 0.5504587155963303, 0.5519877675840978, 0.5152905198776758, 0.5458715596330275, 0.5283307810107197, 0.5237366003062787, 0.5053598774885145], [0.3440366972477064, 0.4617737003058104, 0.5519877675840978, 0.5596330275229358, 0.573394495412844, 0.5886850152905199, 0.5565749235474006, 0.5681470137825421, 0.5635528330781011, 0.5604900459418071], [0.5657492354740061, 0.581039755351682, 0.5856269113149847, 0.5718654434250765, 0.5642201834862385, 0.5626911314984709, 0.5489296636085627, 0.5635528330781011, 0.5436447166921899, 0.5911179173047473], [0.4327217125382263, 0.518348623853211, 0.555045871559633, 0.5305810397553516, 0.5565749235474006, 0.5382262996941896, 0.5565749235474006, 0.5237366003062787, 0.5329249617151608, 0.5298621745788668], [0.2217125382262997, 0.41131498470948014, 0.4801223241590214, 0.5198776758409785, 0.5489296636085627, 0.5688073394495413, 0.5474006116207951, 0.552833078101072, 0.5604900459418071, 0.5513016845329249], [0.5382262996941896, 0.5443425076452599, 0.5443425076452599, 0.5397553516819572, 0.5474006116207951, 0.5290519877675841, 0.5214067278287462, 0.5252679938744257, 0.5191424196018377, 0.5329249617151608], [0.573394495412844, 0.5611620795107034, 0.581039755351682, 0.5642201834862385, 0.5688073394495413, 0.5535168195718655, 0.5290519877675841, 0.5421133231240429, 0.5022970903522205, 0.5497702909647779], [0.5458715596330275, 0.5535168195718655, 0.5168195718654435, 0.5657492354740061, 0.5795107033639144, 0.573394495412844, 0.5565749235474006, 0.5421133231240429, 0.5421133231240429, 0.557427258805513], [0.3256880733944954, 0.6131498470948012, 0.6024464831804281, 0.5840978593272171, 0.6070336391437309, 0.5412844036697247, 0.5688073394495413, 0.5712098009188361, 0.5467075038284839, 0.555895865237366], [0.5290519877675841, 0.5565749235474006, 0.5244648318042814, 0.5229357798165137, 0.5611620795107034, 0.5443425076452599, 0.5504587155963303, 0.5359877488514548, 0.5206738131699847, 0.5298621745788668], [0.5963302752293578, 0.6116207951070336, 0.591743119266055, 0.5596330275229358, 0.5565749235474006, 0.5626911314984709, 0.555045871559633, 0.5497702909647779, 0.55895865237366, 0.5497702909647779], [0.5611620795107034, 0.5489296636085627, 0.5397553516819572, 0.536697247706422, 0.5489296636085627, 0.5474006116207951, 0.5336391437308868, 0.5160796324655437, 0.5375191424196019, 0.5359877488514548], [0.5397553516819572, 0.555045871559633, 0.5519877675840978, 0.555045871559633, 0.5397553516819572, 0.5443425076452599, 0.5458715596330275, 0.5436447166921899, 0.5497702909647779, 0.552833078101072], [0.5458715596330275, 0.536697247706422, 0.5382262996941896, 0.5412844036697247, 0.5290519877675841, 0.5137614678899083, 0.5458715596330275, 0.5191424196018377, 0.5283307810107197, 0.5436447166921899], [0.5688073394495413, 0.5504587155963303, 0.5397553516819572, 0.555045871559633, 0.5489296636085627, 0.5412844036697247, 0.5321100917431193, 0.5053598774885145, 0.5068912710566615, 0.5359877488514548], [0.5764525993883792, 0.5688073394495413, 0.5474006116207951, 0.5749235474006116, 0.5474006116207951, 0.5519877675840978, 0.5428134556574924, 0.5359877488514548, 0.5635528330781011, 0.5130168453292496], [0.5856269113149847, 0.5718654434250765, 0.5703363914373089, 0.5581039755351682, 0.573394495412844, 0.5657492354740061, 0.5596330275229358, 0.5650842266462481, 0.552833078101072, 0.5727411944869831], [0.4648318042813456, 0.5443425076452599, 0.5229357798165137, 0.5504587155963303, 0.5626911314984709, 0.5382262996941896, 0.5535168195718655, 0.5222052067381318, 0.5252679938744257, 0.5421133231240429], [0.4801223241590214, 0.5290519877675841, 0.5565749235474006, 0.5565749235474006, 0.5305810397553516, 0.5504587155963303, 0.5596330275229358, 0.5421133231240429, 0.5436447166921899, 0.5267993874425727], [0.6131498470948012, 0.6055045871559633, 0.5626911314984709, 0.5978593272171254, 0.5718654434250765, 0.5886850152905199, 0.5642201834862385, 0.5650842266462481, 0.5497702909647779, 0.55895865237366], [0.5259938837920489, 0.5581039755351682, 0.5412844036697247, 0.5458715596330275, 0.5825688073394495, 0.5672782874617737, 0.5091743119266054, 0.5237366003062787, 0.5099540581929556, 0.5513016845329249], [0.555045871559633, 0.5749235474006116, 0.5351681957186545, 0.5259938837920489, 0.5519877675840978, 0.5596330275229358, 0.5458715596330275, 0.5329249617151608, 0.5313935681470138, 0.5405819295558959], [0.26299694189602446, 0.463302752293578, 0.5321100917431193, 0.5504587155963303, 0.5397553516819572, 0.5443425076452599, 0.5412844036697247, 0.5421133231240429, 0.555895865237366, 0.5635528330781011], [0.5152905198776758, 0.581039755351682, 0.5504587155963303, 0.5489296636085627, 0.5749235474006116, 0.5489296636085627, 0.5259938837920489, 0.5620214395099541, 0.5222052067381318, 0.5344563552833078], [0.591743119266055, 0.5840978593272171, 0.5718654434250765, 0.5519877675840978, 0.5672782874617737, 0.5642201834862385, 0.5474006116207951, 0.5497702909647779, 0.5298621745788668, 0.552833078101072], [0.5749235474006116, 0.5718654434250765, 0.5703363914373089, 0.5779816513761468, 0.5703363914373089, 0.5703363914373089, 0.5428134556574924, 0.557427258805513, 0.5666156202143952, 0.552833078101072], [0.30428134556574926, 0.5504587155963303, 0.5764525993883792, 0.5688073394495413, 0.5626911314984709, 0.5672782874617737, 0.5412844036697247, 0.5758039816232772, 0.5696784073506891, 0.552833078101072], [0.4908256880733945, 0.5382262996941896, 0.5474006116207951, 0.5703363914373089, 0.5672782874617737, 0.5611620795107034, 0.5581039755351682, 0.5497702909647779, 0.5467075038284839, 0.557427258805513], [0.5443425076452599, 0.5657492354740061, 0.5840978593272171, 0.5779816513761468, 0.5688073394495413, 0.5718654434250765, 0.5504587155963303, 0.5696784073506891, 0.557427258805513, 0.5681470137825421], [0.555045871559633, 0.5565749235474006, 0.5703363914373089, 0.5840978593272171, 0.5657492354740061, 0.5703363914373089, 0.5718654434250765, 0.5390505359877489, 0.55895865237366, 0.555895865237366], [0.5948012232415902, 0.5596330275229358, 0.5642201834862385, 0.5535168195718655, 0.5458715596330275, 0.536697247706422, 0.5107033639143731, 0.5497702909647779, 0.5191424196018377, 0.5375191424196019], [0.45718654434250766, 0.5336391437308868, 0.5244648318042814, 0.5596330275229358, 0.5305810397553516, 0.5642201834862385, 0.5137614678899083, 0.5436447166921899, 0.5160796324655437, 0.5436447166921899], [0.5412844036697247, 0.5275229357798165, 0.5152905198776758, 0.5091743119266054, 0.5443425076452599, 0.5259938837920489, 0.5198776758409785, 0.5022970903522205, 0.49617151607963245, 0.5007656967840735], [0.6529051987767585, 0.6299694189602446, 0.6146788990825688, 0.6055045871559633, 0.591743119266055, 0.581039755351682, 0.5688073394495413, 0.5604900459418071, 0.5482388973966309, 0.5405819295558959]]\n",
      "Using BERT embeddings without augmentation best score: 0.5893959190937148\n",
      "Using BERT embeddings without augmentation best parameters: {'learning_rate': 1.639303640706943e-05, 'weight_decay': 8.202639343676631e-05}\n",
      "\n",
      "\n",
      "Using BERT embeddings with augmentation scores: [[0.36176470588235293, 0.3847058823529412, 0.34196586227192466, 0.3584461447910536, 0.3431430253090053, 0.3478516774573278, 0.36433195997645673, 0.3443201883460859, 0.3549146556798117, 0.3272513243084167], [0.35352941176470587, 0.34411764705882353, 0.3437316068275456, 0.3466745144202472, 0.3331371394938199, 0.353737492642731, 0.3584461447910536, 0.3496174220129488, 0.34196586227192466, 0.33902295467922305], [0.3658823529411765, 0.36764705882352944, 0.37021777516185994, 0.3543260741612713, 0.3325485579752796, 0.3696291936433196, 0.3372572101236021, 0.3590347263095939, 0.3572689817539729, 0.3507945850500294], [0.35705882352941176, 0.32, 0.3366686286050618, 0.35609181871689227, 0.36021188934667453, 0.36021188934667453, 0.3549146556798117, 0.3761035903472631, 0.3360800470865215, 0.3725721012360212], [0.37176470588235294, 0.38176470588235295, 0.34196586227192466, 0.35197174808711007, 0.3731606827545615, 0.3502060035314891, 0.3272513243084167, 0.3472630959387875, 0.3360800470865215, 0.33902295467922305], [0.34, 0.34941176470588237, 0.355503237198352, 0.355503237198352, 0.3402001177163037, 0.3596233078281342, 0.37845791642142435, 0.3478516774573278, 0.36727486756915834, 0.3301942319011183], [0.3658823529411765, 0.35411764705882354, 0.34844025897586817, 0.3466745144202472, 0.3507945850500294, 0.3431430253090053, 0.36021188934667453, 0.3507945850500294, 0.3531489111241907, 0.3443201883460859], [0.31176470588235294, 0.3341176470588235, 0.33549146556798115, 0.36080047086521483, 0.3331371394938199, 0.35197174808711007, 0.355503237198352, 0.3237198351971748, 0.3619776339022955, 0.3443201883460859], [0.3588235294117647, 0.34941176470588237, 0.34137728075338436, 0.3549146556798117, 0.34490876986462626, 0.3596233078281342, 0.3578575632725132, 0.3549146556798117, 0.37139493819894054, 0.37374926427310184], [0.3552941176470588, 0.3447058823529412, 0.3690406121247793, 0.3619776339022955, 0.36727486756915834, 0.3613890523837551, 0.34844025897586817, 0.3531489111241907, 0.3590347263095939, 0.3507945850500294], [0.37823529411764706, 0.34411764705882353, 0.35197174808711007, 0.3590347263095939, 0.32842848734549734, 0.3596233078281342, 0.33843437316068276, 0.34255444379046496, 0.3690406121247793, 0.3337257210123602], [0.36058823529411765, 0.3476470588235294, 0.3507945850500294, 0.3619776339022955, 0.3225426721600942, 0.33431430253090055, 0.34196586227192466, 0.36492054149499703, 0.3472630959387875, 0.3472630959387875], [0.3247058823529412, 0.3552941176470588, 0.3613890523837551, 0.3549146556798117, 0.36492054149499703, 0.340788699234844, 0.3543260741612713, 0.3396115361977634, 0.36668628605061804, 0.3578575632725132], [0.34941176470588237, 0.3511764705882353, 0.3584461447910536, 0.34844025897586817, 0.3572689817539729, 0.37374926427310184, 0.3655091230135374, 0.33431430253090055, 0.35138316656856977, 0.35138316656856977], [0.36176470588235293, 0.36705882352941177, 0.34255444379046496, 0.3543260741612713, 0.35256032960565037, 0.33549146556798115, 0.3619776339022955, 0.34255444379046496, 0.353737492642731, 0.33843437316068276], [0.35411764705882354, 0.35411764705882354, 0.3466745144202472, 0.35197174808711007, 0.3472630959387875, 0.33784579164214246, 0.32901706886403764, 0.3337257210123602, 0.3496174220129488, 0.34490876986462626], [0.3382352941176471, 0.3652941176470588, 0.3584461447910536, 0.34255444379046496, 0.3437316068275456, 0.3331371394938199, 0.35256032960565037, 0.3443201883460859, 0.36492054149499703, 0.3466745144202472], [0.35352941176470587, 0.3458823529411765, 0.3507945850500294, 0.3402001177163037, 0.3360800470865215, 0.3496174220129488, 0.35138316656856977, 0.33195997645673925, 0.34844025897586817, 0.34255444379046496], [0.34705882352941175, 0.35, 0.36786344908769864, 0.35197174808711007, 0.34844025897586817, 0.36727486756915834, 0.37021777516185994, 0.3613890523837551, 0.3366686286050618, 0.3437316068275456], [0.3688235294117647, 0.3423529411764706, 0.3584461447910536, 0.3660977045320777, 0.3613890523837551, 0.3472630959387875, 0.37433784579164214, 0.3543260741612713, 0.33490288404944085, 0.34490876986462626], [0.34, 0.35352941176470587, 0.3625662154208358, 0.3496174220129488, 0.3502060035314891, 0.33902295467922305, 0.3466745144202472, 0.36727486756915834, 0.36021188934667453, 0.3755150088287228], [0.3488235294117647, 0.3382352941176471, 0.3472630959387875, 0.36021188934667453, 0.3372572101236021, 0.34196586227192466, 0.33137139493819895, 0.3266627427898764, 0.34490876986462626, 0.34608593290170686], [0.37, 0.34352941176470586, 0.3466745144202472, 0.3431430253090053, 0.35609181871689227, 0.3631547969393761, 0.33549146556798115, 0.3396115361977634, 0.3502060035314891, 0.33784579164214246], [0.34352941176470586, 0.35294117647058826, 0.35256032960565037, 0.33549146556798115, 0.3402001177163037, 0.3543260741612713, 0.3360800470865215, 0.3472630959387875, 0.3625662154208358, 0.35256032960565037], [0.36470588235294116, 0.34705882352941175, 0.3402001177163037, 0.3366686286050618, 0.34902884049440847, 0.36433195997645673, 0.3443201883460859, 0.3502060035314891, 0.3472630959387875, 0.3478516774573278], [0.3723529411764706, 0.3423529411764706, 0.34608593290170686, 0.3590347263095939, 0.3472630959387875, 0.3360800470865215, 0.33902295467922305, 0.3372572101236021, 0.329605650382578, 0.33137139493819895], [0.33294117647058824, 0.36176470588235293, 0.3584461447910536, 0.34608593290170686, 0.34549735138316656, 0.36021188934667453, 0.3496174220129488, 0.3572689817539729, 0.3502060035314891, 0.3660977045320777], [0.35764705882352943, 0.3517647058823529, 0.3437316068275456, 0.3578575632725132, 0.3690406121247793, 0.3372572101236021, 0.3613890523837551, 0.33078281341965865, 0.3543260741612713, 0.3549146556798117], [0.33294117647058824, 0.33941176470588236, 0.3625662154208358, 0.353737492642731, 0.33902295467922305, 0.35256032960565037, 0.35197174808711007, 0.36021188934667453, 0.35609181871689227, 0.3761035903472631], [0.33941176470588236, 0.3411764705882353, 0.34196586227192466, 0.37021777516185994, 0.35256032960565037, 0.3572689817539729, 0.3472630959387875, 0.33195997645673925, 0.3655091230135374, 0.3472630959387875], [0.35764705882352943, 0.3517647058823529, 0.3325485579752796, 0.3472630959387875, 0.3366686286050618, 0.3372572101236021, 0.35138316656856977, 0.33078281341965865, 0.34902884049440847, 0.3613890523837551], [0.36, 0.3588235294117647, 0.3472630959387875, 0.355503237198352, 0.33078281341965865, 0.3590347263095939, 0.3619776339022955, 0.34490876986462626, 0.3478516774573278, 0.353737492642731], [0.3488235294117647, 0.3476470588235294, 0.3766921718658034, 0.36786344908769864, 0.3372572101236021, 0.36080047086521483, 0.34196586227192466, 0.36668628605061804, 0.3549146556798117, 0.36433195997645673], [0.35823529411764704, 0.3558823529411765, 0.3478516774573278, 0.3596233078281342, 0.3619776339022955, 0.34549735138316656, 0.3572689817539729, 0.33137139493819895, 0.33549146556798115, 0.3596233078281342], [0.3382352941176471, 0.3476470588235294, 0.3431430253090053, 0.3590347263095939, 0.3402001177163037, 0.33902295467922305, 0.34844025897586817, 0.33843437316068276, 0.3549146556798117, 0.3584461447910536], [0.36176470588235293, 0.34941176470588237, 0.36433195997645673, 0.35197174808711007, 0.3478516774573278, 0.33784579164214246, 0.34549735138316656, 0.37374926427310184, 0.34902884049440847, 0.33549146556798115], [0.36058823529411765, 0.36941176470588233, 0.3472630959387875, 0.3402001177163037, 0.36021188934667453, 0.36080047086521483, 0.3578575632725132, 0.35138316656856977, 0.33490288404944085, 0.3472630959387875], [0.3458823529411765, 0.3417647058823529, 0.3466745144202472, 0.32136550912301354, 0.340788699234844, 0.33549146556798115, 0.34902884049440847, 0.3613890523837551, 0.34902884049440847, 0.3502060035314891], [0.36, 0.3611764705882353, 0.35256032960565037, 0.3719835197174809, 0.34902884049440847, 0.3396115361977634, 0.3502060035314891, 0.35609181871689227, 0.34490876986462626, 0.340788699234844], [0.3552941176470588, 0.36, 0.3507945850500294, 0.3590347263095939, 0.3431430253090053, 0.36492054149499703, 0.3496174220129488, 0.3360800470865215, 0.3502060035314891, 0.3443201883460859], [0.37529411764705883, 0.3729411764705882, 0.3531489111241907, 0.355503237198352, 0.3619776339022955, 0.3195997645673926, 0.3625662154208358, 0.3631547969393761, 0.36080047086521483, 0.35138316656856977], [0.36058823529411765, 0.36, 0.33902295467922305, 0.37080635668040024, 0.3337257210123602, 0.3443201883460859, 0.3331371394938199, 0.3260741612713361, 0.3396115361977634, 0.3584461447910536], [0.36470588235294116, 0.3423529411764706, 0.33902295467922305, 0.35609181871689227, 0.35256032960565037, 0.33078281341965865, 0.34255444379046496, 0.3272513243084167, 0.3549146556798117, 0.3719835197174809], [0.3382352941176471, 0.3552941176470588, 0.3478516774573278, 0.3590347263095939, 0.34490876986462626, 0.3596233078281342, 0.3507945850500294, 0.36492054149499703, 0.34490876986462626, 0.35197174808711007], [0.3594117647058824, 0.3564705882352941, 0.3496174220129488, 0.3531489111241907, 0.37492642731018244, 0.329605650382578, 0.3549146556798117, 0.36492054149499703, 0.35609181871689227, 0.33843437316068276], [0.35764705882352943, 0.3270588235294118, 0.3596233078281342, 0.3437316068275456, 0.368452030606239, 0.35138316656856977, 0.35256032960565037, 0.33431430253090055, 0.3366686286050618, 0.3266627427898764], [0.3564705882352941, 0.3564705882352941, 0.37374926427310184, 0.36433195997645673, 0.33902295467922305, 0.34490876986462626, 0.33784579164214246, 0.37080635668040024, 0.3496174220129488, 0.35197174808711007], [0.33941176470588236, 0.3376470588235294, 0.3443201883460859, 0.3549146556798117, 0.35197174808711007, 0.353737492642731, 0.35609181871689227, 0.33137139493819895, 0.34902884049440847, 0.36374337845791643], [0.3423529411764706, 0.3547058823529412, 0.37728075338434375, 0.36433195997645673, 0.3402001177163037, 0.3613890523837551, 0.3572689817539729, 0.3431430253090053, 0.35197174808711007, 0.3301942319011183], [0.3564705882352941, 0.3517647058823529, 0.37139493819894054, 0.34902884049440847, 0.34255444379046496, 0.3625662154208358, 0.3549146556798117, 0.3402001177163037, 0.35609181871689227, 0.35609181871689227]]\n",
      "Using BERT embeddings with augmentation best score: 0.35763694907038746\n",
      "Using BERT embeddings with augmentation best parameters: {'learning_rate': 0.0002738246363978017, 'weight_decay': 1.3118316853159452e-05}\n",
      "\n",
      "\n",
      "Using TF-IDF embeddings without augmentation scores: [[0.3761467889908257, 0.4418960244648318, 0.4602446483180428, 0.44801223241590216, 0.4954128440366973, 0.5244648318042814, 0.536697247706422, 0.5941807044410413, 0.5451761102603369, 0.5819295558958653], [0.6758409785932722, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.678407350689127, 0.6799387442572741], [0.22629969418960244, 0.3562691131498471, 0.5045871559633027, 0.5779816513761468, 0.6422018348623854, 0.6712538226299695, 0.672782874617737, 0.6814701378254211, 0.6799387442572741, 0.6768759571209801], [0.1651376146788991, 0.19877675840978593, 0.25688073394495414, 0.2798165137614679, 0.37155963302752293, 0.481651376146789, 0.5489296636085627, 0.5911179173047473, 0.6385911179173047, 0.663093415007657], [0.5963302752293578, 0.6788990825688074, 0.6819571865443425, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.5932721712538226, 0.6070336391437309, 0.6437308868501529, 0.6605504587155964, 0.6712538226299695, 0.6758409785932722, 0.6788990825688074, 0.6799387442572741, 0.6799387442572741, 0.678407350689127], [0.654434250764526, 0.6773700305810397, 0.6804281345565749, 0.6788990825688074, 0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6799387442572741, 0.6799387442572741, 0.6799387442572741], [0.6743119266055045, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.581039755351682, 0.6498470948012233, 0.6712538226299695, 0.6758409785932722, 0.6773700305810397, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.3470948012232416, 0.5030581039755352, 0.6299694189602446, 0.654434250764526, 0.6620795107033639, 0.6788990825688074, 0.6773700305810397, 0.6799387442572741, 0.6799387442572741, 0.678407350689127], [0.29510703363914376, 0.32262996941896027, 0.3730886850152905, 0.3837920489296636, 0.45871559633027525, 0.4984709480122324, 0.5244648318042814, 0.5742725880551302, 0.5666156202143952, 0.6171516079632465], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.678407350689127, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6799387442572741, 0.678407350689127, 0.6768759571209801], [0.6162079510703364, 0.6743119266055045, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.678407350689127], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.654434250764526, 0.6743119266055045, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6452599388379205, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6830015313935681, 0.6799387442572741, 0.6799387442572741], [0.24617737003058104, 0.29510703363914376, 0.2691131498470948, 0.3287461773700306, 0.40061162079510704, 0.44954128440366975, 0.4648318042813456, 0.4946401225114854, 0.5344563552833078, 0.5773353751914242], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6799387442572741, 0.6799387442572741, 0.6799387442572741], [0.3333333333333333, 0.5229357798165137, 0.6376146788990825, 0.6697247706422018, 0.6743119266055045, 0.6773700305810397, 0.6819571865443425, 0.6799387442572741, 0.6799387442572741, 0.6814701378254211], [0.43883792048929665, 0.6590214067278287, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.678407350689127, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.678407350689127, 0.678407350689127], [0.6391437308868502, 0.6758409785932722, 0.6758409785932722, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6830015313935681, 0.6814701378254211, 0.6799387442572741], [0.6758409785932722, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6819571865443425, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.45718654434250766, 0.6559633027522935, 0.6743119266055045, 0.6788990825688074, 0.6758409785932722, 0.6804281345565749, 0.6788990825688074, 0.6799387442572741, 0.678407350689127, 0.678407350689127], [0.6697247706422018, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.3853211009174312, 0.6162079510703364, 0.6651376146788991, 0.6758409785932722, 0.6819571865443425, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.16819571865443425, 0.172782874617737, 0.191131498470948, 0.18960244648318042, 0.19724770642201836, 0.20336391437308868, 0.20795107033639143, 0.2113323124042879, 0.22970903522205208, 0.26952526799387444], [0.42201834862385323, 0.6529051987767585, 0.6712538226299695, 0.6758409785932722, 0.6758409785932722, 0.6804281345565749, 0.6804281345565749, 0.6799387442572741, 0.678407350689127, 0.6799387442572741], [0.6743119266055045, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6712538226299695, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.5015290519877675, 0.6681957186544343, 0.672782874617737, 0.6758409785932722, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6467889908256881, 0.672782874617737, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.25382262996941896, 0.3119266055045872, 0.43883792048929665, 0.5030581039755352, 0.6009174311926605, 0.636085626911315, 0.6467889908256881, 0.667687595712098, 0.6707503828483921, 0.6768759571209801], [0.6758409785932722, 0.6773700305810397, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6070336391437309, 0.6758409785932722, 0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.678407350689127, 0.6799387442572741], [0.6773700305810397, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.3746177370030581, 0.5688073394495413, 0.6483180428134556, 0.6758409785932722, 0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.40825688073394495, 0.5489296636085627, 0.5871559633027523, 0.6452599388379205, 0.672782874617737, 0.6697247706422018, 0.6819571865443425, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6758409785932722, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.47706422018348627, 0.5657492354740061, 0.6055045871559633, 0.6345565749235474, 0.6743119266055045, 0.6697247706422018, 0.6788990825688074, 0.6753445635528331, 0.6768759571209801, 0.6799387442572741], [0.6574923547400612, 0.672782874617737, 0.6788990825688074, 0.6773700305810397, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6391437308868502, 0.6788990825688074, 0.6758409785932722, 0.6788990825688074, 0.6773700305810397, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6758409785932722, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6681957186544343, 0.6773700305810397, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741]]\n",
      "Using TF-IDF embeddings without augmentation best score: 0.6805873620223762\n",
      "Using TF-IDF embeddings without augmentation best parameters: {'learning_rate': 0.00035128291989658535, 'weight_decay': 0.0004388332527791302}\n",
      "\n",
      "\n",
      "Using TF-IDF embeddings with augmentation scores: [[0.43176470588235294, 0.4294117647058823, 0.4290759270158917, 0.4331959976456739, 0.4355503237198352, 0.4326074161271336, 0.43143025309005295, 0.43496174220129485, 0.43496174220129485, 0.43437316068275456], [0.43529411764705883, 0.43529411764705883, 0.43496174220129485, 0.43496174220129485, 0.4320188346085933, 0.42789876397881105, 0.43790464979399646, 0.4261330194231901, 0.40965273690406123, 0.42319011183048855], [0.32, 0.3352941176470588, 0.3719835197174809, 0.36021188934667453, 0.37021777516185994, 0.379635079458505, 0.3931724543849323, 0.38434373160682755, 0.39552678045909356, 0.41024131842260153], [0.43529411764705883, 0.43529411764705883, 0.43496174220129485, 0.43437316068275456, 0.43084167157151265, 0.43437316068275456, 0.43378457916421426, 0.4331959976456739, 0.42789876397881105, 0.41671571512654504], [0.3982352941176471, 0.4223529411764706, 0.4326074161271336, 0.43437316068275456, 0.42789876397881105, 0.4367274867569158, 0.4355503237198352, 0.4367274867569158, 0.43437316068275456, 0.43143025309005295], [0.3164705882352941, 0.32294117647058823, 0.3190111830488523, 0.31842260153031193, 0.34608593290170686, 0.33431430253090055, 0.33784579164214246, 0.34137728075338436, 0.3725721012360212, 0.37845791642142435], [0.38235294117647056, 0.41294117647058826, 0.4049440847557387, 0.429664508534432, 0.43084167157151265, 0.43437316068275456, 0.4355503237198352, 0.43437316068275456, 0.43496174220129485, 0.4331959976456739], [0.2776470588235294, 0.29058823529411765, 0.28958210712183635, 0.29899941141848146, 0.3231312536786345, 0.36374337845791643, 0.36668628605061804, 0.3725721012360212, 0.38434373160682755, 0.4002354326074161], [0.43529411764705883, 0.43470588235294116, 0.43496174220129485, 0.4326074161271336, 0.4355503237198352, 0.43378457916421426, 0.43437316068275456, 0.43437316068275456, 0.42377869334902885, 0.42436727486756914], [0.4335294117647059, 0.43470588235294116, 0.43437316068275456, 0.43378457916421426, 0.43790464979399646, 0.43378457916421426, 0.43496174220129485, 0.4331959976456739, 0.429664508534432, 0.43496174220129485], [0.39647058823529413, 0.43058823529411766, 0.4320188346085933, 0.4355503237198352, 0.43790464979399646, 0.4367274867569158, 0.4355503237198352, 0.43378457916421426, 0.43496174220129485, 0.4331959976456739], [0.4411764705882353, 0.43, 0.43378457916421426, 0.43378457916421426, 0.43496174220129485, 0.4361389052383755, 0.4355503237198352, 0.43496174220129485, 0.43496174220129485, 0.43378457916421426], [0.4335294117647059, 0.43588235294117644, 0.43437316068275456, 0.43378457916421426, 0.43496174220129485, 0.4331959976456739, 0.43496174220129485, 0.43496174220129485, 0.4331959976456739, 0.43496174220129485], [0.42411764705882354, 0.4329411764705882, 0.4355503237198352, 0.43378457916421426, 0.43496174220129485, 0.43437316068275456, 0.43437316068275456, 0.43496174220129485, 0.43437316068275456, 0.4331959976456739], [0.37058823529411766, 0.3982352941176471, 0.43437316068275456, 0.43437316068275456, 0.4331959976456739, 0.4367274867569158, 0.43731606827545616, 0.43437316068275456, 0.4326074161271336, 0.4355503237198352], [0.4147058823529412, 0.4323529411764706, 0.429664508534432, 0.43378457916421426, 0.4384932313125368, 0.43437316068275456, 0.43378457916421426, 0.43437316068275456, 0.4331959976456739, 0.43143025309005295], [0.2958823529411765, 0.35352941176470587, 0.3902295467922307, 0.4161271336080047, 0.4361389052383755, 0.43437316068275456, 0.4384932313125368, 0.43437316068275456, 0.43496174220129485, 0.43496174220129485], [0.42823529411764705, 0.43176470588235294, 0.4355503237198352, 0.4355503237198352, 0.43437316068275456, 0.4320188346085933, 0.43437316068275456, 0.43378457916421426, 0.43437316068275456, 0.4331959976456739], [0.27941176470588236, 0.27588235294117647, 0.28193054738081225, 0.30253090052972337, 0.31430253090052973, 0.31842260153031193, 0.3578575632725132, 0.3584461447910536, 0.3660977045320777, 0.36080047086521483], [0.38294117647058823, 0.41411764705882353, 0.43084167157151265, 0.43084167157151265, 0.4284873454973514, 0.4290759270158917, 0.4331959976456739, 0.4320188346085933, 0.4326074161271336, 0.43496174220129485], [0.4041176470588235, 0.43529411764705883, 0.4331959976456739, 0.43437316068275456, 0.43378457916421426, 0.4331959976456739, 0.4355503237198352, 0.43378457916421426, 0.43496174220129485, 0.43437316068275456], [0.4258823529411765, 0.43588235294117644, 0.4320188346085933, 0.4326074161271336, 0.43378457916421426, 0.43496174220129485, 0.4326074161271336, 0.43437316068275456, 0.4355503237198352, 0.43025309005297235], [0.43588235294117644, 0.43411764705882355, 0.4331959976456739, 0.43437316068275456, 0.4355503237198352, 0.43437316068275456, 0.4326074161271336, 0.43437316068275456, 0.43496174220129485, 0.43378457916421426], [0.43117647058823527, 0.43823529411764706, 0.4320188346085933, 0.429664508534432, 0.42436727486756914, 0.4149499705709241, 0.42377869334902885, 0.4149499705709241, 0.4255444379046498, 0.39493819894055326], [0.43529411764705883, 0.43470588235294116, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.4355503237198352, 0.4355503237198352, 0.4331959976456739, 0.43437316068275456, 0.4331959976456739], [0.42411764705882354, 0.4388235294117647, 0.43437316068275456, 0.43437316068275456, 0.43496174220129485, 0.43437316068275456, 0.4361389052383755, 0.43496174220129485, 0.4320188346085933, 0.4331959976456739], [0.42176470588235293, 0.43117647058823527, 0.4331959976456739, 0.43496174220129485, 0.43437316068275456, 0.4355503237198352, 0.43437316068275456, 0.4355503237198352, 0.43496174220129485, 0.43496174220129485], [0.43117647058823527, 0.4335294117647059, 0.43084167157151265, 0.43378457916421426, 0.43437316068275456, 0.4367274867569158, 0.43143025309005295, 0.43437316068275456, 0.4326074161271336, 0.43084167157151265], [0.4211764705882353, 0.43176470588235294, 0.4361389052383755, 0.4355503237198352, 0.43496174220129485, 0.43496174220129485, 0.4367274867569158, 0.4355503237198352, 0.4331959976456739, 0.4355503237198352], [0.38294117647058823, 0.4188235294117647, 0.4226015303119482, 0.4284873454973514, 0.43378457916421426, 0.43437316068275456, 0.43496174220129485, 0.43496174220129485, 0.4355503237198352, 0.4331959976456739], [0.3352941176470588, 0.35058823529411764, 0.37845791642142435, 0.41965862271924664, 0.41730429664508534, 0.43025309005297235, 0.42731018246027075, 0.4355503237198352, 0.4355503237198352, 0.43143025309005295], [0.43588235294117644, 0.43529411764705883, 0.43378457916421426, 0.43437316068275456, 0.43437316068275456, 0.4331959976456739, 0.43378457916421426, 0.43496174220129485, 0.43084167157151265, 0.429664508534432], [0.4294117647058823, 0.43588235294117644, 0.43437316068275456, 0.4355503237198352, 0.43496174220129485, 0.4355503237198352, 0.43437316068275456, 0.43437316068275456, 0.4355503237198352, 0.4326074161271336], [0.43529411764705883, 0.43529411764705883, 0.4326074161271336, 0.4331959976456739, 0.4320188346085933, 0.43437316068275456, 0.43378457916421426, 0.4290759270158917, 0.41965862271924664, 0.4220129487934079], [0.39294117647058824, 0.42470588235294116, 0.43143025309005295, 0.4355503237198352, 0.4331959976456739, 0.4361389052383755, 0.4331959976456739, 0.43437316068275456, 0.4331959976456739, 0.4331959976456739], [0.4235294117647059, 0.43529411764705883, 0.43025309005297235, 0.43084167157151265, 0.4361389052383755, 0.43025309005297235, 0.43437316068275456, 0.4367274867569158, 0.4331959976456739, 0.4326074161271336], [0.43529411764705883, 0.4364705882352941, 0.4355503237198352, 0.4355503237198352, 0.44143613890523836, 0.4290759270158917, 0.42377869334902885, 0.4326074161271336, 0.4155385520894644, 0.4220129487934079], [0.4164705882352941, 0.43176470588235294, 0.4367274867569158, 0.43731606827545616, 0.4331959976456739, 0.43378457916421426, 0.4361389052383755, 0.43437316068275456, 0.43084167157151265, 0.4320188346085933], [0.4329411764705882, 0.43470588235294116, 0.43496174220129485, 0.43496174220129485, 0.4331959976456739, 0.4355503237198352, 0.43143025309005295, 0.43378457916421426, 0.4331959976456739, 0.4331959976456739], [0.43176470588235294, 0.43529411764705883, 0.43437316068275456, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.4361389052383755, 0.43496174220129485, 0.43437316068275456, 0.43496174220129485], [0.2911764705882353, 0.30470588235294116, 0.3231312536786345, 0.33784579164214246, 0.3731606827545615, 0.3937610359034726, 0.40965273690406123, 0.41730429664508534, 0.4255444379046498, 0.4161271336080047], [0.40352941176470586, 0.43176470588235294, 0.43496174220129485, 0.4290759270158917, 0.43378457916421426, 0.4331959976456739, 0.4355503237198352, 0.4326074161271336, 0.43437316068275456, 0.43378457916421426], [0.39647058823529413, 0.4258823529411765, 0.42672160094173045, 0.43025309005297235, 0.43378457916421426, 0.43437316068275456, 0.43378457916421426, 0.4355503237198352, 0.4320188346085933, 0.43084167157151265], [0.4223529411764706, 0.43411764705882355, 0.4361389052383755, 0.4361389052383755, 0.4361389052383755, 0.43378457916421426, 0.43437316068275456, 0.43437316068275456, 0.4355503237198352, 0.43378457916421426], [0.37588235294117645, 0.4076470588235294, 0.3961153619776339, 0.4226015303119482, 0.42672160094173045, 0.42731018246027075, 0.43084167157151265, 0.4255444379046498, 0.4320188346085933, 0.42731018246027075], [0.4335294117647059, 0.4370588235294118, 0.43437316068275456, 0.4355503237198352, 0.43496174220129485, 0.4355503237198352, 0.4384932313125368, 0.43378457916421426, 0.4326074161271336, 0.4320188346085933], [0.39, 0.42058823529411765, 0.429664508534432, 0.4331959976456739, 0.43143025309005295, 0.43437316068275456, 0.43437316068275456, 0.4367274867569158, 0.43143025309005295, 0.43790464979399646], [0.4188235294117647, 0.4364705882352941, 0.43437316068275456, 0.43496174220129485, 0.43496174220129485, 0.4361389052383755, 0.43496174220129485, 0.4326074161271336, 0.4331959976456739, 0.43496174220129485], [0.35705882352941176, 0.42176470588235293, 0.42789876397881105, 0.4326074161271336, 0.43378457916421426, 0.43378457916421426, 0.43378457916421426, 0.43378457916421426, 0.4331959976456739, 0.43437316068275456], [0.4264705882352941, 0.43176470588235294, 0.43378457916421426, 0.4320188346085933, 0.4326074161271336, 0.43378457916421426, 0.43496174220129485, 0.43378457916421426, 0.4355503237198352, 0.43437316068275456]]\n",
      "Using TF-IDF embeddings with augmentation best score: 0.4349104663642973\n",
      "Using TF-IDF embeddings with augmentation best parameters: {'learning_rate': 8.907800526083353e-05, 'weight_decay': 0.0005285481392211958}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression results:\\n\\n\")\n",
    "print(f\"Using BERT embeddings without augmentation scores: {lr_results['bert_without_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best score: {lr_results['bert_without_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best parameters: {lr_results['bert_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using BERT embeddings with augmentation scores: {lr_results['bert_with_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best score: {lr_results['bert_with_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best parameters: {lr_results['bert_with_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation scores: {lr_results['tfidf_without_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best score: {lr_results['tfidf_without_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best parameters: {lr_results['tfidf_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation scores: {lr_results['tfidf_with_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best score: {lr_results['tfidf_with_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best parameters: {lr_results['tfidf_with_augmentation'][0]}\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 22:03:42,125] A new study created in memory with name: no-name-e7d8686d-d737-4da4-8661-077eac4273bb\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 22:05:03,006] Trial 0 finished with value: 0.6801286464260459 and parameters: {'C': 0.0001056793472547752, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'scale'}. Best is trial 0 with value: 0.6801286464260459.\n",
      "\n",
      "  2%|▏         | 1/50 [01:20<1:06:03, 80.88s/it]\u001B[A[I 2025-01-21 22:06:19,737] Trial 1 finished with value: 0.6801286464260459 and parameters: {'C': 0.00022333376636132435, 'kernel': 'poly', 'degree': 2, 'gamma': 'scale'}. Best is trial 0 with value: 0.6801286464260459.\n",
      "\n",
      "  4%|▍         | 2/50 [02:37<1:02:45, 78.44s/it]\u001B[A[I 2025-01-21 22:07:05,551] Trial 2 finished with value: 0.8438137787955847 and parameters: {'C': 0.0003560135369092698, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      "  6%|▌         | 3/50 [03:23<49:46, 63.54s/it]  \u001B[A[I 2025-01-21 22:07:48,959] Trial 3 finished with value: 0.8436613419128838 and parameters: {'C': 0.0003036981402476598, 'kernel': 'linear', 'degree': 5, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      "  8%|▊         | 4/50 [04:06<42:37, 55.59s/it]\u001B[A[I 2025-01-21 22:09:03,464] Trial 4 finished with value: 0.6801286464260459 and parameters: {'C': 0.00036531583702150377, 'kernel': 'rbf', 'degree': 5, 'gamma': 'auto'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 10%|█         | 5/50 [05:21<46:48, 62.41s/it]\u001B[A[I 2025-01-21 22:10:18,640] Trial 5 finished with value: 0.6801286464260459 and parameters: {'C': 0.00031554596516872487, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 12%|█▏        | 6/50 [06:36<48:57, 66.75s/it]\u001B[A[I 2025-01-21 22:11:03,635] Trial 6 finished with value: 0.8428963476029241 and parameters: {'C': 0.3918184111671311, 'kernel': 'rbf', 'degree': 5, 'gamma': 'auto'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 14%|█▍        | 7/50 [07:21<42:44, 59.64s/it]\u001B[A[I 2025-01-21 22:11:45,885] Trial 7 finished with value: 0.8412134537842281 and parameters: {'C': 0.13246290432098112, 'kernel': 'sigmoid', 'degree': 2, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 16%|█▌        | 8/50 [08:03<37:52, 54.10s/it]\u001B[A[I 2025-01-21 22:12:48,047] Trial 8 finished with value: 0.8173494715053085 and parameters: {'C': 28.961854698403236, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 18%|█▊        | 9/50 [09:05<38:41, 56.62s/it]\u001B[A[I 2025-01-21 22:13:56,734] Trial 9 finished with value: 0.6801286464260459 and parameters: {'C': 0.0012890245894426914, 'kernel': 'poly', 'degree': 5, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 20%|██        | 10/50 [10:14<40:13, 60.35s/it]\u001B[A[I 2025-01-21 22:14:35,984] Trial 10 finished with value: 0.8383073183753178 and parameters: {'C': 0.008406516845346561, 'kernel': 'linear', 'degree': 4, 'gamma': 'auto'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 22%|██▏       | 11/50 [10:53<35:01, 53.89s/it]\u001B[A[I 2025-01-21 22:15:25,856] Trial 11 finished with value: 0.8375423240653582 and parameters: {'C': 0.009335330689061234, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 24%|██▍       | 12/50 [11:43<33:21, 52.67s/it]\u001B[A[I 2025-01-21 22:16:10,737] Trial 12 finished with value: 0.839378591398907 and parameters: {'C': 0.006637111483496907, 'kernel': 'linear', 'degree': 3, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 26%|██▌       | 13/50 [12:28<31:01, 50.31s/it]\u001B[A[I 2025-01-21 22:17:54,961] Trial 13 finished with value: 0.7847654438933925 and parameters: {'C': 0.9030085296452676, 'kernel': 'linear', 'degree': 5, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 28%|██▊       | 14/50 [14:12<39:57, 66.59s/it]\u001B[A[I 2025-01-21 22:18:33,671] Trial 14 finished with value: 0.8435089050301829 and parameters: {'C': 0.0021077416542716037, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 30%|███       | 15/50 [14:51<33:56, 58.19s/it]\u001B[A[I 2025-01-21 22:19:16,135] Trial 15 finished with value: 0.8272878411097218 and parameters: {'C': 0.03739541262402665, 'kernel': 'linear', 'degree': 3, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 32%|███▏      | 16/50 [15:34<30:17, 53.46s/it]\u001B[A[I 2025-01-21 22:25:19,499] Trial 16 finished with value: 0.7624309819183163 and parameters: {'C': 5.456071918110046, 'kernel': 'linear', 'degree': 5, 'gamma': 'scale'}. Best is trial 2 with value: 0.8438137787955847.\n",
      "\n",
      " 34%|███▍      | 17/50 [21:37<41:58, 76.32s/it]   \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for svm: {'C': 0.0003560135369092698, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}\n",
      "Best F1 score for svm: 0.8438137787955847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "svm_results = {}\n",
    "svm_results['bert_without_augmentation'] = optimize_model('svm', X_bert_no_augmentation, y_bert_no_augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Dvg-ZLIiyunH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 22:25:19,509] A new study created in memory with name: no-name-94bea806-3901-4049-b252-f124a4afb4e1\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 22:31:13,275] Trial 0 finished with value: 0.8001407056053734 and parameters: {'C': 0.0005557124134415947, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 0 with value: 0.8001407056053734.\n",
      "\n",
      "  2%|▏         | 1/50 [05:53<4:48:54, 353.76s/it]\u001B[A[I 2025-01-21 22:45:32,500] Trial 1 finished with value: 0.5062383408925666 and parameters: {'C': 0.000589949334947208, 'kernel': 'poly', 'degree': 5, 'gamma': 'scale'}. Best is trial 0 with value: 0.8001407056053734.\n",
      "\n",
      "  4%|▍         | 2/50 [20:12<8:05:11, 606.50s/it]\u001B[A\n",
      "[I 2025-01-21 22:45:32,505] A new study created in memory with name: no-name-259a2ba7-9296-4c4b-b7e7-508a73dabc9a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for svm: {'C': 0.0005557124134415947, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}\n",
      "Best F1 score for svm: 0.8001407056053734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 22:46:33,015] Trial 0 finished with value: 0.6801286464260459 and parameters: {'C': 79.7971925806194, 'kernel': 'poly', 'degree': 3, 'gamma': 'auto'}. Best is trial 0 with value: 0.6801286464260459.\n",
      "\n",
      "  2%|▏         | 1/50 [01:00<49:24, 60.51s/it]\u001B[A[I 2025-01-21 22:47:40,813] Trial 1 finished with value: 0.6801286464260459 and parameters: {'C': 0.0003724160232412946, 'kernel': 'rbf', 'degree': 5, 'gamma': 'auto'}. Best is trial 0 with value: 0.6801286464260459.\n",
      "\n",
      "  4%|▍         | 2/50 [02:08<51:50, 64.80s/it]\u001B[A[I 2025-01-21 22:49:03,000] Trial 2 finished with value: 0.6801286464260459 and parameters: {'C': 0.20500688205834142, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 0 with value: 0.6801286464260459.\n",
      "\n",
      "  6%|▌         | 3/50 [03:30<56:58, 72.74s/it]\u001B[A[I 2025-01-21 22:50:16,155] Trial 3 finished with value: 0.730456233521128 and parameters: {'C': 0.06090560165176679, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 3 with value: 0.730456233521128.\n",
      "\n",
      "  8%|▊         | 4/50 [04:43<55:53, 72.90s/it]\u001B[A[I 2025-01-21 22:51:06,514] Trial 4 finished with value: 0.761668797504812 and parameters: {'C': 3.969319643490677, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 10%|█         | 5/50 [05:34<48:34, 64.77s/it]\u001B[A[I 2025-01-21 22:52:14,987] Trial 5 finished with value: 0.6801286464260459 and parameters: {'C': 0.0001820730399489636, 'kernel': 'linear', 'degree': 2, 'gamma': 'auto'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 12%|█▏        | 6/50 [06:42<48:25, 66.03s/it]\u001B[A[I 2025-01-21 22:53:30,617] Trial 6 finished with value: 0.7208208644178129 and parameters: {'C': 40.63654953056346, 'kernel': 'sigmoid', 'degree': 2, 'gamma': 'auto'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 14%|█▍        | 7/50 [07:58<49:34, 69.17s/it]\u001B[A[I 2025-01-21 22:54:33,071] Trial 7 finished with value: 0.6801286464260459 and parameters: {'C': 0.0328828334621112, 'kernel': 'sigmoid', 'degree': 2, 'gamma': 'auto'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 16%|█▌        | 8/50 [09:00<46:55, 67.03s/it]\u001B[A[I 2025-01-21 22:56:07,176] Trial 8 finished with value: 0.6801286464260459 and parameters: {'C': 0.032187129007406196, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 18%|█▊        | 9/50 [10:34<51:35, 75.49s/it]\u001B[A[I 2025-01-21 22:57:40,700] Trial 9 finished with value: 0.6801286464260459 and parameters: {'C': 0.007286845837420061, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 20%|██        | 10/50 [12:08<54:02, 81.06s/it]\u001B[A[I 2025-01-21 22:58:31,550] Trial 10 finished with value: 0.7590642576487723 and parameters: {'C': 4.584739141160728, 'kernel': 'sigmoid', 'degree': 5, 'gamma': 'scale'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 22%|██▏       | 11/50 [12:59<46:40, 71.81s/it]\u001B[A[I 2025-01-21 22:59:21,748] Trial 11 finished with value: 0.7616662217663945 and parameters: {'C': 3.850562895347861, 'kernel': 'sigmoid', 'degree': 5, 'gamma': 'scale'}. Best is trial 4 with value: 0.761668797504812.\n",
      "\n",
      " 24%|██▍       | 12/50 [13:49<41:19, 65.24s/it]\u001B[A[I 2025-01-21 23:00:13,193] Trial 12 finished with value: 0.7726784401328144 and parameters: {'C': 2.1770886565694334, 'kernel': 'sigmoid', 'degree': 5, 'gamma': 'scale'}. Best is trial 12 with value: 0.7726784401328144.\n",
      "\n",
      " 26%|██▌       | 13/50 [14:40<37:39, 61.06s/it]\u001B[A[I 2025-01-21 23:01:05,872] Trial 13 finished with value: 0.7731348141487653 and parameters: {'C': 2.197918847562293, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 13 with value: 0.7731348141487653.\n",
      "\n",
      " 28%|██▊       | 14/50 [15:33<35:07, 58.53s/it]\u001B[A[I 2025-01-21 23:03:30,362] Trial 14 finished with value: 0.6874725449700512 and parameters: {'C': 1.1339467931728215, 'kernel': 'poly', 'degree': 5, 'gamma': 'scale'}. Best is trial 13 with value: 0.7731348141487653.\n",
      "\n",
      " 30%|███       | 15/50 [17:57<49:15, 84.44s/it]\u001B[A[I 2025-01-21 23:04:30,016] Trial 15 finished with value: 0.7821658213561498 and parameters: {'C': 0.49503644907234334, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 15 with value: 0.7821658213561498.\n",
      "\n",
      " 32%|███▏      | 16/50 [18:57<43:37, 76.98s/it]\u001B[A[I 2025-01-21 23:05:31,788] Trial 16 finished with value: 0.7772714500470659 and parameters: {'C': 0.33286147109245495, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 15 with value: 0.7821658213561498.\n",
      "\n",
      " 34%|███▍      | 17/50 [19:59<39:49, 72.41s/it]\u001B[A[I 2025-01-21 23:06:35,564] Trial 17 finished with value: 0.7703867354154667 and parameters: {'C': 0.24646356931816926, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 15 with value: 0.7821658213561498.\n",
      "\n",
      " 36%|███▌      | 18/50 [21:03<37:25, 70.17s/it]\u001B[A\n",
      "[I 2025-01-21 23:06:35,567] A new study created in memory with name: no-name-ae4c8b47-7fc6-45b6-b981-3a273b38eb66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for svm: {'C': 0.49503644907234334, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}\n",
      "Best F1 score for svm: 0.7821658213561498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 23:14:57,168] Trial 0 finished with value: 0.7840745074957588 and parameters: {'C': 0.9560707529752526, 'kernel': 'linear', 'degree': 4, 'gamma': 'auto'}. Best is trial 0 with value: 0.7840745074957588.\n",
      "\n",
      "  2%|▏         | 1/50 [08:21<6:49:38, 501.60s/it]\u001B[A[I 2025-01-21 23:23:08,982] Trial 1 finished with value: 0.7827793857978741 and parameters: {'C': 0.866907409787206, 'kernel': 'linear', 'degree': 4, 'gamma': 'auto'}. Best is trial 0 with value: 0.7840745074957588.\n",
      "\n",
      "  4%|▍         | 2/50 [16:33<6:36:40, 495.84s/it]\u001B[A[I 2025-01-21 23:38:57,640] Trial 2 finished with value: 0.43502821729044766 and parameters: {'C': 0.7878594571168033, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 0 with value: 0.7840745074957588.\n",
      "\n",
      "  6%|▌         | 3/50 [32:22<8:27:05, 647.36s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for svm: {'C': 0.9560707529752526, 'kernel': 'linear', 'degree': 4, 'gamma': 'auto'}\n",
      "Best F1 score for svm: 0.7840745074957588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "svm_results['bert_with_augmentation'] = optimize_model('svm', X_bert_with_augmentation, y_bert_with_augmentation)\n",
    "svm_results['tfidf_without_augmentation'] = optimize_model('svm', X_tfidf_no_augmentation, y_tfidf_no_augmentation)\n",
    "svm_results['tfidf_with_augmentation'] = optimize_model('svm', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM results:\n",
      "\n",
      "\n",
      "Using BERT embeddings without augmentation scores: [[0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.845565749235474, 0.8532110091743119, 0.8394495412844036, 0.8532110091743119, 0.8425076452599388, 0.8379204892966361, 0.8241590214067278, 0.8453292496171516, 0.8529862174578867, 0.8437978560490046], [0.8470948012232415, 0.8532110091743119, 0.8394495412844036, 0.8501529051987767, 0.8394495412844036, 0.8379204892966361, 0.8241590214067278, 0.8453292496171516, 0.8545176110260337, 0.8453292496171516], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.8409785932721713, 0.8516819571865444, 0.8409785932721713, 0.8547400611620795, 0.8379204892966361, 0.8379204892966361, 0.8226299694189603, 0.8468606431852986, 0.8545176110260337, 0.8407350689127105], [0.8440366972477065, 0.8470948012232415, 0.8394495412844036, 0.8501529051987767, 0.8363914373088684, 0.8425076452599388, 0.8165137614678899, 0.8407350689127105, 0.8514548238897397, 0.8437978560490046], [0.8058103975535168, 0.8073394495412844, 0.8333333333333334, 0.8348623853211009, 0.8119266055045872, 0.8165137614678899, 0.7981651376146789, 0.8039816232771823, 0.8330781010719756, 0.8284839203675345], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.8318042813455657, 0.8532110091743119, 0.8425076452599388, 0.8440366972477065, 0.8302752293577982, 0.8348623853211009, 0.8165137614678899, 0.8300153139356815, 0.8483920367534457, 0.8514548238897397], [0.8318042813455657, 0.8516819571865444, 0.8425076452599388, 0.8440366972477065, 0.827217125382263, 0.8363914373088684, 0.8149847094801224, 0.8284839203675345, 0.8483920367534457, 0.8499234303215927], [0.8333333333333334, 0.8532110091743119, 0.8379204892966361, 0.8486238532110092, 0.8302752293577982, 0.8363914373088684, 0.8180428134556575, 0.8315467075038285, 0.8545176110260337, 0.8499234303215927], [0.7691131498470948, 0.7675840978593272, 0.8042813455657493, 0.790519877675841, 0.7889908256880734, 0.7889908256880734, 0.77217125382263, 0.7825421133231241, 0.7886676875957122, 0.7947932618683001], [0.8470948012232415, 0.8593272171253823, 0.8379204892966361, 0.845565749235474, 0.8409785932721713, 0.8348623853211009, 0.8211009174311926, 0.8392036753445635, 0.8606431852986217, 0.8483920367534457], [0.8256880733944955, 0.827217125382263, 0.8379204892966361, 0.845565749235474, 0.8226299694189603, 0.8333333333333334, 0.8180428134556575, 0.7993874425727412, 0.8346094946401225, 0.8284839203675345], [0.7599388379204893, 0.7477064220183486, 0.7798165137614679, 0.764525993883792, 0.7431192660550459, 0.77217125382263, 0.7584097859327217, 0.7687595712098009, 0.7611026033690659, 0.7687595712098009]]\n",
      "Using BERT embeddings without augmentation best score: 0.8438137787955847\n",
      "Using BERT embeddings without augmentation best parameters: {'C': 0.0003560135369092698, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}\n",
      "\n",
      "\n",
      "Using BERT embeddings with augmentation scores: [[0.8041176470588235, 0.8052941176470588, 0.7992937021777516, 0.7998822836962919, 0.794585050029429, 0.7822248381400824, 0.8128310771041789, 0.807533843437316, 0.7992937021777516, 0.79635079458505], [0.5052941176470588, 0.5052941176470588, 0.5044143613890524, 0.5044143613890524, 0.5161859917598587, 0.5020600353148911, 0.5061801059446733, 0.4985285462036492, 0.5120659211300765, 0.5079458505002943]]\n",
      "Using BERT embeddings with augmentation best score: 0.8001407056053734\n",
      "Using BERT embeddings with augmentation best parameters: {'C': 0.0005557124134415947, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}\n",
      "\n",
      "\n",
      "Using TF-IDF embeddings without augmentation scores: [[0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.7293577981651376, 0.746177370030581, 0.7308868501529052, 0.7262996941896025, 0.7186544342507645, 0.7262996941896025, 0.7431192660550459, 0.7289433384379785, 0.7289433384379785, 0.7258805513016845], [0.7629969418960245, 0.753822629969419, 0.764525993883792, 0.7568807339449541, 0.72782874617737, 0.7675840978593272, 0.7691131498470948, 0.7488514548238897, 0.7871362940275651, 0.777947932618683], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.7232415902140673, 0.7324159021406728, 0.7125382262996942, 0.7155963302752294, 0.7140672782874617, 0.7140672782874617, 0.72782874617737, 0.7258805513016845, 0.7243491577335375, 0.7182235834609495], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.6788990825688074, 0.6788990825688074, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6804281345565749, 0.6814701378254211, 0.6799387442572741, 0.6799387442572741], [0.7599388379204893, 0.7691131498470948, 0.7553516819571865, 0.753822629969419, 0.7339449541284404, 0.77217125382263, 0.7660550458715596, 0.7473200612557427, 0.777947932618683, 0.7549770290964778], [0.7629969418960245, 0.7691131498470948, 0.7660550458715596, 0.7477064220183486, 0.7308868501529052, 0.7691131498470948, 0.7737003058103975, 0.7442572741194488, 0.776416539050536, 0.776416539050536], [0.7752293577981652, 0.7737003058103975, 0.7813455657492355, 0.7660550458715596, 0.7599388379204893, 0.7828746177370031, 0.7706422018348624, 0.7549770290964778, 0.7947932618683001, 0.7672281776416539], [0.7767584097859327, 0.7737003058103975, 0.7844036697247706, 0.7782874617737003, 0.753822629969419, 0.7844036697247706, 0.7782874617737003, 0.7503828483920367, 0.7901990811638591, 0.7611026033690659], [0.6850152905198776, 0.691131498470948, 0.6834862385321101, 0.6834862385321101, 0.6880733944954128, 0.6880733944954128, 0.6850152905198776, 0.6906584992343032, 0.6921898928024502, 0.6875957120980092], [0.7935779816513762, 0.7874617737003058, 0.7859327217125383, 0.7706422018348624, 0.7553516819571865, 0.7813455657492355, 0.7828746177370031, 0.7733537519142419, 0.8055130168453293, 0.7856049004594181], [0.7874617737003058, 0.7752293577981652, 0.7859327217125383, 0.764525993883792, 0.7415902140672783, 0.7798165137614679, 0.7828746177370031, 0.7656967840735069, 0.8039816232771823, 0.7856049004594181], [0.7798165137614679, 0.77217125382263, 0.7859327217125383, 0.7660550458715596, 0.7293577981651376, 0.764525993883792, 0.7767584097859327, 0.7626339969372129, 0.7886676875957122, 0.777947932618683]]\n",
      "Using TF-IDF embeddings without augmentation best score: 0.7821658213561498\n",
      "Using TF-IDF embeddings without augmentation best parameters: {'C': 0.49503644907234334, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}\n",
      "\n",
      "\n",
      "Using TF-IDF embeddings with augmentation scores: [[0.7776470588235294, 0.7964705882352942, 0.7863449087698646, 0.7769276044732195, 0.7892878163625662, 0.7645673925838729, 0.7904649793996469, 0.7910535609181871, 0.7828134196586227, 0.785167745732784], [0.7788235294117647, 0.7994117647058824, 0.786933490288405, 0.7757504414361389, 0.7857563272513243, 0.7616244849911713, 0.786933490288405, 0.7886992348440259, 0.7786933490288405, 0.785167745732784], [0.43529411764705883, 0.43529411764705883, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485]]\n",
      "Using TF-IDF embeddings with augmentation best score: 0.7840745074957588\n",
      "Using TF-IDF embeddings with augmentation best parameters: {'C': 0.9560707529752526, 'kernel': 'linear', 'degree': 4, 'gamma': 'auto'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM results:\\n\\n\")\n",
    "print(f\"Using BERT embeddings without augmentation scores: {svm_results['bert_without_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best score: {svm_results['bert_without_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best parameters: {svm_results['bert_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using BERT embeddings with augmentation scores: {svm_results['bert_with_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best score: {svm_results['bert_with_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best parameters: {svm_results['bert_with_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation scores: {svm_results['tfidf_without_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best score: {svm_results['tfidf_without_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best parameters: {svm_results['tfidf_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation scores: {svm_results['tfidf_with_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best score: {svm_results['tfidf_with_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best parameters: {svm_results['tfidf_with_augmentation'][0]}\\n\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 23:38:57,661] A new study created in memory with name: no-name-95977fa0-2db4-4638-8c80-1e059a72647f\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-21 23:49:38,235] Trial 0 finished with value: 0.8214800192946223 and parameters: {'booster': 'dart', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.0003039309394725608, 'gamma': 0.00011561856099347192, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.23597894465475702, 'skip_drop': 2.6927273011447618e-05}. Best is trial 0 with value: 0.8214800192946223.\n",
      "\n",
      "  2%|▏         | 1/50 [10:40<8:43:08, 640.57s/it]\u001B[A[I 2025-01-22 00:02:35,735] Trial 1 finished with value: 0.8413651881928151 and parameters: {'booster': 'dart', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.021095344741750326, 'gamma': 3.9120857204649044e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.040842423024035804, 'skip_drop': 0.0015853552518449807}. Best is trial 1 with value: 0.8413651881928151.\n",
      "\n",
      "  4%|▍         | 2/50 [23:38<9:27:13, 709.04s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for xgboost: {'booster': 'dart', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.021095344741750326, 'gamma': 3.9120857204649044e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.040842423024035804, 'skip_drop': 0.0015853552518449807}\n",
      "Best F1 score for xgboost: 0.8413651881928151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_results = {}\n",
    "xgb_results['bert_without_augmentation'] = optimize_model('xgboost', X_bert_no_augmentation, y_bert_no_augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ze5Iu-hQyvkX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-22 00:02:35,748] A new study created in memory with name: no-name-52b9a333-5004-47ab-aa9b-bf23208103ac\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-22 00:05:19,206] Trial 0 finished with value: 0.7781306650971159 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'min_child_weight': 2, 'eta': 1.530201987912871e-07, 'gamma': 0.6105421880756111, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.7781306650971159.\n",
      "\n",
      "  2%|▏         | 1/50 [02:43<2:13:29, 163.46s/it]\u001B[A[I 2025-01-22 00:06:35,431] Trial 1 finished with value: 0.8234460409237269 and parameters: {'booster': 'gblinear'}. Best is trial 1 with value: 0.8234460409237269.\n",
      "\n",
      "  4%|▍         | 2/50 [03:59<1:29:42, 112.14s/it]\u001B[A[I 2025-01-22 00:31:18,263] Trial 2 finished with value: 0.43502821729044766 and parameters: {'booster': 'dart', 'max_depth': 7, 'min_child_weight': 10, 'eta': 1.777911040105423e-08, 'gamma': 5.325087413727096e-08, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.006538399229886496, 'skip_drop': 0.0007106197356892675}. Best is trial 1 with value: 0.8234460409237269.\n",
      "\n",
      "  6%|▌         | 3/50 [28:42<7:29:46, 574.17s/it]\u001B[A\n",
      "[I 2025-01-22 00:31:18,267] A new study created in memory with name: no-name-9426b193-4352-4855-9e96-5eb819a60315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for xgboost: {'booster': 'gblinear'}\n",
      "Best F1 score for xgboost: 0.8234460409237269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-22 00:32:24,144] Trial 0 finished with value: 0.7674845806932015 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.0002802047486933304, 'gamma': 6.513704429429651e-05, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      "  2%|▏         | 1/50 [01:05<53:47, 65.87s/it]\u001B[A[I 2025-01-22 00:33:19,099] Trial 1 finished with value: 0.7601388088848926 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 2, 'eta': 0.0009668725597862803, 'gamma': 0.016763959241005454, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      "  4%|▍         | 2/50 [02:00<47:33, 59.45s/it]\u001B[A[I 2025-01-22 00:40:43,693] Trial 2 finished with value: 0.766410731931195 and parameters: {'booster': 'dart', 'max_depth': 9, 'min_child_weight': 7, 'eta': 4.939113527583739e-05, 'gamma': 0.00583518111631256, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 3.666764500535061e-08, 'skip_drop': 7.266373659608185e-06}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      "  6%|▌         | 3/50 [09:25<3:04:19, 235.32s/it]\u001B[A[I 2025-01-22 00:41:16,052] Trial 3 finished with value: 0.7500358261798052 and parameters: {'booster': 'gblinear'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      "  8%|▊         | 4/50 [09:57<1:58:58, 155.19s/it]\u001B[A[I 2025-01-22 00:41:47,186] Trial 4 finished with value: 0.7498826868229905 and parameters: {'booster': 'gblinear'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 10%|█         | 5/50 [10:28<1:22:50, 110.45s/it]\u001B[A[I 2025-01-22 00:42:18,429] Trial 5 finished with value: 0.7498826868229905 and parameters: {'booster': 'gblinear'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 12%|█▏        | 6/50 [11:00<1:01:15, 83.52s/it] \u001B[A[I 2025-01-22 00:42:49,297] Trial 6 finished with value: 0.7500355920217674 and parameters: {'booster': 'gblinear'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 14%|█▍        | 7/50 [11:31<47:31, 66.31s/it]  \u001B[A[I 2025-01-22 00:50:04,051] Trial 7 finished with value: 0.7667174789609004 and parameters: {'booster': 'dart', 'max_depth': 9, 'min_child_weight': 4, 'eta': 0.0025587131043773833, 'gamma': 0.2178771832784132, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.6901235045899958e-05, 'skip_drop': 4.7235979874720105e-05}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 16%|█▌        | 8/50 [18:45<2:08:31, 183.60s/it]\u001B[A[I 2025-01-22 00:50:34,717] Trial 8 finished with value: 0.7581494021945291 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'eta': 0.001448447432834496, 'gamma': 2.3168353376680137e-08, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 18%|█▊        | 9/50 [19:16<1:32:47, 135.79s/it]\u001B[A[I 2025-01-22 00:51:09,455] Trial 9 finished with value: 0.7579969653118283 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'eta': 6.230837232226174e-05, 'gamma': 8.726599056800781e-08, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 20%|██        | 10/50 [19:51<1:09:43, 104.60s/it]\u001B[A[I 2025-01-22 00:51:29,612] Trial 10 finished with value: 0.7202085411485921 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'min_child_weight': 10, 'eta': 3.087375971734415e-08, 'gamma': 1.7802004782741874e-05, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.7674845806932015.\n",
      "\n",
      " 22%|██▏       | 11/50 [20:11<1:11:34, 110.12s/it]\u001B[A\n",
      "[I 2025-01-22 00:51:29,617] A new study created in memory with name: no-name-333aa906-9550-44f6-b91e-91658d76ae43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for xgboost: {'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.0002802047486933304, 'gamma': 6.513704429429651e-05, 'grow_policy': 'lossguide'}\n",
      "Best F1 score for xgboost: 0.7674845806932015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[I 2025-01-22 00:53:43,223] Trial 0 finished with value: 0.6993282207526919 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 5, 'eta': 0.0005093095769966941, 'gamma': 0.2755313681357525, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.6993282207526919.\n",
      "\n",
      "  2%|▏         | 1/50 [02:13<1:49:06, 133.60s/it]\u001B[A[I 2025-01-22 00:54:57,485] Trial 1 finished with value: 0.7797195582176367 and parameters: {'booster': 'gblinear'}. Best is trial 1 with value: 0.7797195582176367.\n",
      "\n",
      "  4%|▍         | 2/50 [03:27<1:18:57, 98.70s/it] \u001B[A[I 2025-01-22 01:08:17,167] Trial 2 finished with value: 0.6038717930962849 and parameters: {'booster': 'dart', 'max_depth': 3, 'min_child_weight': 6, 'eta': 0.002200044207834882, 'gamma': 3.130789620824195e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.18103649974028194, 'skip_drop': 8.493998683190917e-07}. Best is trial 1 with value: 0.7797195582176367.\n",
      "\n",
      "  6%|▌         | 3/50 [16:47<5:28:02, 418.78s/it]\u001B[A[I 2025-01-22 01:09:31,962] Trial 3 finished with value: 0.7797784163694907 and parameters: {'booster': 'gblinear'}. Best is trial 3 with value: 0.7797784163694907.\n",
      "\n",
      "  8%|▊         | 4/50 [18:02<3:36:57, 282.98s/it]\u001B[A[I 2025-01-22 01:11:58,922] Trial 4 finished with value: 0.6943259356715022 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.0009537513126116157, 'gamma': 0.02764426473499009, 'grow_policy': 'lossguide'}. Best is trial 3 with value: 0.7797784163694907.\n",
      "\n",
      " 10%|█         | 5/50 [20:29<3:04:23, 245.86s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for xgboost: {'booster': 'gblinear'}\n",
      "Best F1 score for xgboost: 0.7797784163694907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_results['bert_with_augmentation'] = optimize_model('xgboost', X_bert_with_augmentation, y_bert_with_augmentation)\n",
    "xgb_results['tfidf_without_augmentation'] = optimize_model('xgboost', X_tfidf_no_augmentation, y_tfidf_no_augmentation)\n",
    "xgb_results['tfidf_with_augmentation'] = optimize_model('xgboost', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost results:\n",
      "\n",
      "\n",
      "Using BERT embeddings without augmentation scores: [[0.8195718654434251, 0.8211009174311926, 0.8058103975535168, 0.8333333333333334, 0.8042813455657493, 0.8149847094801224, 0.8363914373088684, 0.8147013782542113, 0.8238897396630934, 0.8407350689127105], [0.8440366972477065, 0.8409785932721713, 0.8348623853211009, 0.8608562691131498, 0.8348623853211009, 0.8394495412844036, 0.8302752293577982, 0.8376722817764165, 0.8499234303215927, 0.8407350689127105]]\n",
      "Using BERT embeddings without augmentation best score: 0.8413651881928151\n",
      "Using BERT embeddings without augmentation best parameters: {'booster': 'dart', 'max_depth': 7, 'min_child_weight': 7, 'eta': 0.021095344741750326, 'gamma': 3.9120857204649044e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.040842423024035804, 'skip_drop': 0.0015853552518449807}\n",
      "\n",
      "\n",
      "Using BERT embeddings with augmentation scores: [[0.7776470588235294, 0.7823529411764706, 0.7692760447321955, 0.7751618599175986, 0.7692760447321955, 0.7686874632136551, 0.7975279576221307, 0.7816362566215421, 0.7769276044732195, 0.7828134196586227], [0.8247058823529412, 0.8270588235294117, 0.8181283107710418, 0.8187168922895821, 0.835197174808711, 0.809299587992937, 0.8293113596233078, 0.8328428487345497, 0.8122424955856387, 0.8269570335491465], [0.43529411764705883, 0.43529411764705883, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485, 0.43496174220129485]]\n",
      "Using BERT embeddings with augmentation best score: 0.8234460409237269\n",
      "Using BERT embeddings with augmentation best parameters: {'booster': 'gblinear'}\n",
      "\n",
      "\n",
      "Using TF-IDF embeddings without augmentation scores: [[0.7660550458715596, 0.7584097859327217, 0.7553516819571865, 0.77217125382263, 0.7385321100917431, 0.7599388379204893, 0.7752293577981652, 0.776416539050536, 0.7947932618683001, 0.777947932618683], [0.7737003058103975, 0.764525993883792, 0.746177370030581, 0.764525993883792, 0.7415902140672783, 0.7477064220183486, 0.7553516819571865, 0.7672281776416539, 0.7871362940275651, 0.7534456355283308], [0.7599388379204893, 0.7599388379204893, 0.7584097859327217, 0.77217125382263, 0.7492354740061162, 0.7629969418960245, 0.7752293577981652, 0.7718223583460949, 0.7856049004594181, 0.7687595712098009], [0.7629969418960245, 0.7660550458715596, 0.7431192660550459, 0.7400611620795107, 0.7415902140672783, 0.7492354740061162, 0.7629969418960245, 0.7335375191424196, 0.7473200612557427, 0.7534456355283308], [0.7629969418960245, 0.7660550458715596, 0.7431192660550459, 0.7400611620795107, 0.7415902140672783, 0.7492354740061162, 0.7629969418960245, 0.7320061255742726, 0.7473200612557427, 0.7534456355283308], [0.7629969418960245, 0.7660550458715596, 0.7431192660550459, 0.7400611620795107, 0.7415902140672783, 0.7492354740061162, 0.7629969418960245, 0.7320061255742726, 0.7473200612557427, 0.7534456355283308], [0.7629969418960245, 0.7660550458715596, 0.7431192660550459, 0.7400611620795107, 0.7415902140672783, 0.7492354740061162, 0.764525993883792, 0.7320061255742726, 0.7473200612557427, 0.7534456355283308], [0.7737003058103975, 0.7568807339449541, 0.7522935779816514, 0.77217125382263, 0.7431192660550459, 0.7706422018348624, 0.7660550458715596, 0.7733537519142419, 0.7993874425727412, 0.7595712098009189], [0.7614678899082569, 0.764525993883792, 0.7584097859327217, 0.7553516819571865, 0.7339449541284404, 0.7507645259938838, 0.7599388379204893, 0.7626339969372129, 0.781010719754977, 0.7534456355283308], [0.7614678899082569, 0.7584097859327217, 0.753822629969419, 0.7568807339449541, 0.735474006116208, 0.7584097859327217, 0.7553516819571865, 0.7672281776416539, 0.77947932618683, 0.7534456355283308], [0.7140672782874617, 0.7308868501529052, 0.72782874617737, 0.7140672782874617, 0.7018348623853211, 0.7324159021406728, 0.7171253822629969, 0.7166921898928025, 0.7427258805513017, 0.7044410413476263]]\n",
      "Using TF-IDF embeddings without augmentation best score: 0.7674845806932015\n",
      "Using TF-IDF embeddings without augmentation best parameters: {'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'eta': 0.0002802047486933304, 'gamma': 6.513704429429651e-05, 'grow_policy': 'lossguide'}\n",
      "\n",
      "\n",
      "Using TF-IDF embeddings with augmentation scores: [[0.7088235294117647, 0.7047058823529412, 0.6915832842848735, 0.7010005885815186, 0.709240729841083, 0.683343143025309, 0.7086521483225426, 0.7045320776927605, 0.6798116539140671, 0.7015891701000588], [0.7817647058823529, 0.7829411764705883, 0.7763390229546793, 0.7675103001765744, 0.7904649793996469, 0.7716303708063567, 0.7822248381400824, 0.7892878163625662, 0.773984696880518, 0.7810476751030018], [0.6082352941176471, 0.61, 0.5862271924661565, 0.6209535020600353, 0.5985874043555033, 0.6015303119482048, 0.6156562683931724, 0.6115361977633902, 0.5879929370217775, 0.5979988228369629], [0.7817647058823529, 0.7829411764705883, 0.7763390229546793, 0.7675103001765744, 0.7904649793996469, 0.7716303708063567, 0.7822248381400824, 0.7892878163625662, 0.7745732783990583, 0.7810476751030018], [0.6982352941176471, 0.7041176470588235, 0.6809888169511478, 0.7033549146556798, 0.7033549146556798, 0.6709829311359623, 0.7121836374337845, 0.7004120070629782, 0.672160094173043, 0.6974690994702767]]\n",
      "Using TF-IDF embeddings with augmentation best score: 0.7797784163694907\n",
      "Using TF-IDF embeddings with augmentation best parameters: {'booster': 'gblinear'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost results:\\n\\n\")\n",
    "print(f\"Using BERT embeddings without augmentation scores: {xgb_results['bert_without_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best score: {xgb_results['bert_without_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best parameters: {xgb_results['bert_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using BERT embeddings with augmentation scores: {xgb_results['bert_with_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best score: {xgb_results['bert_with_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best parameters: {xgb_results['bert_with_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation scores: {xgb_results['tfidf_without_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best score: {xgb_results['tfidf_without_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best parameters: {xgb_results['tfidf_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation scores: {xgb_results['tfidf_with_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best score: {xgb_results['tfidf_with_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best parameters: {xgb_results['tfidf_with_augmentation'][0]}\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-22 01:11:58,946] A new study created in memory with name: no-name-4b0ac8b7-ed74-4a89-a46b-e182a7c894ae\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A[W 2025-01-22 01:11:58,952] Trial 0 failed with parameters: {'batch_norm': True} because of the following error: KeyError('layers').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amita\\AppData\\Local\\Temp\\ipykernel_35216\\3444861739.py\", line 127, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, model_name, X, y, folds_scores), n_trials=n_trials, timeout=timout, callbacks=[progress_bar])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amita\\AppData\\Local\\Temp\\ipykernel_35216\\3444861739.py\", line 107, in objective\n",
      "    model = Classifier(params, model_type=model_name, log=False)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\classifiers.py\", line 80, in __init__\n",
      "    self.model = DNN(config)\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amita\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\classifiers.py\", line 42, in __init__\n",
      "    input_size = config[\"layers\"][0]\n",
      "                 ~~~~~~^^^^^^^^^^\n",
      "KeyError: 'layers'\n",
      "[W 2025-01-22 01:11:58,956] Trial 0 failed with value None.\n",
      "  0%|          | 0/50 [3:30:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layers'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m dnn_results \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m----> 2\u001B[0m dnn_results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert_without_augmentation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdnn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_bert_no_augmentation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_bert_no_augmentation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m dnn_results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert_with_augmentation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m optimize_model(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdnn\u001B[39m\u001B[38;5;124m'\u001B[39m, X_bert_with_augmentation, y_bert_with_augmentation)\n\u001B[0;32m      4\u001B[0m dnn_results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtfidf_without_augmentation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m optimize_model(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdnn\u001B[39m\u001B[38;5;124m'\u001B[39m, X_tfidf_no_augmentation, y_tfidf_no_augmentation)\n",
      "Cell \u001B[1;32mIn[9], line 127\u001B[0m, in \u001B[0;36moptimize_model\u001B[1;34m(model_name, X, y, n_trials, timout)\u001B[0m\n\u001B[0;32m    125\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    126\u001B[0m progress_bar \u001B[38;5;241m=\u001B[39m TqdmCallback(n_trials)\n\u001B[1;32m--> 127\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfolds_scores\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# Close progress bar\u001B[39;00m\n\u001B[0;32m    129\u001B[0m progress_bar\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    385\u001B[0m \n\u001B[0;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 475\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 63\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    247\u001B[0m ):\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[9], line 127\u001B[0m, in \u001B[0;36moptimize_model.<locals>.<lambda>\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m    125\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    126\u001B[0m progress_bar \u001B[38;5;241m=\u001B[39m TqdmCallback(n_trials)\n\u001B[1;32m--> 127\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(\u001B[38;5;28;01mlambda\u001B[39;00m trial: \u001B[43mobjective\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfolds_scores\u001B[49m\u001B[43m)\u001B[49m, n_trials\u001B[38;5;241m=\u001B[39mn_trials, timeout\u001B[38;5;241m=\u001B[39mtimout, callbacks\u001B[38;5;241m=\u001B[39m[progress_bar])\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# Close progress bar\u001B[39;00m\n\u001B[0;32m    129\u001B[0m progress_bar\u001B[38;5;241m.\u001B[39mclose()\n",
      "Cell \u001B[1;32mIn[9], line 107\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial, model_name, X, y, folds_scores)\u001B[0m\n\u001B[0;32m    103\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrate_drop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39msuggest_float(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrate_drop\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1e-8\u001B[39m, \u001B[38;5;241m1.0\u001B[39m, log\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    104\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_drop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39msuggest_float(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_drop\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1e-8\u001B[39m, \u001B[38;5;241m1.0\u001B[39m, log\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 107\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;66;03m# Create a pipeline with just the classifier since feature prep is external\u001B[39;00m\n\u001B[0;32m    110\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[0;32m    111\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclassifier\u001B[39m\u001B[38;5;124m'\u001B[39m, model)\n\u001B[0;32m    112\u001B[0m ])\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\classifiers.py:80\u001B[0m, in \u001B[0;36mClassifier.__init__\u001B[1;34m(self, config, model_type, log)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog \u001B[38;5;241m=\u001B[39m log\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogistic_regression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdnn\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;66;03m# A one layered logistic regression implementation using the DNN class\u001B[39;00m\n\u001B[1;32m---> 80\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mDNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m], weight_decay\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()  \u001B[38;5;66;03m# Cross-Entropy Loss for multiclass problem\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\classifiers.py:42\u001B[0m, in \u001B[0;36mDNN.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28msuper\u001B[39m(DNN, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m     41\u001B[0m layers \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 42\u001B[0m input_size \u001B[38;5;241m=\u001B[39m \u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlayers\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Iterate through the hidden layers\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output_size \u001B[38;5;129;01min\u001B[39;00m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayers\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:  \u001B[38;5;66;03m# Skip the last layer (number of classes)\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'layers'"
     ]
    }
   ],
   "source": [
    "dnn_results = {}\n",
    "dnn_results['bert_without_augmentation'] = optimize_model('dnn', X_bert_no_augmentation, y_bert_no_augmentation)\n",
    "dnn_results['bert_with_augmentation'] = optimize_model('dnn', X_bert_with_augmentation, y_bert_with_augmentation)\n",
    "dnn_results['tfidf_without_augmentation'] = optimize_model('dnn', X_tfidf_no_augmentation, y_tfidf_no_augmentation)\n",
    "dnn_results['tfidf_with_augmentation'] = optimize_model('dnn', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"DNN results:\\n\\n\")\n",
    "print(f\"Using BERT embeddings without augmentation scores: {dnn_results['bert_without_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best score: {dnn_results['bert_without_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings without augmentation best parameters: {dnn_results['bert_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using BERT embeddings with augmentation scores: {dnn_results['bert_with_augmentation'][2]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best score: {dnn_results['bert_with_augmentation'][1]}\")\n",
    "print(f\"Using BERT embeddings with augmentation best parameters: {dnn_results['bert_with_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation scores: {dnn_results['tfidf_without_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best score: {dnn_results['tfidf_without_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings without augmentation best parameters: {dnn_results['tfidf_without_augmentation'][0]}\\n\\n\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation scores: {dnn_results['tfidf_with_augmentation'][2]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best score: {dnn_results['tfidf_with_augmentation'][1]}\")\n",
    "print(f\"Using TF-IDF embeddings with augmentation best parameters: {dnn_results['tfidf_with_augmentation'][0]}\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
