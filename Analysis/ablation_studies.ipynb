{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8539af0c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(r'C:\\Users\\shaha\\Projects\\Python Projects\\Israel-Palestine-Political-Affiliation-Text-Classification')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:31:56.792932Z",
     "start_time": "2025-01-24T15:31:52.887661Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "from classifiers import *\n",
    "from dataset import EmbeddingDataset\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7151fa6dc52f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:31:59.941917Z",
     "start_time": "2025-01-24T15:31:59.937429Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "def calculate_f1_score(predictions, test_data_package, valid_labels=[0,1,2]):\n",
    "    '''\n",
    "    Uses the y_test from test_data_package to evaluate the model while ignoring bad labels.\n",
    "    \n",
    "    Args:\n",
    "        test_data_package (tuple): A tuple containing (DataLoader, (X_test, y_test)).\n",
    "        valid_labels (list): A list of valid labels to consider for the report.\n",
    "        \n",
    "    Return:\n",
    "        tuple: (F1 score (float), classification_report)\n",
    "    '''\n",
    "    test_dataloader, (X_test, y_test) = test_data_package\n",
    "    true_labels = y_test\n",
    "        \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')  # Use 'weighted' for multi-class F1\n",
    "    \n",
    "    # Return F1 score and classification report\n",
    "    return f1, classification_report(true_labels, predictions, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aad08d",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99737687e8c7e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:34:15.192275Z",
     "start_time": "2025-01-24T15:33:24.810147Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_embedding_undersampled_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets=UNDERSAMPLING_TARGETS,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "bert_embedding_with_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=3,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets={},\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "tfidf_embedding_undersampled_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets=UNDERSAMPLING_TARGETS,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='tf-idf'\n",
    ")\n",
    "\n",
    "tfidf_embedding_with_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=3,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets={},\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='tf-idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5755d",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543a56ab318198c",
   "metadata": {},
   "source": [
    "## Bert Embedding without Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7eb5852d36c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:34:23.680221Z",
     "start_time": "2025-01-24T15:34:15.200281Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(bert_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "test_dataset = EmbeddingDataset(\n",
    "                data_path=DATA_PATH,\n",
    "                subset='TEST',\n",
    "                id_column_idx=ID_COLUMN_IDX,\n",
    "                comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "                label_column_idx=LABEL_COLUMN_IDX,\n",
    "                subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "                augmented_classes=[],\n",
    "                augmentation_ratio=0,\n",
    "                augmentation_methods=[],\n",
    "                adversation_ratio = 0,\n",
    "                undersampling_targets={},\n",
    "                embedder=Embedder(), \n",
    "                embedding_method='distilbert')\n",
    "\n",
    "test_data_package = get_dataloader(test_dataset,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a552e2f504e073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:34:23.688635Z",
     "start_time": "2025-01-24T15:34:23.685628Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['logistic_regression', 'xgboost']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed4495a2fa90c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:39:07.430235Z",
     "start_time": "2025-01-24T15:34:23.695097Z"
    }
   },
   "outputs": [],
   "source": [
    "models_evals_bert = {}\n",
    "models_evals_bert_preds = {}\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    models_evals_bert_preds[MODEL_TYPE] = predictions\n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = calculate_accuracy(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_bert[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fa767",
   "metadata": {},
   "source": [
    "## Bert Embedding with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(bert_embedding_with_augmentation_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10995c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evals_bert_aug = {}\n",
    "models_evals_bert_aug_preds = {}\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    models_evals_bert_aug_preds[MODEL_TYPE] = predictions\n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = calculate_accuracy(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_bert_aug[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754afcbc0e921c64",
   "metadata": {},
   "source": [
    "## TF-IDF Vector no Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(tfidf_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "test_dataset = EmbeddingDataset(\n",
    "                data_path=DATA_PATH,\n",
    "                subset='TEST',\n",
    "                id_column_idx=ID_COLUMN_IDX,\n",
    "                comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "                label_column_idx=LABEL_COLUMN_IDX,\n",
    "                subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "                augmented_classes=[],\n",
    "                augmentation_ratio=0,\n",
    "                augmentation_methods=[],\n",
    "                adversation_ratio = 0,\n",
    "                undersampling_targets={},\n",
    "                embedder=Embedder(), \n",
    "                embedding_method='tf-idf')\n",
    "\n",
    "test_data_package = get_dataloader(test_dataset,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9ba2a9813858a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:48:17.477887Z",
     "start_time": "2025-01-24T15:39:50.521598Z"
    }
   },
   "outputs": [],
   "source": [
    "models_evals_tfidf = {}\n",
    "models_evals_tfidf_preds = {}\n",
    "train_data_package = get_dataloader(tfidf_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    models_evals_tfidf_preds[MODEL_TYPE] = predictions\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    \n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = assess_model(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_tfidf[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e1a51",
   "metadata": {},
   "source": [
    "## TF-IDF with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(tfidf_embedding_with_augmentation_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "\n",
    "test_dataset = EmbeddingDataset(\n",
    "                data_path=DATA_PATH,\n",
    "                subset='TEST',\n",
    "                id_column_idx=ID_COLUMN_IDX,\n",
    "                comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "                label_column_idx=LABEL_COLUMN_IDX,\n",
    "                subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "                augmented_classes=[],\n",
    "                augmentation_ratio=0,\n",
    "                augmentation_methods=[],\n",
    "                adversation_ratio = 0,\n",
    "                undersampling_targets={},\n",
    "                embedder=Embedder(), \n",
    "                embedding_method='tf-idf')\n",
    "\n",
    "test_data_package = get_dataloader(test_dataset,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf471e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evals_tfidf_aug = {}\n",
    "train_data_package = get_dataloader(tfidf_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    \n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = calculate_accuracy(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_tfidf_aug[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e60db77f65297",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13e451bd7f4939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:29:27.303739Z",
     "start_time": "2025-01-24T15:29:27.290638Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_models(models, metric):\n",
    "    model_names = list(models.keys())\n",
    "    accuracy_values = [models[model][metric] for model in model_names]\n",
    "    \n",
    "    # Define the x-axis positions for the bars\n",
    "    x = np.arange(len(model_names))\n",
    "    \n",
    "    # Define the width of the bars\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot bars for accuracy\n",
    "    bars1 = ax.bar(x - bar_width/2, accuracy_values, bar_width, label=metric, color='b')\n",
    "    \n",
    "    # Plot bars for F1 score\n",
    "    \n",
    "    # Adding labels, title, and ticks\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(f'{metric} Scores')\n",
    "    ax.set_title(f'Models Comparison: {metric}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ce1457215c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_bert, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5507626a89cb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_bert, 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3504be4d9876795",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_tfidf, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeee2f8d1a70903",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_tfidf, 'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eba266",
   "metadata": {},
   "source": [
    "## Ablation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b2cc90181fc5d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:49:07.115863Z",
     "start_time": "2025-01-24T15:49:07.110722Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_ablation_test(test_data_package, predictions):\n",
    "    test_dataloader, (X_test, true_labels) = test_data_package\n",
    "    valid_labels = [0,1,2]\n",
    "    valid_mask = [label in valid_labels for label in true_labels]\n",
    "    true_labels = [label for label, mask in zip(true_labels, valid_mask) if mask]\n",
    "    predictions = [pred for pred, mask in zip(predictions, valid_mask) if mask]\n",
    "    return true_labels, predictions, valid_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e449e9c857f08b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:51:01.527853Z",
     "start_time": "2025-01-24T15:51:01.521533Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score_instance(prediction, true_label):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score for a single instance.\n",
    "    \"\"\"\n",
    "    if prediction == true_label == 1:\n",
    "        # True Positive (TP)\n",
    "        tp, fp, fn = 1, 0, 0\n",
    "    elif prediction == 1 and true_label == 0:\n",
    "        # False Positive (FP)\n",
    "        tp, fp, fn = 0, 1, 0\n",
    "    elif prediction == 0 and true_label == 1:\n",
    "        # False Negative (FN)\n",
    "        tp, fp, fn = 0, 0, 1\n",
    "    else:\n",
    "        # True Negative (TN)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if precision + recall > 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "179313ed379b3e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:53:10.626972Z",
     "start_time": "2025-01-24T15:53:10.623119Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def perform_paired_ttest(predictions_1, predictions_2, ground_truth, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a paired t-test to compare the performance of two models based on their predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions_1: List of predictions from model 1.\n",
    "    - predictions_2: List of predictions from model 2.\n",
    "    - ground_truth: List of true labels for the dataset.\n",
    "    - alpha: Significance level (default is 0.05).\n",
    "    \n",
    "    Returns:\n",
    "    - t_stat: The t-statistic value.\n",
    "    - p_value: The p-value from the t-test.\n",
    "    - result: 'Significant' if p-value < alpha, otherwise 'Not Significant'.\n",
    "    \"\"\"\n",
    "    # Compute per-sample correctness for each model\n",
    "    model_1_correctness = [1 if pred == true_label else 0 for pred, true_label in zip(predictions_1, ground_truth)]\n",
    "    model_2_correctness = [1 if pred == true_label else 0 for pred, true_label in zip(predictions_2, ground_truth)]\n",
    "\n",
    "    # Perform paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(model_1_correctness, model_2_correctness)\n",
    "    \n",
    "    # Determine significance\n",
    "    result = 'Significant' if p_value < alpha else 'Not Significant'\n",
    "\n",
    "    print(f\"T-Statistic: {t_stat}\")\n",
    "    print(f\"P-Value: {p_value}\")\n",
    "    print(f\"Result: {result}\")\n",
    "\n",
    "    return t_stat, p_value, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51767939f4ecfbab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:49:15.213606Z",
     "start_time": "2025-01-24T15:49:15.206606Z"
    }
   },
   "outputs": [],
   "source": [
    "true_labels, _, _ = prep_ablation_test(test_data_package, [])\n",
    "\n",
    "print('Significance Test for Model Selection:')\n",
    "perform_paired_ttest(models_evals_bert_preds['logistic_regression'], models_evals_bert_preds['xgboost'], true_labels)\n",
    "\n",
    "print('Significance Test for Vector Selection:')\n",
    "perform_paired_ttest(models_evals_tfidf_preds['xgboost'], models_evals_bert_preds['xgboost'], true_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
