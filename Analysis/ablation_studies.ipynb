{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8539af0c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138f4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\Analysis\n",
      "C:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(r'C:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:31:56.792932Z",
     "start_time": "2025-01-24T15:31:52.887661Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\yonat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\yonat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yonat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "from classifiers import *\n",
    "from dataset import EmbeddingDataset\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7151fa6dc52f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:31:59.941917Z",
     "start_time": "2025-01-24T15:31:59.937429Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_f1_score(predictions, test_data_package, valid_labels=[0,1,2]):\n",
    "    '''\n",
    "    Uses the y_test from test_data_package to evaluate the model while ignoring bad labels.\n",
    "    \n",
    "    Args:\n",
    "        test_data_package (tuple): A tuple containing (DataLoader, (X_test, y_test)).\n",
    "        valid_labels (list): A list of valid labels to consider for the report.\n",
    "        \n",
    "    Return:\n",
    "        tuple: (F1 score (float), classification_report)\n",
    "    '''\n",
    "    test_dataloader, (X_test, y_test) = test_data_package\n",
    "    true_labels = y_test\n",
    "        \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')  # Use 'weighted' for multi-class F1\n",
    "    \n",
    "    # Return F1 score and classification report\n",
    "    return f1, classification_report(true_labels, predictions, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aad08d",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99737687e8c7e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:34:15.192275Z",
     "start_time": "2025-01-24T15:33:24.810147Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\yonat\\\\CodeProjects\\\\Israel-Palestine-Political-Affiliation-Text-Classification\\\\Data\\\\full_research_data_tagged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#  Build the datasets\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m txt_regular \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_column_idx\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mID_COLUMN_IDX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment_column_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCOMMENT_COLUMN_IDX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_column_idx\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLABEL_COLUMN_IDX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_column_idx\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSUBSET_COLUMN_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TRAIN / VAL / TEST column\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# ‑‑ no aug\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmentation_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mundersampling_targets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# ‑‑ no undersampling\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m bert_embedding_data \u001b[38;5;241m=\u001b[39m EmbeddingDataset(\n\u001b[0;32m     16\u001b[0m     text_dataset\u001b[38;5;241m=\u001b[39mtxt_regular,\n\u001b[0;32m     17\u001b[0m     embedder\u001b[38;5;241m=\u001b[39mEmbedder(),\n\u001b[0;32m     18\u001b[0m     embedding_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m tfidf_embedding_data \u001b[38;5;241m=\u001b[39m EmbeddingDataset(\n\u001b[0;32m     22\u001b[0m     text_dataset\u001b[38;5;241m=\u001b[39mtxt_regular,\n\u001b[0;32m     23\u001b[0m     embedder\u001b[38;5;241m=\u001b[39mEmbedder(),\n\u001b[0;32m     24\u001b[0m     embedding_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf-idf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m~\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\dataset.py:162\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, csv_path, id_column_idx, comment_column_idx, label_column_idx, split_column_idx, augmented_classes, augmentation_ratio, augmentation_methods, adversation_ratio, undersampling_targets)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregular\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# ------------ load --------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43m_load_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# ------------ basic text cleaning ------------------------------------------\u001b[39;00m\n\u001b[0;32m    165\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__preprocess(df)\n",
      "File \u001b[1;32m~\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\dataset.py:35\u001b[0m, in \u001b[0;36m_load_csv\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _CSVCache\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:             \u001b[38;5;66;03m# load once, reuse for all splits\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m         _CSVCache\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         _CSVCache\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO‑8859‑1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yonat\\\\CodeProjects\\\\Israel-Palestine-Political-Affiliation-Text-Classification\\\\Data\\\\full_research_data_tagged.csv'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "#  Build the datasets\n",
    "# ---------------------------------------------\n",
    "txt_regular = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,  # TRAIN / VAL / TEST column\n",
    "    augmented_classes = [],                 # ‑‑ no aug\n",
    "    augmentation_ratio= 0,\n",
    "    undersampling_targets = {},             # ‑‑ no undersampling\n",
    ")\n",
    "\n",
    "bert_embedding_data = EmbeddingDataset(\n",
    "    text_dataset=txt_regular,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "tfidf_embedding_data = EmbeddingDataset(\n",
    "    text_dataset=txt_regular,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='tf-idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5755d",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543a56ab318198c",
   "metadata": {},
   "source": [
    "## Bert Embedding without Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7eb5852d36c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:34:23.680221Z",
     "start_time": "2025-01-24T15:34:15.200281Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(bert_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "test_dataset = EmbeddingDataset(\n",
    "                data_path=DATA_PATH,\n",
    "                subset='TEST',\n",
    "                id_column_idx=ID_COLUMN_IDX,\n",
    "                comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "                label_column_idx=LABEL_COLUMN_IDX,\n",
    "                subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "                augmented_classes=[],\n",
    "                augmentation_ratio=0,\n",
    "                augmentation_methods=[],\n",
    "                adversation_ratio = 0,\n",
    "                undersampling_targets={},\n",
    "                embedder=Embedder(), \n",
    "                embedding_method='distilbert')\n",
    "\n",
    "test_data_package = get_dataloader(test_dataset,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a552e2f504e073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:34:23.688635Z",
     "start_time": "2025-01-24T15:34:23.685628Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['logistic_regression', 'xgboost']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed4495a2fa90c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:39:07.430235Z",
     "start_time": "2025-01-24T15:34:23.695097Z"
    }
   },
   "outputs": [],
   "source": [
    "models_evals_bert = {}\n",
    "models_evals_bert_preds = {}\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    models_evals_bert_preds[MODEL_TYPE] = predictions\n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = calculate_accuracy(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_bert[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fa767",
   "metadata": {},
   "source": [
    "## Bert Embedding with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(bert_embedding_with_augmentation_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10995c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evals_bert_aug = {}\n",
    "models_evals_bert_aug_preds = {}\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    models_evals_bert_aug_preds[MODEL_TYPE] = predictions\n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = calculate_accuracy(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_bert_aug[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754afcbc0e921c64",
   "metadata": {},
   "source": [
    "## TF-IDF Vector no Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(tfidf_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "test_dataset = EmbeddingDataset(\n",
    "                data_path=DATA_PATH,\n",
    "                subset='TEST',\n",
    "                id_column_idx=ID_COLUMN_IDX,\n",
    "                comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "                label_column_idx=LABEL_COLUMN_IDX,\n",
    "                subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "                augmented_classes=[],\n",
    "                augmentation_ratio=0,\n",
    "                augmentation_methods=[],\n",
    "                adversation_ratio = 0,\n",
    "                undersampling_targets={},\n",
    "                embedder=Embedder(), \n",
    "                embedding_method='tf-idf')\n",
    "\n",
    "test_data_package = get_dataloader(test_dataset,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9ba2a9813858a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:48:17.477887Z",
     "start_time": "2025-01-24T15:39:50.521598Z"
    }
   },
   "outputs": [],
   "source": [
    "models_evals_tfidf = {}\n",
    "models_evals_tfidf_preds = {}\n",
    "train_data_package = get_dataloader(tfidf_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    models_evals_tfidf_preds[MODEL_TYPE] = predictions\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    \n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = assess_model(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_tfidf[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e1a51",
   "metadata": {},
   "source": [
    "## TF-IDF with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_package = get_dataloader(tfidf_embedding_with_augmentation_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "\n",
    "test_dataset = EmbeddingDataset(\n",
    "                data_path=DATA_PATH,\n",
    "                subset='TEST',\n",
    "                id_column_idx=ID_COLUMN_IDX,\n",
    "                comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "                label_column_idx=LABEL_COLUMN_IDX,\n",
    "                subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "                augmented_classes=[],\n",
    "                augmentation_ratio=0,\n",
    "                augmentation_methods=[],\n",
    "                adversation_ratio = 0,\n",
    "                undersampling_targets={},\n",
    "                embedder=Embedder(), \n",
    "                embedding_method='tf-idf')\n",
    "\n",
    "test_data_package = get_dataloader(test_dataset,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf471e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evals_tfidf_aug = {}\n",
    "train_data_package = get_dataloader(tfidf_embedding_undersampled_data,  \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, \n",
    "                            num_workers=2)\n",
    "for MODEL_TYPE in models:\n",
    "    print(f'[Testing Status]: Fitting a {MODEL_TYPE} classifier...')\n",
    "    model_config = MODEL_CONFIG.get(MODEL_TYPE)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    classifier = Classifier(model_config, \n",
    "                            model_type=MODEL_TYPE,\n",
    "                            log=False)\n",
    "    classifier.fit(train_data_package)\n",
    "    \n",
    "    # Test the model\n",
    "    print(f'[Testing Status]: Testing on test subset...')\n",
    "    predictions = classifier.predict(test_data_package)\n",
    "    \n",
    "    # Show accuracy score per class + macro (classification report)\n",
    "    # Calculate accuracy and show classification report\n",
    "    accuracy, report = calculate_accuracy(predictions, test_data_package)\n",
    "    f1, report = calculate_f1_score(predictions, test_data_package)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 score: {f1 * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    models_evals_tfidf_aug[MODEL_TYPE] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e60db77f65297",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13e451bd7f4939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:29:27.303739Z",
     "start_time": "2025-01-24T15:29:27.290638Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_models(models, metric):\n",
    "    model_names = list(models.keys())\n",
    "    accuracy_values = [models[model][metric] for model in model_names]\n",
    "    \n",
    "    # Define the x-axis positions for the bars\n",
    "    x = np.arange(len(model_names))\n",
    "    \n",
    "    # Define the width of the bars\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot bars for accuracy\n",
    "    bars1 = ax.bar(x - bar_width/2, accuracy_values, bar_width, label=metric, color='b')\n",
    "    \n",
    "    # Plot bars for F1 score\n",
    "    \n",
    "    # Adding labels, title, and ticks\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(f'{metric} Scores')\n",
    "    ax.set_title(f'Models Comparison: {metric}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ce1457215c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_bert, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5507626a89cb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_bert, 'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3504be4d9876795",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_tfidf, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeee2f8d1a70903",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(models_evals_tfidf, 'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eba266",
   "metadata": {},
   "source": [
    "## Ablation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b2cc90181fc5d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:49:07.115863Z",
     "start_time": "2025-01-24T15:49:07.110722Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_ablation_test(test_data_package, predictions):\n",
    "    test_dataloader, (X_test, true_labels) = test_data_package\n",
    "    valid_labels = [0,1,2]\n",
    "    valid_mask = [label in valid_labels for label in true_labels]\n",
    "    true_labels = [label for label, mask in zip(true_labels, valid_mask) if mask]\n",
    "    predictions = [pred for pred, mask in zip(predictions, valid_mask) if mask]\n",
    "    return true_labels, predictions, valid_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e449e9c857f08b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:51:01.527853Z",
     "start_time": "2025-01-24T15:51:01.521533Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score_instance(prediction, true_label):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score for a single instance.\n",
    "    \"\"\"\n",
    "    if prediction == true_label == 1:\n",
    "        # True Positive (TP)\n",
    "        tp, fp, fn = 1, 0, 0\n",
    "    elif prediction == 1 and true_label == 0:\n",
    "        # False Positive (FP)\n",
    "        tp, fp, fn = 0, 1, 0\n",
    "    elif prediction == 0 and true_label == 1:\n",
    "        # False Negative (FN)\n",
    "        tp, fp, fn = 0, 0, 1\n",
    "    else:\n",
    "        # True Negative (TN)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if precision + recall > 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "179313ed379b3e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:53:10.626972Z",
     "start_time": "2025-01-24T15:53:10.623119Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def perform_paired_ttest(predictions_1, predictions_2, ground_truth, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a paired t-test to compare the performance of two models based on their predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions_1: List of predictions from model 1.\n",
    "    - predictions_2: List of predictions from model 2.\n",
    "    - ground_truth: List of true labels for the dataset.\n",
    "    - alpha: Significance level (default is 0.05).\n",
    "    \n",
    "    Returns:\n",
    "    - t_stat: The t-statistic value.\n",
    "    - p_value: The p-value from the t-test.\n",
    "    - result: 'Significant' if p-value < alpha, otherwise 'Not Significant'.\n",
    "    \"\"\"\n",
    "    # Compute per-sample correctness for each model\n",
    "    model_1_correctness = [1 if pred == true_label else 0 for pred, true_label in zip(predictions_1, ground_truth)]\n",
    "    model_2_correctness = [1 if pred == true_label else 0 for pred, true_label in zip(predictions_2, ground_truth)]\n",
    "\n",
    "    # Perform paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(model_1_correctness, model_2_correctness)\n",
    "    \n",
    "    # Determine significance\n",
    "    result = 'Significant' if p_value < alpha else 'Not Significant'\n",
    "\n",
    "    print(f\"T-Statistic: {t_stat}\")\n",
    "    print(f\"P-Value: {p_value}\")\n",
    "    print(f\"Result: {result}\")\n",
    "\n",
    "    return t_stat, p_value, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51767939f4ecfbab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T15:49:15.213606Z",
     "start_time": "2025-01-24T15:49:15.206606Z"
    }
   },
   "outputs": [],
   "source": [
    "true_labels, _, _ = prep_ablation_test(test_data_package, [])\n",
    "\n",
    "print('Significance Test for Model Selection:')\n",
    "perform_paired_ttest(models_evals_bert_preds['logistic_regression'], models_evals_bert_preds['xgboost'], true_labels)\n",
    "\n",
    "print('Significance Test for Vector Selection:')\n",
    "perform_paired_ttest(models_evals_tfidf_preds['xgboost'], models_evals_bert_preds['xgboost'], true_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
