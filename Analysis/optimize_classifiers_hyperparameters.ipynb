{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5VNBSOv-kT"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working in: c:\\Users\\yonat\\CodeProjects\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "\n",
    "# Move from …\\Analysis  →  …\\Israel-Palestine-Political-Affiliation-Text-Classification\n",
    "os.chdir(pathlib.Path.cwd().parent)\n",
    "\n",
    "print(\"Now working in:\", pathlib.Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UD9uGUUSvuxq"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local Code\n",
    "from classifiers import *\n",
    "from dataset import EmbeddingDataset, TextDataset\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75rhsfaiwuOx"
   },
   "source": [
    "# Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper dataset\n",
    "class HelperDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(HelperDataset).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.y[item]\n",
    "    \n",
    "# Custom tqdm callback\n",
    "class TqdmCallback:\n",
    "    def __init__(self, n_trials):\n",
    "        self.pbar = tqdm(total=n_trials)\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0.  Utilities and Hyperparameters Space\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "model_hyperparameters = {\n",
    "    'logistic_regression': {\n",
    "        'num_epochs': (5, 20, 'int'),\n",
    "        'learning_rate': (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform'),\n",
    "        \"batch_norm\": ([False], 'categorical'),    # Do not modify in optimization\n",
    "        \"drop_out\": (0, 0, 'int'),    # Do not modify in optimization\n",
    "        \"layers\": ([[768, 3]], 'categorical') # Do not modify in optimization\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': (1e-4, 1e2, 'loguniform'),\n",
    "        'kernel': (['linear', 'rbf', 'sigmoid'], 'categorical'),\n",
    "        'degree': (2, 4, 'int'),\n",
    "        'gamma': (['scale', 'auto'], 'categorical')\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': (50, 200, 'int'),\n",
    "        'learning_rate': (1e-3, 0.2, 'loguniform'),\n",
    "        'booster': (['gbtree', 'gblinear', 'dart'], 'categorical'),\n",
    "        'max_depth': (3, 10, 'int'),\n",
    "        'min_child_weight': (2, 10, 'int'),\n",
    "        'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "        'subsample': (0.5, 1.0, 'uniform'),\n",
    "        'reg_alpha': (1e-8, 10.0, 'loguniform'),\n",
    "        'reg_lambda': (1e-8, 10.0, 'loguniform'),\n",
    "        'gamma': (1e-8, 1.0, 'loguniform')\n",
    "    },\n",
    "    'dnn': {\n",
    "        \"num_epochs\": (5, 20, 'int'),  # Adjust after trial and error\n",
    "        \"learning_rate\": (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform'),\n",
    "        \"batch_norm\": ([True, False], 'categorical'),\n",
    "        \"drop_out\": (0.0, 0.5, 'uniform'),\n",
    "        \"layers\": ([[768, 64, 3],\n",
    "                    [768, 128, 3],\n",
    "                    [768, 64, 64, 3],\n",
    "                    [768, 128, 64, 3],\n",
    "                    [768, 512, 32, 3],\n",
    "                    [768, 512, 128, 3],\n",
    "                    [768, 512, 128, 64, 3]], 'categorical')  # Layer dimensions, including an input and output layer.\n",
    "    }\n",
    "}\n",
    "\n",
    "def _build_estimator(model_name: str, params: dict) -> Classifier:\n",
    "    \"\"\"\n",
    "    Always create a **fresh** classifier for a trial.\n",
    "    \"\"\"\n",
    "    if model_name not in {\"logistic_regression\", \"svm\", \"xgboost\", \"dnn\"}:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    return Classifier(params, model_type=model_name, log=False)\n",
    "\n",
    "def macro_f1_01(y_true, y_pred_or_proba, threshold_0=0.5, threshold_1=0.5):\n",
    "    \"\"\"\n",
    "    Macro‑F1 for classes 0 & 1 (class 2 ignored).\n",
    "    Used in order to optimize the classifier towards the important classes: 0 & 1.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred_or_proba)\n",
    "\n",
    "    # probs → hard labels\n",
    "    if yp.ndim == 2:\n",
    "        if threshold_0 != 0.5 or threshold_1 != 0.5:    # custom cut‑offs\n",
    "            y_pred = np.full(len(yp), 2, dtype=int)\n",
    "            y_pred[yp[:, 0] >= threshold_0] = 0\n",
    "            y_pred[yp[:, 1] >= threshold_1] = 1\n",
    "        else:                                           # argmax\n",
    "            y_pred = yp.argmax(axis=1)\n",
    "    else:\n",
    "        y_pred = yp.astype(int)\n",
    "\n",
    "    return f1_score(y_true, y_pred, labels=[0, 1],\n",
    "                    average=\"macro\", zero_division=0)\n",
    "\n",
    "def _bootstrap_ci(y_true, y_pred, n_iter: int = 1000, alpha: float = .05):\n",
    "    \"\"\"basic percentile bootstrap CI around macro‑F1(0,1)\"\"\"\n",
    "    y_true = np.asarray(y_true)  # <-- safe cast\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    n       = len(y_true)\n",
    "    stats   = []\n",
    "    for _ in range(n_iter):\n",
    "        idx  = resample(np.arange(n), replace=True, n_samples=n)\n",
    "        stats.append(macro_f1_01(y_true[idx], y_pred[idx]))\n",
    "    lower, upper = np.percentile(stats, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return float(lower), float(upper)\n",
    "\n",
    "def _score_on_validation(estimator: Classifier,\n",
    "                         X_tr, y_tr, X_val, y_val, return_ci: bool = False):\n",
    "    \"\"\"\n",
    "    Fit on TRAIN → score on VAL.\n",
    "    Handles both scikit‑learn and PyTorch heads.\n",
    "    return_ci flag will return both score and confidence interval.\n",
    "    \"\"\"\n",
    "    # ‑‑‑ prepare loaders   (HelperDataset just wraps (X,y) tensors/ndarrays)\n",
    "    train_loader = DataLoader(HelperDataset(X_tr,  y_tr),\n",
    "                              batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(HelperDataset(X_val, y_val),\n",
    "                              batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # -- fit + predict ------------------------------------------------------\n",
    "    estimator.fit((train_loader, (X_tr, y_tr)))\n",
    "    preds = estimator.predict((val_loader, (X_val, y_val)))\n",
    "\n",
    "    # -- score --------------------------------------------------------------\n",
    "    score = macro_f1_01(y_val, preds)\n",
    "\n",
    "    if not return_ci:\n",
    "        return score\n",
    "\n",
    "    ci_low, ci_high = _bootstrap_ci(y_val, preds)\n",
    "    return score, ci_low, ci_high\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Optuna — hyper‑param suggestion helper\n",
    "# ------------------------------------------------------------------\n",
    "def suggest_hyperparameters(trial, hp_space):\n",
    "    \"\"\"\n",
    "    hp_space is the dict that lives in model_hyperparameters[…]\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    for key, spec in hp_space.items():\n",
    "        if len(spec) == 2 and spec[1] == \"categorical\":\n",
    "            params[key] = trial.suggest_categorical(key, spec[0])\n",
    "\n",
    "        elif len(spec) == 3:\n",
    "            low, high, kind = spec\n",
    "            if kind == \"loguniform\":\n",
    "                params[key] = trial.suggest_float(key, low, high, log=True)\n",
    "            elif kind == \"uniform\":\n",
    "                params[key] = trial.suggest_float(key, low, high)\n",
    "            elif kind == \"int\":\n",
    "                params[key] = trial.suggest_int(key, low, high)\n",
    "            elif kind == \"categorical\":\n",
    "                params[key] = trial.suggest_categorical(key, low)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown param type: {kind}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Malformed spec for {key}: {spec}\")\n",
    "    return params\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Optuna objective  (no CV — uses VAL split)\n",
    "# ------------------------------------------------------------------\n",
    "def objective(trial, model_name: str, X_train, y_train, X_val,   y_val) -> float:\n",
    "\n",
    "    # -------- suggest hyper‑parameters\n",
    "    params = suggest_hyperparameters(trial,\n",
    "                                     model_hyperparameters[model_name])\n",
    "\n",
    "    # logistic_regression  ≡  one‑layer DNN, override a few things\n",
    "    if model_name == \"logistic_regression\":\n",
    "        params.update(batch_norm=False, drop_out=0.0, layers=[768, 3])\n",
    "\n",
    "    # booster‑specific extras for XGBoost\n",
    "    if model_name == \"xgboost\" and params[\"booster\"] in {\"gbtree\", \"dart\"}:\n",
    "        params.update(\n",
    "            grow_policy=trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            gamma=trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            min_child_weight=trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        )\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params.update(\n",
    "                sample_type   = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"]),\n",
    "                normalize_type= trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"]),\n",
    "                rate_drop     = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True),\n",
    "                skip_drop     = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "            )\n",
    "\n",
    "    # -------- build, fit, score\n",
    "    estimator = _build_estimator(model_name, params)\n",
    "    eval_score = _score_on_validation(estimator, X_train, y_train, X_val, y_val, return_ci=False)\n",
    "    return eval_score\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Optimiser wrapper  (no CV, keeps history)\n",
    "# ------------------------------------------------------------------\n",
    "def optimize_model(model_name: str, X_train, y_train, X_val,   y_val, n_trials: int = 50, timeout: int  = 36000):\n",
    "    \"\"\"\n",
    "    Tune hyper‑parameters by maximising macro‑F1 (0+1 classes) on VAL set.\n",
    "    Returns: best_params, best_score, all_trial_scores\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "\n",
    "    def _optuna_obj(trial):\n",
    "        score = objective(trial, model_name, X_train, y_train, X_val,   y_val)\n",
    "        all_scores.append(score)\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(_optuna_obj, n_trials=n_trials, timeout=timeout, callbacks=[TqdmCallback(n_trials)], n_jobs=3) # Uses all available cores (-1)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score  = study.best_value\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # refit *best* on full TRAIN and compute bootstrap CI --------------\n",
    "    # ------------------------------------------------------------------\n",
    "    best_est   = _build_estimator(model_name, best_params)\n",
    "\n",
    "    best_score, ci_low, ci_high = _score_on_validation(\n",
    "            best_est, X_train, y_train, X_val, y_val, return_ci=True)\n",
    "    \n",
    "    print(f\"[{model_name}]  best F1={best_score:.4f}   \"\n",
    "            f\"Best hyperparameters: {best_params}\",\n",
    "            f\"bootstrap 95% CI=({ci_low:.4f}, {ci_high:.4f})\")\n",
    "\n",
    "    return best_params, best_score, (ci_low, ci_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Optimize models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7ekXAYOkw0w_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Comments: 100%|██████████| 43214/43214 [00:01<00:00, 27100.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextDataset] rows: train=29690, val=4213, test=8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Comments: 100%|██████████| 43214/43214 [00:01<00:00, 30924.06it/s]\n",
      "Augment: 100%|██████████| 29690/29690 [00:59<00:00, 502.14row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextDataset] rows: train=55997, val=4213, test=8656\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Data\\\\cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m embedding_sets: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, EmbeddingDataset]] \u001b[38;5;241m=\u001b[39m {}   \u001b[38;5;66;03m# {method : {variant : EmbeddingDataset}}\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m EMB_METHODS:\n\u001b[0;32m     44\u001b[0m     embedding_sets[method] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregular\u001b[39m\u001b[38;5;124m\"\u001b[39m      : \u001b[43mEmbeddingDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_regular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmented\u001b[39m\u001b[38;5;124m\"\u001b[39m    : EmbeddingDataset(txt_augmented,    embedder, method),\n\u001b[0;32m     47\u001b[0m     }\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 3.  Fetch TRAIN and VAL tensors for each dataset\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[0;32m     52\u001b[0m (X_bert_reg_tr ,  y_bert_reg_tr ,\n\u001b[0;32m     53\u001b[0m  X_bert_reg_val,  y_bert_reg_val) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     54\u001b[0m      \u001b[38;5;241m*\u001b[39membedding_sets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregular\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_subset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:],\n\u001b[0;32m     55\u001b[0m      \u001b[38;5;241m*\u001b[39membedding_sets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregular\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_subset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAL\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:] )\n",
      "File \u001b[1;32mc:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\dataset.py:345\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, text_dataset, embedder, embedding_method, cache_dir)\u001b[0m\n\u001b[0;32m    335\u001b[0m def __init__(self, text_dataset, embedder, embedding_method, cache_dir=r\"Data\\cache\"):\n\u001b[0;32m    336\u001b[0m     \"\"\"\n\u001b[0;32m    337\u001b[0m     Creates the Dataset instance which fits the classification task.\n\u001b[0;32m    338\u001b[0m     Most recurrent parameters are used to initiate the TextDataset in the init.\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m         cache_dir (str): Directory to save the pre-computed embeddings instead of re-calculate.\n\u001b[0;32m    344\u001b[0m     \"\"\"\n\u001b[1;32m--> 345\u001b[0m     # ---- load CSV once -------------------------------------------------\n\u001b[0;32m    346\u001b[0m     self.text_dataset = text_dataset\n\u001b[0;32m    347\u001b[0m     self.embedder, self.embedding_method = embedder, embedding_method\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pathlib.py:1323\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[1;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Data\\\\cache'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create 4 different EmbeddingDataset datasets out of the 2 TextDataset datasets:\n",
    "augmented dataset - with distilbert embedding or tfidf, \n",
    "undersampled balanced dataset - with distilbert embedding or tfidf,\n",
    "regular dataset - with distilbert embedding or tfidf\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1.  Build the three text‑level datasets\n",
    "# ---------------------------------------------\n",
    "txt_regular = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,  # TRAIN / VAL / TEST column\n",
    "    augmented_classes = [],                 # ‑‑ no aug\n",
    "    augmentation_ratio= 0,\n",
    "    undersampling_targets = {},             # ‑‑ no undersampling\n",
    ")\n",
    "\n",
    "txt_augmented = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,\n",
    "    augmented_classes = AUGMENTED_CLASSES,           # e.g. [\"Pro‑Israel\"]\n",
    "    augmentation_ratio= 3,                            # add 3 extra copies\n",
    "    undersampling_targets = {},                       # keep original class sizes\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2.  Wrap each text set in 1‑EmbeddingDataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "EMB_METHODS = [\"distilbert\", \"tf-idf\"]    \n",
    "embedder    = Embedder()                  # your wrapper around HF / TF‑IDF\n",
    "\n",
    "embedding_sets: dict[str, dict[str, EmbeddingDataset]] = {}   # {method : {variant : EmbeddingDataset}}\n",
    "\n",
    "for method in EMB_METHODS:\n",
    "    embedding_sets[method] = {\n",
    "        \"regular\"      : EmbeddingDataset(txt_regular,      embedder, method),\n",
    "        \"augmented\"    : EmbeddingDataset(txt_augmented,    embedder, method),\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3.  Fetch TRAIN and VAL tensors for each dataset\n",
    "# ---------------------------------------------\n",
    "(X_bert_reg_tr ,  y_bert_reg_tr ,\n",
    " X_bert_reg_val,  y_bert_reg_val) = (\n",
    "     *embedding_sets[\"distilbert\"][\"regular\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"distilbert\"][\"regular\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_bert_aug_tr ,  y_bert_aug_tr ,\n",
    " X_bert_aug_val,  y_bert_aug_val) = (\n",
    "     *embedding_sets[\"distilbert\"][\"augmented\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"distilbert\"][\"augmented\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_tfidf_reg_tr , y_tfidf_reg_tr ,\n",
    " X_tfidf_reg_val, y_tfidf_reg_val) = (\n",
    "     *embedding_sets[\"tf-idf\"][\"regular\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"tf-idf\"][\"regular\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_tfidf_aug_tr , y_tfidf_aug_tr ,\n",
    " X_tfidf_aug_val, y_tfidf_aug_val) = (\n",
    "     *embedding_sets[\"tf-idf\"][\"augmented\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"tf-idf\"][\"augmented\"].get_subset(\"VAL\")[:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 01:34:53,210] A new study created in memory with name: no-name-3261fe19-a354-41d4-8ead-7223de803f7f\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:36:15,220] Trial 6 finished with value: 0.5630624755445415 and parameters: {'num_epochs': 11, 'learning_rate': 5.8861649521441797e-05, 'weight_decay': 0.0001283251298666932, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 6 with value: 0.5630624755445415.\n",
      "  2%|▏         | 1/50 [01:22<1:06:58, 82.01s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:36:29,210] Trial 7 finished with value: 0.5897455966419685 and parameters: {'num_epochs': 13, 'learning_rate': 0.00021308052631900011, 'weight_decay': 0.00042842478035934984, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 7 with value: 0.5897455966419685.\n",
      "  4%|▍         | 2/50 [01:36<33:35, 42.00s/it]  c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:36:30,591] Trial 5 finished with value: 0.5463948677420855 and parameters: {'num_epochs': 13, 'learning_rate': 3.320329497884912e-05, 'weight_decay': 1.2078949298058857e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 7 with value: 0.5897455966419685.\n",
      "  6%|▌         | 3/50 [01:37<18:22, 23.45s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:36:36,665] Trial 0 finished with value: 0.5977162497332067 and parameters: {'num_epochs': 14, 'learning_rate': 0.00045512545569932134, 'weight_decay': 0.00018064560656977412, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 0 with value: 0.5977162497332067.\n",
      "  8%|▊         | 4/50 [01:43<12:43, 16.59s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:36:46,608] Trial 2 finished with value: 0.5891249030650133 and parameters: {'num_epochs': 15, 'learning_rate': 0.00026293229686457834, 'weight_decay': 7.21493863419631e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 0 with value: 0.5977162497332067.\n",
      " 10%|█         | 5/50 [01:53<10:38, 14.19s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:36:52,890] Trial 4 finished with value: 0.5822812520639048 and parameters: {'num_epochs': 16, 'learning_rate': 7.324859793954511e-05, 'weight_decay': 0.0009101569991052333, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 0 with value: 0.5977162497332067.\n",
      " 12%|█▏        | 6/50 [01:59<08:26, 11.50s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:37:00,374] Trial 3 finished with value: 0.5953973423159482 and parameters: {'num_epochs': 17, 'learning_rate': 0.0006100363174551247, 'weight_decay': 0.00024287132205608246, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 0 with value: 0.5977162497332067.\n",
      " 14%|█▍        | 7/50 [02:07<07:18, 10.19s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:37:25,266] Trial 1 finished with value: 0.5793753793984049 and parameters: {'num_epochs': 20, 'learning_rate': 5.9367510562586765e-05, 'weight_decay': 0.00012196671195732463, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 0 with value: 0.5977162497332067.\n",
      " 16%|█▌        | 8/50 [02:32<10:24, 14.87s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:37:38,977] Trial 11 finished with value: 0.6006987603722242 and parameters: {'num_epochs': 8, 'learning_rate': 0.0008529579909635162, 'weight_decay': 9.748381850543333e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 18%|█▊        | 9/50 [02:45<09:54, 14.51s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:37:47,665] Trial 14 finished with value: 0.5666890261113913 and parameters: {'num_epochs': 6, 'learning_rate': 0.00013638088635422452, 'weight_decay': 7.244486847862944e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 20%|██        | 10/50 [02:54<08:28, 12.71s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:37:49,517] Trial 8 finished with value: 0.5076249594572944 and parameters: {'num_epochs': 12, 'learning_rate': 1.8829416259988672e-05, 'weight_decay': 0.00044997220533105563, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 22%|██▏       | 11/50 [02:56<06:06,  9.39s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:27,346] Trial 17 finished with value: 0.5895175438596492 and parameters: {'num_epochs': 5, 'learning_rate': 0.0009715452856082422, 'weight_decay': 2.441852291676427e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 24%|██▍       | 12/50 [03:34<11:25, 18.04s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:33,215] Trial 9 finished with value: 0.5630559169788841 and parameters: {'num_epochs': 16, 'learning_rate': 4.3823736418692753e-05, 'weight_decay': 2.1490137020003924e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 26%|██▌       | 13/50 [03:40<08:51, 14.35s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:36,509] Trial 15 finished with value: 0.5842082631597807 and parameters: {'num_epochs': 9, 'learning_rate': 0.0001608309530369186, 'weight_decay': 0.00095867827425376, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 28%|██▊       | 14/50 [03:43<06:36, 11.01s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:44,489] Trial 12 finished with value: 0.4919992841659846 and parameters: {'num_epochs': 15, 'learning_rate': 1.0687896259774099e-05, 'weight_decay': 4.351013592976392e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 30%|███       | 15/50 [03:51<05:53, 10.10s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:49,632] Trial 13 finished with value: 0.5960815306101366 and parameters: {'num_epochs': 15, 'learning_rate': 0.000611246794485962, 'weight_decay': 0.0007740755088428278, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 32%|███▏      | 16/50 [03:56<04:52,  8.61s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:52,172] Trial 10 finished with value: 0.5730561089369166 and parameters: {'num_epochs': 18, 'learning_rate': 4.368695580229175e-05, 'weight_decay': 0.00013047767143347362, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 34%|███▍      | 17/50 [03:58<03:43,  6.78s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:38:52,660] Trial 18 finished with value: 0.5950386653548064 and parameters: {'num_epochs': 8, 'learning_rate': 0.000987373006379323, 'weight_decay': 2.985761638405681e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 36%|███▌      | 18/50 [03:59<02:36,  4.89s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:39:35,631] Trial 20 finished with value: 0.5922869231197462 and parameters: {'num_epochs': 8, 'learning_rate': 0.0004358475781946421, 'weight_decay': 4.635054993919896e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 38%|███▊      | 19/50 [04:42<08:26, 16.33s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:39:37,386] Trial 19 finished with value: 0.5925614321346471 and parameters: {'num_epochs': 9, 'learning_rate': 0.00044750946681584993, 'weight_decay': 3.881586874479654e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 40%|████      | 20/50 [04:44<05:58, 11.95s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:39:40,285] Trial 21 finished with value: 0.5902413368644859 and parameters: {'num_epochs': 8, 'learning_rate': 0.0004559999726092239, 'weight_decay': 0.000227617213078047, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 11 with value: 0.6006987603722242.\n",
      " 42%|████▏     | 21/50 [04:47<04:27,  9.24s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:39:43,226] Trial 16 finished with value: 0.601803676725633 and parameters: {'num_epochs': 16, 'learning_rate': 0.0006425341011554212, 'weight_decay': 0.00012128658739361826, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 44%|████▍     | 22/50 [04:50<03:25,  7.35s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:39:52,074] Trial 23 finished with value: 0.5859790289966855 and parameters: {'num_epochs': 8, 'learning_rate': 0.0003828172819304919, 'weight_decay': 0.00021634510747329722, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 46%|████▌     | 23/50 [04:58<03:30,  7.80s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:39:55,792] Trial 22 finished with value: 0.5900691895935111 and parameters: {'num_epochs': 9, 'learning_rate': 0.0003850860074524765, 'weight_decay': 0.00019099329041828898, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 48%|████▊     | 24/50 [05:02<02:50,  6.57s/it][I 2025-05-15 01:39:55,794] Trial 24 finished with value: 0.5917177974462249 and parameters: {'num_epochs': 8, 'learning_rate': 0.0003720879517473405, 'weight_decay': 0.00023022663800031262, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 50%|█████     | 25/50 [05:02<01:49,  4.40s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:40:03,893] Trial 25 finished with value: 0.5939334804004752 and parameters: {'num_epochs': 9, 'learning_rate': 0.0003781635098091602, 'weight_decay': 0.00022629060193654656, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 52%|█████▏    | 26/50 [05:10<02:07,  5.32s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:40:53,653] Trial 26 finished with value: 0.5901635776340832 and parameters: {'num_epochs': 10, 'learning_rate': 0.0003139311141264612, 'weight_decay': 0.00022350686457436785, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 54%|█████▍    | 27/50 [06:00<06:29, 16.94s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:41:03,346] Trial 27 finished with value: 0.5926275848406808 and parameters: {'num_epochs': 11, 'learning_rate': 0.00030385703385117657, 'weight_decay': 0.00020603430467529092, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 56%|█████▌    | 28/50 [06:10<05:29, 14.97s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:41:07,457] Trial 28 finished with value: 0.5932795041773917 and parameters: {'num_epochs': 11, 'learning_rate': 0.0002948626041179441, 'weight_decay': 0.0002143270078864167, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 58%|█████▊    | 29/50 [06:14<04:10, 11.93s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:41:09,342] Trial 29 finished with value: 0.5929416175317814 and parameters: {'num_epochs': 11, 'learning_rate': 0.0003001019930578626, 'weight_decay': 0.00022218642197696734, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 60%|██████    | 30/50 [06:16<03:01,  9.06s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:41:18,053] Trial 30 finished with value: 0.598344693281402 and parameters: {'num_epochs': 11, 'learning_rate': 0.0007565428003347033, 'weight_decay': 0.00017347277627566813, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 62%|██████▏   | 31/50 [06:24<02:50,  8.96s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:41:22,644] Trial 31 finished with value: 0.5975664868722796 and parameters: {'num_epochs': 11, 'learning_rate': 0.0007623742531356097, 'weight_decay': 6.857996586129758e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 16 with value: 0.601803676725633.\n",
      " 64%|██████▍   | 32/50 [06:29<02:18,  7.68s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:41:22,742] Trial 32 finished with value: 0.6029066051629755 and parameters: {'num_epochs': 11, 'learning_rate': 0.0006923367136361102, 'weight_decay': 7.94273331091091e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 32 with value: 0.6029066051629755.\n",
      "c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:42:33,571] Trial 33 finished with value: 0.6052216748768473 and parameters: {'num_epochs': 19, 'learning_rate': 0.0006680521803255355, 'weight_decay': 7.496192832116148e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 33 with value: 0.6052216748768473.\n",
      " 68%|██████▊   | 34/50 [07:40<05:25, 20.34s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:42:52,091] Trial 35 finished with value: 0.6006041099638542 and parameters: {'num_epochs': 14, 'learning_rate': 0.0006775972046538386, 'weight_decay': 7.902576346473999e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 33 with value: 0.6052216748768473.\n",
      " 70%|███████   | 35/50 [07:58<04:58, 19.89s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:43:21,109] Trial 34 finished with value: 0.6060206313794129 and parameters: {'num_epochs': 19, 'learning_rate': 0.0006523435623039619, 'weight_decay': 7.846317273752116e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 34 with value: 0.6060206313794129.\n",
      " 72%|███████▏  | 36/50 [08:27<05:11, 22.26s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:43:44,595] Trial 37 finished with value: 0.6075557424632232 and parameters: {'num_epochs': 20, 'learning_rate': 0.0006763931659185851, 'weight_decay': 9.239168299828083e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 74%|███████▍  | 37/50 [08:51<04:53, 22.59s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:43:44,915] Trial 36 finished with value: 0.6033608840586329 and parameters: {'num_epochs': 20, 'learning_rate': 0.0007297695591197275, 'weight_decay': 9.145780029065215e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 76%|███████▌  | 38/50 [08:51<03:16, 16.38s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:43:53,521] Trial 38 finished with value: 0.6000693601525924 and parameters: {'num_epochs': 20, 'learning_rate': 0.0006546894425493324, 'weight_decay': 9.44873833117611e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 78%|███████▊  | 39/50 [09:00<02:35, 14.17s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:44:00,552] Trial 39 finished with value: 0.605420987463241 and parameters: {'num_epochs': 20, 'learning_rate': 0.0006664824786830526, 'weight_decay': 0.00010134916967768176, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 80%|████████  | 40/50 [09:07<02:01, 12.10s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:44:00,765] Trial 40 finished with value: 0.6055001061358911 and parameters: {'num_epochs': 20, 'learning_rate': 0.000665622103749275, 'weight_decay': 9.337505738405693e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 82%|████████▏ | 41/50 [09:07<01:17,  8.63s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:45:10,931] Trial 41 finished with value: 0.6069277368834936 and parameters: {'num_epochs': 20, 'learning_rate': 0.000598423320129568, 'weight_decay': 9.339342767618265e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 84%|████████▍ | 42/50 [10:17<03:34, 26.76s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:45:27,971] Trial 42 finished with value: 0.6058804096465147 and parameters: {'num_epochs': 20, 'learning_rate': 0.0005607409370974367, 'weight_decay': 9.19485532148852e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 86%|████████▌ | 43/50 [10:34<02:47, 23.88s/it][I 2025-05-15 01:45:52,864] Trial 43 finished with value: 0.605717157283359 and parameters: {'num_epochs': 20, 'learning_rate': 0.0005316816317617052, 'weight_decay': 5.5793992037743286e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 88%|████████▊ | 44/50 [10:59<02:25, 24.18s/it][I 2025-05-15 01:46:09,691] Trial 44 finished with value: 0.5935387490482893 and parameters: {'num_epochs': 20, 'learning_rate': 0.00020936720223484401, 'weight_decay': 5.6175659416756364e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 90%|█████████ | 45/50 [11:16<01:49, 21.99s/it][I 2025-05-15 01:46:10,722] Trial 46 finished with value: 0.5949025545461604 and parameters: {'num_epochs': 19, 'learning_rate': 0.00021137972298620358, 'weight_decay': 5.42037945529533e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 92%|█████████▏| 46/50 [11:17<01:02, 15.73s/it][I 2025-05-15 01:46:10,906] Trial 47 finished with value: 0.5921885810063835 and parameters: {'num_epochs': 18, 'learning_rate': 0.00021267051847430782, 'weight_decay': 5.9592727331416254e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 94%|█████████▍| 47/50 [11:17<00:33, 11.08s/it][I 2025-05-15 01:46:11,065] Trial 45 finished with value: 0.5952831018111482 and parameters: {'num_epochs': 20, 'learning_rate': 0.0002149482144393313, 'weight_decay': 5.6297283336245904e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 96%|█████████▌| 48/50 [11:17<00:15,  7.81s/it][I 2025-05-15 01:46:12,292] Trial 48 finished with value: 0.5989799751985108 and parameters: {'num_epochs': 19, 'learning_rate': 0.00022789639783250528, 'weight_decay': 5.5556917385042634e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      " 98%|█████████▊| 49/50 [11:19<00:05,  5.84s/it][I 2025-05-15 01:46:18,201] Trial 49 finished with value: 0.6015432116527875 and parameters: {'num_epochs': 19, 'learning_rate': 0.0004995217170149194, 'weight_decay': 5.992498907886467e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 37 with value: 0.6075557424632232.\n",
      "100%|██████████| 50/50 [11:24<00:00, 13.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[logistic_regression]  best F1=0.6002   Best hyperparameters: {'num_epochs': 20, 'learning_rate': 0.0006763931659185851, 'weight_decay': 9.239168299828083e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]} bootstrap 95% CI=(0.5742, 0.6256)\n"
     ]
    }
   ],
   "source": [
    "lr_results = {}\n",
    "lr_results['bert_without_augmentation'] = optimize_model(\n",
    "    model_name = 'logistic_regression',\n",
    "    X_train    = X_bert_reg_tr,\n",
    "    y_train    = y_bert_reg_tr,\n",
    "    X_val      = X_bert_reg_val,\n",
    "    y_val      = y_bert_reg_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 01:46:33,982] A new study created in memory with name: no-name-cb60c1a9-c45d-4756-9363-82660773b95e\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:48:28,639] Trial 3 finished with value: 0.6038500506585613 and parameters: {'num_epochs': 8, 'learning_rate': 0.0004801712813066417, 'weight_decay': 5.031909189369294e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 3 with value: 0.6038500506585613.\n",
      "  2%|▏         | 1/50 [01:54<1:33:38, 114.66s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:48:59,324] Trial 1 finished with value: 0.5986384710224565 and parameters: {'num_epochs': 10, 'learning_rate': 6.03519916469526e-05, 'weight_decay': 1.0510129853039426e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 3 with value: 0.6038500506585613.\n",
      "  4%|▍         | 2/50 [02:25<52:12, 65.26s/it]   c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:49:26,864] Trial 0 finished with value: 0.5915897692184126 and parameters: {'num_epochs': 12, 'learning_rate': 3.1941967629191086e-05, 'weight_decay': 0.00031891355885052905, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 3 with value: 0.6038500506585613.\n",
      "  6%|▌         | 3/50 [02:52<37:37, 48.04s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:49:42,286] Trial 8 finished with value: 0.5975267364394032 and parameters: {'num_epochs': 5, 'learning_rate': 0.00013857047115313555, 'weight_decay': 0.0007029673533730797, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 3 with value: 0.6038500506585613.\n",
      "  8%|▊         | 4/50 [03:08<26:57, 35.16s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:49:43,267] Trial 6 finished with value: 0.6023212243127045 and parameters: {'num_epochs': 13, 'learning_rate': 6.290522296214971e-05, 'weight_decay': 0.00016254300608227923, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 3 with value: 0.6038500506585613.\n",
      " 10%|█         | 5/50 [03:09<17:07, 22.84s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:49:43,358] Trial 5 finished with value: 0.5965121551698189 and parameters: {'num_epochs': 13, 'learning_rate': 4.824893702445483e-05, 'weight_decay': 0.00024261831575493942, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 3 with value: 0.6038500506585613.\n",
      "c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:50:09,766] Trial 4 finished with value: 0.6044551678460155 and parameters: {'num_epochs': 15, 'learning_rate': 0.0004868923464374815, 'weight_decay': 1.3732681929380455e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 14%|█▍        | 7/50 [03:35<12:52, 17.97s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:50:28,194] Trial 2 finished with value: 0.602729627866537 and parameters: {'num_epochs': 16, 'learning_rate': 0.0004095727506878115, 'weight_decay': 1.931682066348361e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 16%|█▌        | 8/50 [03:54<12:39, 18.09s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:50:43,719] Trial 9 finished with value: 0.58504951426112 and parameters: {'num_epochs': 7, 'learning_rate': 2.4427819670386898e-05, 'weight_decay': 0.0006661973595497205, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 18%|█▊        | 9/50 [04:09<11:52, 17.38s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:50:56,218] Trial 11 finished with value: 0.5878014184397162 and parameters: {'num_epochs': 5, 'learning_rate': 2.3048200454796076e-05, 'weight_decay': 1.3247429297475527e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 20%|██        | 10/50 [04:22<10:40, 16.01s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:51:22,897] Trial 7 finished with value: 0.595129466282656 and parameters: {'num_epochs': 20, 'learning_rate': 2.6907789499734064e-05, 'weight_decay': 0.00016161077561638665, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 22%|██▏       | 11/50 [04:48<12:23, 19.07s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:51:42,622] Trial 12 finished with value: 0.5992461595602728 and parameters: {'num_epochs': 8, 'learning_rate': 9.582789935721061e-05, 'weight_decay': 0.0007376511210871975, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 24%|██▍       | 12/50 [05:08<12:11, 19.26s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:52:08,079] Trial 10 finished with value: 0.5869048634552345 and parameters: {'num_epochs': 11, 'learning_rate': 1.6819417378984804e-05, 'weight_decay': 0.0004831270953044454, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.6044551678460155.\n",
      " 26%|██▌       | 13/50 [05:34<13:00, 21.08s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:53:10,979] Trial 13 finished with value: 0.6094514888841129 and parameters: {'num_epochs': 14, 'learning_rate': 0.0007550680805405223, 'weight_decay': 1.6834022740399785e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 28%|██▊       | 14/50 [06:36<20:03, 33.44s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:53:55,794] Trial 15 finished with value: 0.6073745949714942 and parameters: {'num_epochs': 14, 'learning_rate': 0.000585449800205051, 'weight_decay': 0.0009460561649334184, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 30%|███       | 15/50 [07:21<21:28, 36.82s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:54:49,228] Trial 14 finished with value: 0.6041140830322069 and parameters: {'num_epochs': 19, 'learning_rate': 0.0002778226735940465, 'weight_decay': 0.0007137121931784842, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 32%|███▏      | 16/50 [08:15<23:40, 41.77s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:55:11,158] Trial 16 finished with value: 0.6058149692007586 and parameters: {'num_epochs': 18, 'learning_rate': 0.0002798502398664083, 'weight_decay': 1.1097014162003908e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 34%|███▍      | 17/50 [08:37<19:42, 35.85s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:55:32,787] Trial 18 finished with value: 0.6067475057499996 and parameters: {'num_epochs': 17, 'learning_rate': 0.0009893173263180357, 'weight_decay': 4.0519141736447214e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 36%|███▌      | 18/50 [08:58<16:51, 31.60s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:55:51,006] Trial 17 finished with value: 0.6083645236540649 and parameters: {'num_epochs': 20, 'learning_rate': 0.0009707925545887629, 'weight_decay': 4.72313530835781e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 38%|███▊      | 19/50 [09:17<14:15, 27.59s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:55:55,184] Trial 19 finished with value: 0.6085811333035025 and parameters: {'num_epochs': 17, 'learning_rate': 0.0009106015968860879, 'weight_decay': 4.202078379255289e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 40%|████      | 20/50 [09:21<10:17, 20.58s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:56:03,162] Trial 20 finished with value: 0.6092298437011927 and parameters: {'num_epochs': 16, 'learning_rate': 0.0009958815285125429, 'weight_decay': 4.879933123190217e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 13 with value: 0.6094514888841129.\n",
      " 42%|████▏     | 21/50 [09:29<08:07, 16.80s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:57:08,930] Trial 21 finished with value: 0.6108733310799392 and parameters: {'num_epochs': 16, 'learning_rate': 0.0008997254467020284, 'weight_decay': 3.8238891604423175e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 44%|████▍     | 22/50 [10:34<14:41, 31.48s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:58:07,908] Trial 22 finished with value: 0.607333980144129 and parameters: {'num_epochs': 17, 'learning_rate': 0.00023323393253001, 'weight_decay': 5.86013166245498e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 46%|████▌     | 23/50 [11:33<17:52, 39.73s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:58:44,414] Trial 23 finished with value: 0.610479797979798 and parameters: {'num_epochs': 16, 'learning_rate': 0.0009981071240051495, 'weight_decay': 5.276134167143352e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 48%|████▊     | 24/50 [12:10<16:47, 38.76s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:58:53,742] Trial 24 finished with value: 0.6093066396888303 and parameters: {'num_epochs': 15, 'learning_rate': 0.0009856491277100254, 'weight_decay': 4.864929904042248e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 50%|█████     | 25/50 [12:19<12:28, 29.93s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:59:13,187] Trial 25 finished with value: 0.6105212024709982 and parameters: {'num_epochs': 15, 'learning_rate': 0.0009198400674183119, 'weight_decay': 4.2240137411205524e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 52%|█████▏    | 26/50 [12:39<10:42, 26.79s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:59:52,483] Trial 27 finished with value: 0.6061549457730168 and parameters: {'num_epochs': 16, 'learning_rate': 0.0001750690675113322, 'weight_decay': 2.6241008420334e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 54%|█████▍    | 27/50 [13:18<11:42, 30.54s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 01:59:58,139] Trial 28 finished with value: 0.6094071725427284 and parameters: {'num_epochs': 16, 'learning_rate': 0.0007347500304162669, 'weight_decay': 2.6293255729300106e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 56%|█████▌    | 28/50 [13:24<08:27, 23.07s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:00:43,752] Trial 26 finished with value: 0.6104647389332063 and parameters: {'num_epochs': 20, 'learning_rate': 0.0008334612666102779, 'weight_decay': 3.3034564431080734e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 58%|█████▊    | 29/50 [14:09<10:26, 29.84s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:00:52,040] Trial 29 finished with value: 0.6066449468386514 and parameters: {'num_epochs': 15, 'learning_rate': 0.000216212837928103, 'weight_decay': 2.238333801661755e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 60%|██████    | 30/50 [14:18<07:47, 23.37s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:01:51,058] Trial 30 finished with value: 0.6040252740210015 and parameters: {'num_epochs': 15, 'learning_rate': 0.000621828541271802, 'weight_decay': 2.4492214179821765e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 62%|██████▏   | 31/50 [15:17<10:47, 34.07s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:02:21,851] Trial 32 finished with value: 0.6086669297175791 and parameters: {'num_epochs': 14, 'learning_rate': 0.0005806379524195704, 'weight_decay': 2.6255148371940627e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 64%|██████▍   | 32/50 [15:47<09:55, 33.08s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:02:24,797] Trial 31 finished with value: 0.6061471056824217 and parameters: {'num_epochs': 15, 'learning_rate': 0.0005856693971201074, 'weight_decay': 2.5066264884300458e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 66%|██████▌   | 33/50 [15:50<06:48, 24.04s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:03:37,943] Trial 33 finished with value: 0.6076168284789644 and parameters: {'num_epochs': 18, 'learning_rate': 0.0006409781890418201, 'weight_decay': 2.3663362821454477e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 68%|██████▊   | 34/50 [17:03<10:20, 38.77s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:03:40,887] Trial 36 finished with value: 0.6040534013369774 and parameters: {'num_epochs': 12, 'learning_rate': 0.0003801018081865056, 'weight_decay': 8.981347328834226e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 70%|███████   | 35/50 [17:06<07:00, 28.02s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:03:49,849] Trial 37 finished with value: 0.6085612920151326 and parameters: {'num_epochs': 12, 'learning_rate': 0.0003641982958979115, 'weight_decay': 8.223431730990018e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 72%|███████▏  | 36/50 [17:15<05:12, 22.31s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:04:19,141] Trial 34 finished with value: 0.6107949554165025 and parameters: {'num_epochs': 18, 'learning_rate': 0.0006527307829275367, 'weight_decay': 8.412742222627802e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 74%|███████▍  | 37/50 [17:45<05:17, 24.40s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:04:22,526] Trial 35 finished with value: 0.602716278227702 and parameters: {'num_epochs': 18, 'learning_rate': 0.00034724185566074043, 'weight_decay': 7.344873755242981e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 76%|███████▌  | 38/50 [17:48<03:37, 18.10s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:06:32,915] Trial 38 finished with value: 0.605071456061578 and parameters: {'num_epochs': 19, 'learning_rate': 0.0003605891407490065, 'weight_decay': 7.518781362044512e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 78%|███████▊  | 39/50 [19:58<09:29, 51.78s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:06:34,252] Trial 41 finished with value: 0.604718394694168 and parameters: {'num_epochs': 12, 'learning_rate': 0.0003920599129985071, 'weight_decay': 0.00010830410081262874, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 80%|████████  | 40/50 [20:00<06:06, 36.65s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:06:48,978] Trial 39 finished with value: 0.6044253525692982 and parameters: {'num_epochs': 18, 'learning_rate': 0.00036275145138218784, 'weight_decay': 7.876210317736325e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 82%|████████▏ | 41/50 [20:14<04:30, 30.07s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:06:49,275] Trial 40 finished with value: 0.6033392811494209 and parameters: {'num_epochs': 18, 'learning_rate': 0.0003361332834430374, 'weight_decay': 7.301302760774149e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 84%|████████▍ | 42/50 [20:15<02:49, 21.14s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:08:19,098] Trial 42 finished with value: 0.6056465273856577 and parameters: {'num_epochs': 19, 'learning_rate': 0.00037870326003822206, 'weight_decay': 7.255222705048907e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 86%|████████▌ | 43/50 [21:45<04:52, 41.74s/it][I 2025-05-15 02:08:29,386] Trial 43 finished with value: 0.6041834763309661 and parameters: {'num_epochs': 19, 'learning_rate': 0.00044319790003747526, 'weight_decay': 7.090741884605756e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 88%|████████▊ | 44/50 [21:55<03:13, 32.31s/it][I 2025-05-15 02:08:40,008] Trial 44 finished with value: 0.6022364288818303 and parameters: {'num_epochs': 18, 'learning_rate': 0.000427584842939597, 'weight_decay': 7.37841988404635e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 90%|█████████ | 45/50 [22:06<02:09, 25.80s/it][I 2025-05-15 02:08:49,171] Trial 45 finished with value: 0.6085926841735119 and parameters: {'num_epochs': 19, 'learning_rate': 0.0005211586289968609, 'weight_decay': 0.0001271054628072143, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 92%|█████████▏| 46/50 [22:15<01:23, 20.81s/it][I 2025-05-15 02:09:35,523] Trial 47 finished with value: 0.6019494082844641 and parameters: {'num_epochs': 17, 'learning_rate': 0.0005344214363116576, 'weight_decay': 0.0001104152723548334, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 94%|█████████▍| 47/50 [23:01<01:25, 28.47s/it][I 2025-05-15 02:09:35,555] Trial 46 finished with value: 0.6069698444707137 and parameters: {'num_epochs': 17, 'learning_rate': 0.0005130236970681021, 'weight_decay': 0.00011040278721734885, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      "[I 2025-05-15 02:09:44,762] Trial 49 finished with value: 0.6106256323357659 and parameters: {'num_epochs': 20, 'learning_rate': 0.0007626883009148232, 'weight_decay': 0.0001451514716881977, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      " 98%|█████████▊| 49/50 [23:10<00:17, 17.46s/it][I 2025-05-15 02:09:44,791] Trial 48 finished with value: 0.605159674041307 and parameters: {'num_epochs': 20, 'learning_rate': 0.00048374355204780766, 'weight_decay': 3.420088261488803e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.6108733310799392.\n",
      "100%|██████████| 50/50 [23:10<00:00, 27.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[logistic_regression]  best F1=0.6104   Best hyperparameters: {'num_epochs': 16, 'learning_rate': 0.0008997254467020284, 'weight_decay': 3.8238891604423175e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]} bootstrap 95% CI=(0.5875, 0.6339)\n"
     ]
    }
   ],
   "source": [
    "lr_results['bert_with_augmentation'] = optimize_model(\n",
    "    model_name = 'logistic_regression',\n",
    "    X_train    = X_bert_aug_tr,\n",
    "    y_train    = y_bert_aug_tr,\n",
    "    X_val      = X_bert_aug_val,\n",
    "    y_val      = y_bert_aug_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 02:10:09,139] A new study created in memory with name: no-name-77ac5b94-3c57-4161-b40e-403949f3a284\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:11:09,764] Trial 5 finished with value: 0.010422683651030107 and parameters: {'num_epochs': 8, 'learning_rate': 1.3517061238927537e-05, 'weight_decay': 0.00012884055461963748, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      "  2%|▏         | 1/50 [01:00<49:30, 60.63s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:11:24,085] Trial 3 finished with value: 0.0013386880856760374 and parameters: {'num_epochs': 10, 'learning_rate': 0.0005938447480906891, 'weight_decay': 4.2404745689931965e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      "  4%|▍         | 2/50 [01:14<26:42, 33.39s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:11:39,459] Trial 4 finished with value: 0.0 and parameters: {'num_epochs': 12, 'learning_rate': 0.00044634329231872845, 'weight_decay': 0.000689231690958817, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      "  6%|▌         | 3/50 [01:30<19:42, 25.16s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:11:39,730] Trial 7 finished with value: 0.005333333333333333 and parameters: {'num_epochs': 12, 'learning_rate': 0.0005774684603071122, 'weight_decay': 1.1099787634492617e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      "  8%|▊         | 4/50 [01:30<11:45, 15.34s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:11:47,056] Trial 0 finished with value: 0.0 and parameters: {'num_epochs': 13, 'learning_rate': 3.1296022264166835e-05, 'weight_decay': 1.645789619951364e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      " 10%|█         | 5/50 [01:37<09:20, 12.45s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:11:56,066] Trial 1 finished with value: 0.0 and parameters: {'num_epochs': 14, 'learning_rate': 0.00030742969054178214, 'weight_decay': 2.0500440353543848e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      " 12%|█▏        | 6/50 [01:46<08:16, 11.28s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:12:19,185] Trial 2 finished with value: 0.0026702269692923898 and parameters: {'num_epochs': 17, 'learning_rate': 1.2708221457193894e-05, 'weight_decay': 0.0001358520926953696, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      " 14%|█▍        | 7/50 [02:10<10:51, 15.15s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:12:33,789] Trial 9 finished with value: 0.0 and parameters: {'num_epochs': 9, 'learning_rate': 7.796910121757371e-05, 'weight_decay': 8.145030940209813e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      " 16%|█▌        | 8/50 [02:24<10:28, 14.97s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:12:35,072] Trial 8 finished with value: 0.0 and parameters: {'num_epochs': 11, 'learning_rate': 2.282497221044712e-05, 'weight_decay': 0.0008938248717105215, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.010422683651030107.\n",
      " 18%|█▊        | 9/50 [02:25<07:18, 10.69s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:12:41,843] Trial 12 finished with value: 0.03777564893793408 and parameters: {'num_epochs': 7, 'learning_rate': 1.1195118454244682e-05, 'weight_decay': 0.00032350123267055407, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 12 with value: 0.03777564893793408.\n",
      " 20%|██        | 10/50 [02:32<06:19,  9.48s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:12:42,644] Trial 6 finished with value: 0.0 and parameters: {'num_epochs': 20, 'learning_rate': 0.00011735292706726112, 'weight_decay': 3.5527511235077953e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 12 with value: 0.03777564893793408.\n",
      " 22%|██▏       | 11/50 [02:33<04:26,  6.83s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:12:56,565] Trial 10 finished with value: 0.02199223803363519 and parameters: {'num_epochs': 10, 'learning_rate': 1.655077357008434e-05, 'weight_decay': 9.859630859969391e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 12 with value: 0.03777564893793408.\n",
      " 24%|██▍       | 12/50 [02:47<05:41,  8.98s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:13:20,956] Trial 17 finished with value: 0.0 and parameters: {'num_epochs': 5, 'learning_rate': 0.00012957562516391154, 'weight_decay': 0.0003399431711543535, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 12 with value: 0.03777564893793408.\n",
      " 26%|██▌       | 13/50 [03:11<08:25, 13.65s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:13:22,235] Trial 18 finished with value: 0.03829150228042195 and parameters: {'num_epochs': 5, 'learning_rate': 1.1472561614093099e-05, 'weight_decay': 0.00027353948650575853, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 28%|██▊       | 14/50 [03:13<05:56,  9.91s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:13:22,630] Trial 16 finished with value: 0.0 and parameters: {'num_epochs': 6, 'learning_rate': 0.00015365704905577907, 'weight_decay': 0.0001357283883920187, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 30%|███       | 15/50 [03:13<04:06,  7.04s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:13:29,841] Trial 14 finished with value: 0.0 and parameters: {'num_epochs': 9, 'learning_rate': 1.0996535271769503e-05, 'weight_decay': 0.00038969052045255306, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 32%|███▏      | 16/50 [03:20<04:01,  7.09s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:13:35,424] Trial 11 finished with value: 0.0 and parameters: {'num_epochs': 15, 'learning_rate': 1.2697668074810312e-05, 'weight_decay': 8.482942698034856e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 34%|███▍      | 17/50 [03:26<03:39,  6.64s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:13:35,924] Trial 19 finished with value: 0.011857707509881422 and parameters: {'num_epochs': 5, 'learning_rate': 4.2334005717631465e-05, 'weight_decay': 0.00029559292565519405, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 36%|███▌      | 18/50 [03:26<02:33,  4.80s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:08,218] Trial 20 finished with value: 0.0026702269692923898 and parameters: {'num_epochs': 6, 'learning_rate': 3.243525205153245e-05, 'weight_decay': 0.0002858873449297467, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 38%|███▊      | 19/50 [03:59<06:44, 13.05s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:09,818] Trial 21 finished with value: 0.006657789613848202 and parameters: {'num_epochs': 6, 'learning_rate': 3.772519790046461e-05, 'weight_decay': 0.00029073044596323233, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 40%|████      | 20/50 [04:00<04:48,  9.62s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:18,424] Trial 22 finished with value: 0.0 and parameters: {'num_epochs': 7, 'learning_rate': 4.524541147515087e-05, 'weight_decay': 0.00038050230647112523, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 42%|████▏     | 21/50 [04:09<04:30,  9.31s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:25,003] Trial 13 finished with value: 0.0 and parameters: {'num_epochs': 19, 'learning_rate': 1.0754429900379317e-05, 'weight_decay': 3.484722183589129e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 44%|████▍     | 22/50 [04:15<03:57,  8.49s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:25,810] Trial 23 finished with value: 0.005333333333333333 and parameters: {'num_epochs': 7, 'learning_rate': 4.451212763916479e-05, 'weight_decay': 0.0002909030519271886, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 46%|████▌     | 23/50 [04:16<02:47,  6.19s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:30,980] Trial 24 finished with value: 0.0 and parameters: {'num_epochs': 7, 'learning_rate': 4.253078364039644e-05, 'weight_decay': 0.0002708166423023085, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 48%|████▊     | 24/50 [04:21<02:32,  5.88s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:31,552] Trial 25 finished with value: 0.0 and parameters: {'num_epochs': 7, 'learning_rate': 5.335235127684698e-05, 'weight_decay': 0.0002241439777272816, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.03829150228042195.\n",
      " 50%|█████     | 25/50 [04:22<01:47,  4.29s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:14:54,632] Trial 15 finished with value: 0.10378646198026997 and parameters: {'num_epochs': 18, 'learning_rate': 0.0008853652347122005, 'weight_decay': 5.9293879064545084e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 52%|█████▏    | 26/50 [04:45<03:58,  9.93s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:04,043] Trial 26 finished with value: 0.0 and parameters: {'num_epochs': 7, 'learning_rate': 5.5694745041433007e-05, 'weight_decay': 0.0005109961173439231, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 54%|█████▍    | 27/50 [04:54<03:44,  9.77s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:13,788] Trial 27 finished with value: 0.0 and parameters: {'num_epochs': 8, 'learning_rate': 6.534721518139809e-05, 'weight_decay': 0.0005322419914787326, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 56%|█████▌    | 28/50 [05:04<03:34,  9.76s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:22,277] Trial 28 finished with value: 0.0 and parameters: {'num_epochs': 8, 'learning_rate': 1.986262506848875e-05, 'weight_decay': 0.0001902055708099185, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 58%|█████▊    | 29/50 [05:13<03:17,  9.38s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:29,171] Trial 29 finished with value: 0.0 and parameters: {'num_epochs': 8, 'learning_rate': 2.0317007581591237e-05, 'weight_decay': 0.00018256040989056535, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 60%|██████    | 30/50 [05:20<02:52,  8.63s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:29,777] Trial 30 finished with value: 0.0013386880856760374 and parameters: {'num_epochs': 8, 'learning_rate': 2.0460702193676554e-05, 'weight_decay': 0.00018589790099880676, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 62%|██████▏   | 31/50 [05:20<01:58,  6.23s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:41,669] Trial 31 finished with value: 0.016266266266266267 and parameters: {'num_epochs': 9, 'learning_rate': 1.989460076770381e-05, 'weight_decay': 0.0005478238528782845, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 64%|██████▍   | 32/50 [05:32<02:22,  7.93s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:15:42,255] Trial 32 finished with value: 0.0 and parameters: {'num_epochs': 9, 'learning_rate': 2.049855446702659e-05, 'weight_decay': 0.0005094019532215634, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 66%|██████▌   | 33/50 [05:33<01:37,  5.72s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:00,105] Trial 33 finished with value: 0.0 and parameters: {'num_epochs': 16, 'learning_rate': 0.0002206614005393963, 'weight_decay': 0.00018841941236901345, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 68%|██████▊   | 34/50 [06:50<07:17, 27.36s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:09,094] Trial 34 finished with value: 0.0 and parameters: {'num_epochs': 16, 'learning_rate': 2.1410006427539375e-05, 'weight_decay': 0.00017865213693812388, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 70%|███████   | 35/50 [06:59<05:27, 21.85s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:21,043] Trial 36 finished with value: 0.0 and parameters: {'num_epochs': 15, 'learning_rate': 0.0002117250999613297, 'weight_decay': 4.553845682977755e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 72%|███████▏  | 36/50 [07:11<04:24, 18.88s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:28,935] Trial 35 finished with value: 0.06422800943348889 and parameters: {'num_epochs': 17, 'learning_rate': 0.0009923403607448875, 'weight_decay': 0.00017862592429882326, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 74%|███████▍  | 37/50 [07:19<03:22, 15.58s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:31,534] Trial 39 finished with value: 0.0 and parameters: {'num_epochs': 14, 'learning_rate': 0.0002826448472851593, 'weight_decay': 5.426768558999561e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 76%|███████▌  | 38/50 [07:22<02:20, 11.69s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:36,373] Trial 37 finished with value: 0.08798486713159215 and parameters: {'num_epochs': 16, 'learning_rate': 0.0009152422459886006, 'weight_decay': 6.291357144033606e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 78%|███████▊  | 39/50 [07:27<01:45,  9.63s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:36,883] Trial 38 finished with value: 0.0 and parameters: {'num_epochs': 16, 'learning_rate': 0.00019927491686493333, 'weight_decay': 5.933878269784885e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 80%|████████  | 40/50 [07:27<01:08,  6.90s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:17:39,683] Trial 40 finished with value: 0.0 and parameters: {'num_epochs': 15, 'learning_rate': 0.00022702489323032197, 'weight_decay': 5.902504719240051e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 82%|████████▏ | 41/50 [07:30<00:51,  5.67s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:18:35,990] Trial 42 finished with value: 0.03412558817781386 and parameters: {'num_epochs': 11, 'learning_rate': 0.0009228144253332077, 'weight_decay': 6.189568361277803e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 84%|████████▍ | 42/50 [08:26<02:46, 20.86s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:18:48,844] Trial 43 finished with value: 0.0 and parameters: {'num_epochs': 11, 'learning_rate': 1.446230335242933e-05, 'weight_decay': 5.865553207280956e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 15 with value: 0.10378646198026997.\n",
      " 86%|████████▌ | 43/50 [08:39<02:09, 18.46s/it][I 2025-05-15 02:19:17,156] Trial 41 finished with value: 0.11816100576868355 and parameters: {'num_epochs': 18, 'learning_rate': 0.0009610277794424702, 'weight_decay': 5.8439484296644556e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 41 with value: 0.11816100576868355.\n",
      " 88%|████████▊ | 44/50 [09:08<02:08, 21.41s/it][I 2025-05-15 02:19:39,833] Trial 45 finished with value: 0.11120640863772911 and parameters: {'num_epochs': 18, 'learning_rate': 0.0009442368228603599, 'weight_decay': 6.411495319447623e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 41 with value: 0.11816100576868355.\n",
      " 90%|█████████ | 45/50 [09:30<01:48, 21.79s/it][I 2025-05-15 02:19:43,466] Trial 46 finished with value: 0.1411656240247542 and parameters: {'num_epochs': 18, 'learning_rate': 0.0009787152414867852, 'weight_decay': 2.323570658313695e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 46 with value: 0.1411656240247542.\n",
      " 92%|█████████▏| 46/50 [09:34<01:05, 16.35s/it][I 2025-05-15 02:19:43,651] Trial 47 finished with value: 0.1376836877103686 and parameters: {'num_epochs': 18, 'learning_rate': 0.0009909170398892807, 'weight_decay': 3.062950447379969e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 46 with value: 0.1411656240247542.\n",
      " 94%|█████████▍| 47/50 [09:34<00:34, 11.50s/it][I 2025-05-15 02:19:43,665] Trial 44 finished with value: 0.10102006211401848 and parameters: {'num_epochs': 19, 'learning_rate': 0.0008489830549627469, 'weight_decay': 7.215299535253661e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 46 with value: 0.1411656240247542.\n",
      "[I 2025-05-15 02:19:43,960] Trial 48 finished with value: 0.07963828044799705 and parameters: {'num_epochs': 18, 'learning_rate': 0.0007159494319964956, 'weight_decay': 2.1612767210543125e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 46 with value: 0.1411656240247542.\n",
      " 98%|█████████▊| 49/50 [09:34<00:06,  6.26s/it][I 2025-05-15 02:19:49,386] Trial 49 finished with value: 0.1411656240247542 and parameters: {'num_epochs': 18, 'learning_rate': 0.0009943439750435185, 'weight_decay': 2.5170724102471442e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 46 with value: 0.1411656240247542.\n",
      "100%|██████████| 50/50 [09:40<00:00, 11.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[logistic_regression]  best F1=0.1393   Best hyperparameters: {'num_epochs': 18, 'learning_rate': 0.0009787152414867852, 'weight_decay': 2.323570658313695e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]} bootstrap 95% CI=(0.1195, 0.1593)\n"
     ]
    }
   ],
   "source": [
    "lr_results['tfidf_without_augmentation'] = optimize_model(\n",
    "    model_name = 'logistic_regression',\n",
    "    X_train    = X_tfidf_reg_tr,\n",
    "    y_train    = y_tfidf_reg_tr,\n",
    "    X_val      = X_tfidf_reg_val,\n",
    "    y_val      = y_tfidf_reg_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 02:20:04,621] A new study created in memory with name: no-name-a4f6c63a-1748-4d52-9ccc-0bffdb9ed181\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:21:16,005] Trial 4 finished with value: 0.26947749404920934 and parameters: {'num_epochs': 5, 'learning_rate': 2.8588957111980665e-05, 'weight_decay': 0.00040882135337907104, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 4 with value: 0.26947749404920934.\n",
      "  2%|▏         | 1/50 [01:11<58:18, 71.39s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:21:16,654] Trial 6 finished with value: 0.30210397686468543 and parameters: {'num_epochs': 5, 'learning_rate': 0.0002004764546118579, 'weight_decay': 1.2858218701355141e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 6 with value: 0.30210397686468543.\n",
      "  4%|▍         | 2/50 [01:12<23:49, 29.78s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:21:45,749] Trial 5 finished with value: 0.40815612490271425 and parameters: {'num_epochs': 7, 'learning_rate': 0.00047919651784441913, 'weight_decay': 0.00011483113734142058, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.40815612490271425.\n",
      "  6%|▌         | 3/50 [01:41<23:04, 29.46s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:22:15,300] Trial 2 finished with value: 0.30429133021356347 and parameters: {'num_epochs': 9, 'learning_rate': 0.00013058573831646462, 'weight_decay': 1.3484917813170879e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.40815612490271425.\n",
      "  8%|▊         | 4/50 [02:10<22:36, 29.50s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:22:29,053] Trial 0 finished with value: 0.346457757679949 and parameters: {'num_epochs': 10, 'learning_rate': 0.00023734754263602653, 'weight_decay': 5.797276274060962e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 5 with value: 0.40815612490271425.\n",
      " 10%|█         | 5/50 [02:24<17:51, 23.82s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:23:14,311] Trial 9 finished with value: 0.5142488854429152 and parameters: {'num_epochs': 8, 'learning_rate': 0.0009736188032137145, 'weight_decay': 9.809733921434824e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 9 with value: 0.5142488854429152.\n",
      " 12%|█▏        | 6/50 [03:09<22:48, 31.11s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:23:58,574] Trial 1 finished with value: 0.4673067590078569 and parameters: {'num_epochs': 16, 'learning_rate': 0.0002721899568667654, 'weight_decay': 3.6251684737213215e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 9 with value: 0.5142488854429152.\n",
      " 14%|█▍        | 7/50 [03:53<25:22, 35.41s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:24:12,262] Trial 8 finished with value: 0.2624213570114204 and parameters: {'num_epochs': 12, 'learning_rate': 1.6477761418636087e-05, 'weight_decay': 0.000548882457874822, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 9 with value: 0.5142488854429152.\n",
      " 16%|█▌        | 8/50 [04:07<19:56, 28.49s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:24:14,644] Trial 11 finished with value: 0.28756609591498594 and parameters: {'num_epochs': 8, 'learning_rate': 0.00012033427480922021, 'weight_decay': 0.0004760763628558111, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 9 with value: 0.5142488854429152.\n",
      " 18%|█▊        | 9/50 [04:10<13:53, 20.33s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:24:27,301] Trial 3 finished with value: 0.28196331175296385 and parameters: {'num_epochs': 18, 'learning_rate': 2.1819198681746354e-05, 'weight_decay': 1.5597583034912545e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 9 with value: 0.5142488854429152.\n",
      " 20%|██        | 10/50 [04:22<11:58, 17.96s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:24:55,822] Trial 7 finished with value: 0.5068517261979152 and parameters: {'num_epochs': 20, 'learning_rate': 0.00030359135678035035, 'weight_decay': 5.4154044099420395e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 9 with value: 0.5142488854429152.\n",
      " 22%|██▏       | 11/50 [04:51<13:46, 21.19s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:26:27,968] Trial 10 finished with value: 0.5242578794668259 and parameters: {'num_epochs': 19, 'learning_rate': 0.0003793490089290473, 'weight_decay': 2.2182343992815483e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 24%|██▍       | 12/50 [06:23<27:05, 42.78s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:26:40,826] Trial 15 finished with value: 0.5208194570339691 and parameters: {'num_epochs': 10, 'learning_rate': 0.0006377760358281855, 'weight_decay': 1.4881903727600755e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 26%|██▌       | 13/50 [06:36<20:47, 33.71s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:26:56,165] Trial 12 finished with value: 0.333901297258698 and parameters: {'num_epochs': 18, 'learning_rate': 0.00011006675359313965, 'weight_decay': 5.85662412053739e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 28%|██▊       | 14/50 [06:51<16:53, 28.16s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:27:58,501] Trial 16 finished with value: 0.27232272285907566 and parameters: {'num_epochs': 15, 'learning_rate': 3.564302110067012e-05, 'weight_decay': 0.00033260829208082206, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 30%|███       | 15/50 [07:53<22:26, 38.46s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:28:10,067] Trial 17 finished with value: 0.5176397956066827 and parameters: {'num_epochs': 15, 'learning_rate': 0.0007670163296279951, 'weight_decay': 0.00017995315671694352, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 32%|███▏      | 16/50 [08:05<17:12, 30.37s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:28:11,060] Trial 14 finished with value: 0.24806701030927836 and parameters: {'num_epochs': 17, 'learning_rate': 1.5063622795497048e-05, 'weight_decay': 2.7564572087144427e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 34%|███▍      | 17/50 [08:06<11:50, 21.54s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:28:12,844] Trial 13 finished with value: 0.2980754136127789 and parameters: {'num_epochs': 20, 'learning_rate': 4.702178836866497e-05, 'weight_decay': 3.500302531251994e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 10 with value: 0.5242578794668259.\n",
      " 36%|███▌      | 18/50 [08:08<08:19, 15.60s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:29:51,810] Trial 18 finished with value: 0.5298237625057539 and parameters: {'num_epochs': 20, 'learning_rate': 0.0009418160440954709, 'weight_decay': 0.0001458267548611486, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.5298237625057539.\n",
      " 38%|███▊      | 19/50 [09:47<20:59, 40.64s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:29:57,478] Trial 19 finished with value: 0.5246008007112866 and parameters: {'num_epochs': 14, 'learning_rate': 0.0009425836885102599, 'weight_decay': 0.00016945806144232444, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 18 with value: 0.5298237625057539.\n",
      " 40%|████      | 20/50 [09:52<15:04, 30.14s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:30:23,058] Trial 20 finished with value: 0.5395490157554952 and parameters: {'num_epochs': 15, 'learning_rate': 0.0007259578216796328, 'weight_decay': 2.9527150886816856e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 20 with value: 0.5395490157554952.\n",
      " 42%|████▏     | 21/50 [10:18<13:54, 28.77s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:30:24,018] Trial 21 finished with value: 0.5439658211485839 and parameters: {'num_epochs': 14, 'learning_rate': 0.0009091938598792746, 'weight_decay': 2.6596919033662714e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 44%|████▍     | 22/50 [10:19<09:31, 20.42s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:30:59,180] Trial 22 finished with value: 0.5403854269716868 and parameters: {'num_epochs': 12, 'learning_rate': 0.0009383271908950089, 'weight_decay': 2.420847938796004e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 46%|████▌     | 23/50 [10:54<11:10, 24.85s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:31:08,379] Trial 23 finished with value: 0.2864505203645552 and parameters: {'num_epochs': 12, 'learning_rate': 5.053374530211702e-05, 'weight_decay': 2.516553329581418e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 48%|████▊     | 24/50 [11:03<08:43, 20.15s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:31:11,195] Trial 24 finished with value: 0.2859993101643712 and parameters: {'num_epochs': 12, 'learning_rate': 4.941871391746073e-05, 'weight_decay': 2.283683341871204e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 50%|█████     | 25/50 [11:06<06:13, 14.95s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:31:13,611] Trial 25 finished with value: 0.37779125555150794 and parameters: {'num_epochs': 12, 'learning_rate': 0.0005442292236233647, 'weight_decay': 0.0009565905031541488, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 52%|█████▏    | 26/50 [11:08<04:28, 11.19s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:33:10,683] Trial 27 finished with value: 0.28961009183702713 and parameters: {'num_epochs': 13, 'learning_rate': 7.008715897243023e-05, 'weight_decay': 0.00022286462510515826, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 54%|█████▍    | 27/50 [13:06<16:27, 42.96s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:33:18,362] Trial 26 finished with value: 0.3869076270904836 and parameters: {'num_epochs': 14, 'learning_rate': 0.0004719897714800983, 'weight_decay': 0.0008784306718117035, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 56%|█████▌    | 28/50 [13:13<11:52, 32.37s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:33:34,511] Trial 28 finished with value: 0.5146811567421535 and parameters: {'num_epochs': 13, 'learning_rate': 0.0009963848886023242, 'weight_decay': 0.00019852520152550702, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 58%|█████▊    | 29/50 [13:29<09:37, 27.50s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:33:36,190] Trial 29 finished with value: 0.5078791165747687 and parameters: {'num_epochs': 13, 'learning_rate': 0.0005370874893007286, 'weight_decay': 9.729438060178153e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 60%|██████    | 30/50 [13:31<06:35, 19.76s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:33:58,583] Trial 30 finished with value: 0.524409998603547 and parameters: {'num_epochs': 12, 'learning_rate': 0.0005204250147742103, 'weight_decay': 2.0685459715346536e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 62%|██████▏   | 31/50 [13:53<06:30, 20.55s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:34:20,339] Trial 31 finished with value: 0.5192334652864692 and parameters: {'num_epochs': 13, 'learning_rate': 0.0005039771579313404, 'weight_decay': 4.142573615258422e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 64%|██████▍   | 32/50 [14:15<06:16, 20.91s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:34:39,270] Trial 32 finished with value: 0.3943413868527826 and parameters: {'num_epochs': 14, 'learning_rate': 0.0005271264491825881, 'weight_decay': 0.0008385954054698143, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 66%|██████▌   | 33/50 [14:34<05:45, 20.32s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:34:41,325] Trial 33 finished with value: 0.5082600041902368 and parameters: {'num_epochs': 14, 'learning_rate': 0.0004328252117290406, 'weight_decay': 4.038785812698827e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 68%|██████▊   | 34/50 [14:36<03:57, 14.84s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:36:02,890] Trial 36 finished with value: 0.33392576794336326 and parameters: {'num_epochs': 10, 'learning_rate': 0.00020783682738893355, 'weight_decay': 5.973581601401682e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 70%|███████   | 35/50 [15:58<08:42, 34.86s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:36:05,257] Trial 37 finished with value: 0.33895574690502 and parameters: {'num_epochs': 10, 'learning_rate': 0.00022647058530331546, 'weight_decay': 6.595990509271922e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 72%|███████▏  | 36/50 [16:00<05:51, 25.11s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:36:48,412] Trial 39 finished with value: 0.5180146393192457 and parameters: {'num_epochs': 10, 'learning_rate': 0.000759578771405779, 'weight_decay': 6.493976687361014e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 74%|███████▍  | 37/50 [16:43<06:36, 30.52s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:36:54,914] Trial 34 finished with value: 0.5101347927434884 and parameters: {'num_epochs': 15, 'learning_rate': 0.00040992837910871604, 'weight_decay': 4.228150774305816e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 76%|███████▌  | 38/50 [16:50<04:39, 23.32s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:37:08,771] Trial 40 finished with value: 0.428774205891127 and parameters: {'num_epochs': 10, 'learning_rate': 0.0003376439021728917, 'weight_decay': 1.0186369164387121e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 78%|███████▊  | 39/50 [17:04<03:45, 20.48s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:37:10,601] Trial 41 finished with value: 0.32847322850676025 and parameters: {'num_epochs': 10, 'learning_rate': 0.00018613120520254517, 'weight_decay': 5.76606527373452e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 80%|████████  | 40/50 [17:05<02:28, 14.88s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:37:14,470] Trial 35 finished with value: 0.38972160252835103 and parameters: {'num_epochs': 16, 'learning_rate': 0.00018555612500909772, 'weight_decay': 8.103259061934414e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 82%|████████▏ | 41/50 [17:09<01:44, 11.58s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:37:58,139] Trial 38 finished with value: 0.5405268739998217 and parameters: {'num_epochs': 16, 'learning_rate': 0.0007138872757323392, 'weight_decay': 5.570718017605664e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 84%|████████▍ | 42/50 [17:53<02:49, 21.21s/it]c:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\venv\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [768, 3] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2025-05-15 02:39:59,330] Trial 42 finished with value: 0.5280816958378067 and parameters: {'num_epochs': 16, 'learning_rate': 0.0007209543500401628, 'weight_decay': 0.0001262255906492586, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 21 with value: 0.5439658211485839.\n",
      " 86%|████████▌ | 43/50 [19:54<05:58, 51.20s/it][I 2025-05-15 02:40:15,273] Trial 43 finished with value: 0.5442751007573409 and parameters: {'num_epochs': 17, 'learning_rate': 0.0007273309244100031, 'weight_decay': 1.1436305935567236e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      " 88%|████████▊ | 44/50 [20:10<04:03, 40.62s/it][I 2025-05-15 02:40:35,975] Trial 44 finished with value: 0.5046647329896853 and parameters: {'num_epochs': 16, 'learning_rate': 0.0003403705540334648, 'weight_decay': 1.0180826527003202e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      " 90%|█████████ | 45/50 [20:31<02:53, 34.65s/it][I 2025-05-15 02:40:48,705] Trial 46 finished with value: 0.542171786981605 and parameters: {'num_epochs': 16, 'learning_rate': 0.0007257693147065271, 'weight_decay': 3.119462294928927e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      " 92%|█████████▏| 46/50 [20:44<01:52, 28.07s/it][I 2025-05-15 02:40:49,158] Trial 45 finished with value: 0.5017573583150621 and parameters: {'num_epochs': 17, 'learning_rate': 0.00031514663063395124, 'weight_decay': 1.753557229661513e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      " 94%|█████████▍| 47/50 [20:44<00:59, 19.79s/it][I 2025-05-15 02:40:53,268] Trial 47 finished with value: 0.5292455251788903 and parameters: {'num_epochs': 17, 'learning_rate': 0.0006956366233945281, 'weight_decay': 0.00012628909092531407, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      " 96%|█████████▌| 48/50 [20:48<00:30, 15.08s/it][I 2025-05-15 02:40:53,498] Trial 48 finished with value: 0.5241847826086956 and parameters: {'num_epochs': 17, 'learning_rate': 0.0007614852624071996, 'weight_decay': 0.00014495111021629607, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      " 98%|█████████▊| 49/50 [20:48<00:10, 10.63s/it][I 2025-05-15 02:40:56,565] Trial 49 finished with value: 0.54170991000833 and parameters: {'num_epochs': 16, 'learning_rate': 0.0007230338456776125, 'weight_decay': 3.062138217954883e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]}. Best is trial 43 with value: 0.5442751007573409.\n",
      "100%|██████████| 50/50 [20:51<00:00, 25.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[logistic_regression]  best F1=0.5443   Best hyperparameters: {'num_epochs': 17, 'learning_rate': 0.0007273309244100031, 'weight_decay': 1.1436305935567236e-05, 'batch_norm': False, 'drop_out': 0, 'layers': [768, 3]} bootstrap 95% CI=(0.5189, 0.5687)\n"
     ]
    }
   ],
   "source": [
    "lr_results['tfidf_with_augmentation'] = optimize_model(\n",
    "    model_name = 'logistic_regression',\n",
    "    X_train    = X_tfidf_aug_tr,\n",
    "    y_train    = y_tfidf_aug_tr,\n",
    "    X_val      = X_tfidf_aug_val,\n",
    "    y_val      = y_tfidf_aug_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression results:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Best Eval Score</th>\n",
       "      <th>CI (95%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_without_augmentation</td>\n",
       "      <td>{'num_epochs': 20, 'learning_rate': 0.00067639...</td>\n",
       "      <td>0.600196</td>\n",
       "      <td>(0.5741540967583567, 0.6255775550147233)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert_with_augmentation</td>\n",
       "      <td>{'num_epochs': 16, 'learning_rate': 0.00089972...</td>\n",
       "      <td>0.610361</td>\n",
       "      <td>(0.5874523346355454, 0.6339389397280898)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_without_augmentation</td>\n",
       "      <td>{'num_epochs': 18, 'learning_rate': 0.00097871...</td>\n",
       "      <td>0.139265</td>\n",
       "      <td>(0.11945675943110402, 0.15929143635095555)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_with_augmentation</td>\n",
       "      <td>{'num_epochs': 17, 'learning_rate': 0.00072733...</td>\n",
       "      <td>0.544340</td>\n",
       "      <td>(0.5188994560097875, 0.5687425539703366)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Experiment  \\\n",
       "0   bert_without_augmentation   \n",
       "1      bert_with_augmentation   \n",
       "2  tfidf_without_augmentation   \n",
       "3     tfidf_with_augmentation   \n",
       "\n",
       "                                     Best Parameters  Best Eval Score  \\\n",
       "0  {'num_epochs': 20, 'learning_rate': 0.00067639...         0.600196   \n",
       "1  {'num_epochs': 16, 'learning_rate': 0.00089972...         0.610361   \n",
       "2  {'num_epochs': 18, 'learning_rate': 0.00097871...         0.139265   \n",
       "3  {'num_epochs': 17, 'learning_rate': 0.00072733...         0.544340   \n",
       "\n",
       "                                     CI (95%)  \n",
       "0    (0.5741540967583567, 0.6255775550147233)  \n",
       "1    (0.5874523346355454, 0.6339389397280898)  \n",
       "2  (0.11945675943110402, 0.15929143635095555)  \n",
       "3    (0.5188994560097875, 0.5687425539703366)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Logistic Regression results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Eval Score': value[1],\n",
    "        'CI (95%)': value[2]\n",
    "    }\n",
    "    for key, value in lr_results.items()\n",
    "])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 20:52:23,769] A new study created in memory with name: no-name-ac6006f2-d2b7-445d-93bc-cc87f3a0ad34\n",
      "  0%|          | 0/100 [00:00<?, ?it/s][I 2025-05-15 20:55:00,458] Trial 1 finished with value: 0.5156607742640308 and parameters: {'C': 0.20638795885710978, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'scale'}. Best is trial 1 with value: 0.5156607742640308.\n",
      "  1%|          | 1/100 [02:36<4:18:32, 156.69s/it][I 2025-05-15 21:02:52,366] Trial 3 finished with value: 0.5913438938525919 and parameters: {'C': 0.0008370874233199209, 'kernel': 'linear', 'degree': 2, 'gamma': 'auto'}. Best is trial 3 with value: 0.5913438938525919.\n",
      "  2%|▏         | 2/100 [10:28<9:18:46, 342.11s/it][I 2025-05-15 21:04:37,205] Trial 2 finished with value: 0.18373812038014783 and parameters: {'C': 0.0007346646677901582, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 3 with value: 0.5913438938525919.\n",
      "  3%|▎         | 3/100 [12:13<6:17:55, 233.77s/it][I 2025-05-15 21:05:40,036] Trial 4 finished with value: 0.6001989740401057 and parameters: {'C': 0.0030356349410626507, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 4 with value: 0.6001989740401057.\n",
      "  4%|▍         | 4/100 [13:16<4:26:03, 166.28s/it][I 2025-05-15 21:07:12,624] Trial 5 finished with value: 0.6041189417587114 and parameters: {'C': 0.012902590975498084, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 5 with value: 0.6041189417587114.\n",
      "  5%|▌         | 5/100 [14:48<3:41:12, 139.71s/it][I 2025-05-15 21:12:47,614] Trial 7 finished with value: 0.6038315585499108 and parameters: {'C': 0.013886601476985982, 'kernel': 'linear', 'degree': 2, 'gamma': 'auto'}. Best is trial 5 with value: 0.6041189417587114.\n",
      "  6%|▌         | 6/100 [20:23<5:22:53, 206.10s/it][I 2025-05-15 21:13:38,515] Trial 0 finished with value: 0.6074806839628326 and parameters: {'C': 4.870447478027965, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 0 with value: 0.6074806839628326.\n",
      "  7%|▋         | 7/100 [21:14<4:00:49, 155.37s/it][I 2025-05-15 21:31:18,144] Trial 9 finished with value: 0.3775645810712993 and parameters: {'C': 0.007590692632137859, 'kernel': 'sigmoid', 'degree': 2, 'gamma': 'auto'}. Best is trial 0 with value: 0.6074806839628326.\n",
      "  8%|▊         | 8/100 [38:54<11:19:37, 443.24s/it][I 2025-05-15 21:34:09,128] Trial 8 finished with value: 0.6056372712052229 and parameters: {'C': 3.3426681753264873, 'kernel': 'linear', 'degree': 2, 'gamma': 'auto'}. Best is trial 0 with value: 0.6074806839628326.\n",
      "  9%|▉         | 9/100 [41:45<9:03:09, 358.13s/it] [I 2025-05-15 21:36:13,207] Trial 10 finished with value: 0.5454887766445555 and parameters: {'C': 12.161055720817673, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 0 with value: 0.6074806839628326.\n",
      " 10%|█         | 10/100 [43:49<7:08:48, 285.87s/it][I 2025-05-15 21:38:17,265] Trial 11 finished with value: 0.6133738306474172 and parameters: {'C': 0.4770681333898287, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 11 with value: 0.6133738306474172.\n",
      " 11%|█         | 11/100 [45:53<5:50:35, 236.35s/it][I 2025-05-15 21:44:06,966] Trial 13 finished with value: 0.6205346123923032 and parameters: {'C': 0.7220792888638166, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 13 with value: 0.6205346123923032.\n",
      " 12%|█▏        | 12/100 [51:43<6:37:13, 270.83s/it][I 2025-05-15 21:44:54,469] Trial 12 finished with value: 0.6199652274702985 and parameters: {'C': 93.41167537027911, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 13 with value: 0.6205346123923032.\n",
      " 13%|█▎        | 13/100 [52:30<4:54:36, 203.18s/it][I 2025-05-15 21:51:53,923] Trial 14 finished with value: 0.6148925206124853 and parameters: {'C': 0.4676174503095803, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 13 with value: 0.6205346123923032.\n",
      " 14%|█▍        | 14/100 [59:30<6:24:51, 268.50s/it][I 2025-05-15 21:57:12,323] Trial 15 finished with value: 0.6217930447243242 and parameters: {'C': 61.99498001555057, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 15 with value: 0.6217930447243242.\n",
      " 15%|█▌        | 15/100 [1:04:48<6:41:41, 283.54s/it][I 2025-05-15 21:59:30,785] Trial 16 finished with value: 0.619028391491256 and parameters: {'C': 70.32807451467441, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 15 with value: 0.6217930447243242.\n",
      " 16%|█▌        | 16/100 [1:07:07<5:35:49, 239.87s/it][I 2025-05-15 22:06:42,153] Trial 17 finished with value: 0.6200456854892213 and parameters: {'C': 69.25445519598497, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 15 with value: 0.6217930447243242.\n",
      " 17%|█▋        | 17/100 [1:14:18<6:51:28, 297.46s/it][I 2025-05-15 22:07:27,286] Trial 18 finished with value: 0.5967113985348032 and parameters: {'C': 0.053123528182563134, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 15 with value: 0.6217930447243242.\n",
      " 18%|█▊        | 18/100 [1:15:03<5:02:54, 221.64s/it][I 2025-05-15 22:12:54,281] Trial 20 finished with value: 0.6220428589131382 and parameters: {'C': 1.3443561160534745, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 20 with value: 0.6220428589131382.\n",
      " 19%|█▉        | 19/100 [1:20:30<5:41:55, 253.28s/it][I 2025-05-15 22:13:14,282] Trial 19 finished with value: 0.5954155042161687 and parameters: {'C': 0.04255821826934077, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 20 with value: 0.6220428589131382.\n",
      " 20%|██        | 20/100 [1:20:50<4:04:19, 183.24s/it][I 2025-05-15 22:15:18,445] Trial 21 finished with value: 0.6359422105241284 and parameters: {'C': 14.675737896392514, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 21 with value: 0.6359422105241284.\n",
      " 21%|██        | 21/100 [1:22:54<3:37:55, 165.51s/it][I 2025-05-15 22:16:34,898] Trial 6 finished with value: 0.6044338118022329 and parameters: {'C': 15.552593265705303, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 21 with value: 0.6359422105241284.\n",
      " 22%|██▏       | 22/100 [1:24:11<3:00:24, 138.78s/it][I 2025-05-15 22:17:53,471] Trial 23 finished with value: 0.6364668107855913 and parameters: {'C': 14.50373350994979, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 23 with value: 0.6364668107855913.\n",
      " 23%|██▎       | 23/100 [1:25:29<2:34:55, 120.72s/it][I 2025-05-15 22:18:57,915] Trial 24 finished with value: 0.6276954442092058 and parameters: {'C': 2.8318886046818093, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 23 with value: 0.6364668107855913.\n",
      " 24%|██▍       | 24/100 [1:26:34<2:11:30, 103.83s/it][I 2025-05-15 22:20:13,632] Trial 25 finished with value: 0.6257839409657113 and parameters: {'C': 2.3531544152602475, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 23 with value: 0.6364668107855913.\n",
      " 25%|██▌       | 25/100 [1:27:49<1:59:14, 95.39s/it] [I 2025-05-15 22:20:37,547] Trial 22 finished with value: 0.0 and parameters: {'C': 0.00013168276057006075, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 23 with value: 0.6364668107855913.\n",
      " 26%|██▌       | 26/100 [1:28:13<1:31:12, 73.95s/it][I 2025-05-15 22:21:53,443] Trial 26 finished with value: 0.635994540720946 and parameters: {'C': 14.836894996971942, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 23 with value: 0.6364668107855913.\n",
      " 27%|██▋       | 27/100 [1:29:29<1:30:40, 74.53s/it][I 2025-05-15 22:22:14,142] Trial 28 finished with value: 0.5641046508473525 and parameters: {'C': 11.610040941889899, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 23 with value: 0.6364668107855913.\n",
      " 28%|██▊       | 28/100 [1:29:50<1:10:03, 58.38s/it][I 2025-05-15 22:23:09,904] Trial 27 finished with value: 0.6381790258759752 and parameters: {'C': 14.113323810506161, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 29%|██▉       | 29/100 [1:30:46<1:08:09, 57.60s/it][I 2025-05-15 22:23:15,888] Trial 29 finished with value: 0.5645426144432109 and parameters: {'C': 15.888985836560515, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 30%|███       | 30/100 [1:30:52<49:07, 42.11s/it]  [I 2025-05-15 22:25:09,437] Trial 30 finished with value: 0.6294311008180493 and parameters: {'C': 20.08931427082692, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 31%|███       | 31/100 [1:32:45<1:13:04, 63.54s/it][I 2025-05-15 22:26:13,249] Trial 32 finished with value: 0.6289341203020571 and parameters: {'C': 5.85609028574528, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 32%|███▏      | 32/100 [1:33:49<1:12:06, 63.63s/it][I 2025-05-15 22:26:58,309] Trial 31 finished with value: 0.622631042860523 and parameters: {'C': 32.403467965276086, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 33%|███▎      | 33/100 [1:34:34<1:04:49, 58.05s/it][I 2025-05-15 22:32:45,731] Trial 33 finished with value: 0.6287109715681145 and parameters: {'C': 6.654848924453174, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 34%|███▍      | 34/100 [1:40:21<2:39:21, 144.86s/it][I 2025-05-15 22:33:59,735] Trial 34 finished with value: 0.6224042950041393 and parameters: {'C': 29.500848694098647, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 35%|███▌      | 35/100 [1:41:35<2:13:54, 123.61s/it][I 2025-05-15 22:33:59,967] Trial 35 finished with value: 0.6301074363489352 and parameters: {'C': 6.935587077876553, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 36%|███▌      | 36/100 [1:41:36<1:32:21, 86.59s/it] [I 2025-05-15 22:37:55,803] Trial 36 finished with value: 0.6228960842287445 and parameters: {'C': 33.60534641738411, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 37%|███▋      | 37/100 [1:45:32<2:17:56, 131.37s/it][I 2025-05-15 22:38:49,974] Trial 37 finished with value: 0.6175473301682199 and parameters: {'C': 0.22207118083760866, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 38%|███▊      | 38/100 [1:46:26<1:51:48, 108.21s/it][I 2025-05-15 22:38:53,540] Trial 38 finished with value: 0.6165847236676142 and parameters: {'C': 0.16849729370690947, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 39%|███▉      | 39/100 [1:46:29<1:18:05, 76.81s/it] [I 2025-05-15 22:41:15,738] Trial 39 finished with value: 0.6177278774540889 and parameters: {'C': 0.20583636995330415, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 40%|████      | 40/100 [1:48:51<1:36:25, 96.43s/it][I 2025-05-15 22:41:32,810] Trial 40 finished with value: 0.4810699708454811 and parameters: {'C': 1.3389796245115264, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 41%|████      | 41/100 [1:49:09<1:11:24, 72.62s/it][I 2025-05-15 22:44:59,338] Trial 41 finished with value: 0.6056693266946311 and parameters: {'C': 0.984214431084864, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 42%|████▏     | 42/100 [1:52:35<1:49:02, 112.79s/it][I 2025-05-15 22:51:13,131] Trial 44 finished with value: 0.6310553429656474 and parameters: {'C': 7.24695695379761, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 43%|████▎     | 43/100 [1:58:49<3:01:32, 191.09s/it][I 2025-05-15 22:51:59,529] Trial 42 finished with value: 0.6040586373763606 and parameters: {'C': 1.6614793915145314, 'kernel': 'linear', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 44%|████▍     | 44/100 [1:59:35<2:17:50, 147.68s/it][I 2025-05-15 22:53:11,778] Trial 45 finished with value: 0.6254645317790304 and parameters: {'C': 8.931608426673083, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 45%|████▌     | 45/100 [2:00:48<1:54:37, 125.05s/it][I 2025-05-15 22:57:16,686] Trial 46 finished with value: 0.6291682794205884 and parameters: {'C': 4.0096707386261015, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 46%|████▌     | 46/100 [2:04:52<2:24:54, 161.01s/it][I 2025-05-15 22:58:40,019] Trial 47 finished with value: 0.6256982113689984 and parameters: {'C': 3.0309065721039925, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 47%|████▋     | 47/100 [2:06:16<2:01:38, 137.71s/it][I 2025-05-15 23:00:31,081] Trial 48 finished with value: 0.6229779506465406 and parameters: {'C': 38.23003034911215, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 48%|████▊     | 48/100 [2:08:07<1:52:25, 129.71s/it][I 2025-05-15 23:08:12,123] Trial 43 finished with value: 0.6065742560235099 and parameters: {'C': 6.7393050345329915, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 49%|████▉     | 49/100 [2:15:48<3:14:44, 229.11s/it][I 2025-05-15 23:11:10,255] Trial 51 finished with value: 0.566876827369832 and parameters: {'C': 18.60629973914661, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 50%|█████     | 50/100 [2:18:46<2:58:10, 213.82s/it][I 2025-05-15 23:15:03,065] Trial 52 finished with value: 0.622962086926514 and parameters: {'C': 47.99401349419822, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 51%|█████     | 51/100 [2:22:39<2:59:16, 219.52s/it][I 2025-05-15 23:26:02,149] Trial 53 finished with value: 0.6347015858147307 and parameters: {'C': 12.041539637400266, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 52%|█████▏    | 52/100 [2:33:38<4:41:06, 351.39s/it][I 2025-05-15 23:30:16,705] Trial 54 finished with value: 0.6287213776101005 and parameters: {'C': 19.834530834493407, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 53%|█████▎    | 53/100 [2:37:52<4:12:29, 322.34s/it][I 2025-05-15 23:35:53,389] Trial 55 finished with value: 0.6347744559875395 and parameters: {'C': 10.665707001127243, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 54%|█████▍    | 54/100 [2:43:29<4:10:25, 326.64s/it][I 2025-05-15 23:45:59,690] Trial 56 finished with value: 0.6197296611064038 and parameters: {'C': 98.92899301822754, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 55%|█████▌    | 55/100 [2:53:35<5:07:54, 410.54s/it][I 2025-05-15 23:51:25,406] Trial 57 finished with value: 0.6291682794205884 and parameters: {'C': 4.306874706372224, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 56%|█████▌    | 56/100 [2:59:01<4:42:24, 385.10s/it][I 2025-05-15 23:57:36,627] Trial 58 finished with value: 0.4065401749771886 and parameters: {'C': 0.002146749746209105, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 57%|█████▋    | 57/100 [3:05:12<4:33:00, 380.93s/it][I 2025-05-16 00:00:14,430] Trial 59 finished with value: 0.6352908065651557 and parameters: {'C': 12.546494145956926, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 58%|█████▊    | 58/100 [3:07:50<3:39:47, 313.99s/it][I 2025-05-16 00:05:04,020] Trial 60 finished with value: 0.6060867696030017 and parameters: {'C': 0.5045788923300373, 'kernel': 'linear', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 59%|█████▉    | 59/100 [3:12:40<3:29:33, 306.67s/it][I 2025-05-16 00:09:56,327] Trial 61 finished with value: 0.6238532110091743 and parameters: {'C': 2.0566192032458575, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 60%|██████    | 60/100 [3:17:32<3:21:34, 302.36s/it][I 2025-05-16 00:14:26,878] Trial 50 finished with value: 0.6051366597807086 and parameters: {'C': 18.71373187911659, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 61%|██████    | 61/100 [3:22:03<3:10:19, 292.82s/it][I 2025-05-16 00:14:41,332] Trial 62 finished with value: 0.5642223664287259 and parameters: {'C': 23.725123526202008, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 62%|██████▏   | 62/100 [3:22:17<2:12:33, 209.31s/it][I 2025-05-16 00:16:55,141] Trial 63 finished with value: 0.6316427319734157 and parameters: {'C': 9.63218113039428, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 63%|██████▎   | 63/100 [3:24:31<1:55:06, 186.66s/it][I 2025-05-16 00:17:09,161] Trial 64 finished with value: 0.6334267281635703 and parameters: {'C': 10.343373593838793, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 64%|██████▍   | 64/100 [3:24:45<1:20:55, 134.87s/it][I 2025-05-16 00:19:22,190] Trial 65 finished with value: 0.63465234051843 and parameters: {'C': 11.41687844685941, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 65%|██████▌   | 65/100 [3:26:58<1:18:21, 134.32s/it][I 2025-05-16 00:21:10,533] Trial 66 finished with value: 0.6215286114445779 and parameters: {'C': 53.35420499090341, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 66%|██████▌   | 66/100 [3:28:46<1:11:41, 126.52s/it][I 2025-05-16 00:21:29,527] Trial 67 finished with value: 0.6291682794205884 and parameters: {'C': 4.038905058371451, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 67%|██████▋   | 67/100 [3:29:05<51:50, 94.26s/it]   [I 2025-05-16 00:23:31,192] Trial 68 finished with value: 0.6350466819156129 and parameters: {'C': 12.491090508139516, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 68%|██████▊   | 68/100 [3:31:07<54:39, 102.48s/it][I 2025-05-16 00:23:53,120] Trial 69 finished with value: 0.6344574611651879 and parameters: {'C': 12.138698217556307, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 69%|██████▉   | 69/100 [3:31:29<40:27, 78.32s/it] [I 2025-05-16 00:27:35,930] Trial 70 finished with value: 0.6250120506891554 and parameters: {'C': 27.902142045230306, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 70%|███████   | 70/100 [3:35:12<1:00:49, 121.67s/it][I 2025-05-16 00:28:10,623] Trial 71 finished with value: 0.6221155789048293 and parameters: {'C': 30.157584666727303, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 71%|███████   | 71/100 [3:35:46<46:11, 95.57s/it]   [I 2025-05-16 00:31:28,380] Trial 72 finished with value: 0.6372133730089271 and parameters: {'C': 46.34653685875397, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 72%|███████▏  | 72/100 [3:39:04<58:54, 126.23s/it][I 2025-05-16 00:32:08,525] Trial 73 finished with value: 0.6381790258759752 and parameters: {'C': 14.160425876185107, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 73%|███████▎  | 73/100 [3:39:44<45:10, 100.40s/it][I 2025-05-16 00:35:28,098] Trial 74 finished with value: 0.634706695516288 and parameters: {'C': 58.226279512741435, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 74%|███████▍  | 74/100 [3:43:04<56:23, 130.15s/it][I 2025-05-16 00:36:11,313] Trial 75 finished with value: 0.632980544547252 and parameters: {'C': 64.88413511659907, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 75%|███████▌  | 75/100 [3:43:47<43:21, 104.07s/it][I 2025-05-16 00:39:05,022] Trial 76 finished with value: 0.6372133730089271 and parameters: {'C': 46.40290406734268, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 76%|███████▌  | 76/100 [3:46:41<49:59, 124.96s/it][I 2025-05-16 00:39:37,645] Trial 77 finished with value: 0.6345669700426367 and parameters: {'C': 41.941938279288024, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 77%|███████▋  | 77/100 [3:47:13<37:17, 97.26s/it] [I 2025-05-16 00:42:26,087] Trial 78 finished with value: 0.6352890208741866 and parameters: {'C': 42.843418637994795, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 78%|███████▊  | 78/100 [3:50:02<43:29, 118.62s/it][I 2025-05-16 00:43:25,806] Trial 79 finished with value: 0.6308198555706803 and parameters: {'C': 86.4048731433189, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 79%|███████▉  | 79/100 [3:51:02<35:19, 100.95s/it][I 2025-05-16 00:45:19,378] Trial 80 finished with value: 0.5591300891170968 and parameters: {'C': 79.95257368496083, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 80%|████████  | 80/100 [3:52:55<34:54, 104.73s/it][I 2025-05-16 00:47:34,251] Trial 81 finished with value: 0.568281124497992 and parameters: {'C': 16.853298890076594, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 81%|████████  | 81/100 [3:55:10<36:01, 113.78s/it][I 2025-05-16 00:51:06,597] Trial 83 finished with value: 0.6342906032714579 and parameters: {'C': 39.781044990550306, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 82%|████████▏ | 82/100 [3:58:42<43:00, 143.35s/it][I 2025-05-16 00:54:05,284] Trial 84 finished with value: 0.6310343331815126 and parameters: {'C': 25.74255066384125, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 83%|████████▎ | 83/100 [4:01:41<43:37, 153.95s/it][I 2025-05-16 00:54:59,726] Trial 82 finished with value: 0.0 and parameters: {'C': 0.00010972222143378696, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 84%|████████▍ | 84/100 [4:02:35<33:05, 124.10s/it][I 2025-05-16 00:56:56,203] Trial 85 finished with value: 0.6256866693171568 and parameters: {'C': 5.561074078819267, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 85%|████████▌ | 85/100 [4:04:32<30:27, 121.81s/it][I 2025-05-16 00:57:52,143] Trial 86 finished with value: 0.6258056508974501 and parameters: {'C': 6.513439921043272, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 86%|████████▌ | 86/100 [4:05:28<23:48, 102.05s/it][I 2025-05-16 01:00:41,269] Trial 88 finished with value: 0.625799522707529 and parameters: {'C': 16.458933176325733, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 87%|████████▋ | 87/100 [4:08:17<26:28, 122.18s/it][I 2025-05-16 01:02:32,622] Trial 87 finished with value: 0.5617157540733434 and parameters: {'C': 0.031112781408383028, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 88%|████████▊ | 88/100 [4:10:08<23:47, 118.93s/it][I 2025-05-16 01:03:44,079] Trial 89 finished with value: 0.634940421369773 and parameters: {'C': 38.15509720771581, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 89%|████████▉ | 89/100 [4:11:20<19:11, 104.68s/it][I 2025-05-16 01:05:45,856] Trial 90 finished with value: 0.6379310344827587 and parameters: {'C': 47.129684716644036, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 90%|█████████ | 90/100 [4:13:22<18:18, 109.81s/it][I 2025-05-16 01:06:40,583] Trial 91 finished with value: 0.6307554670533827 and parameters: {'C': 24.459648237122224, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 91%|█████████ | 91/100 [4:14:16<13:59, 93.29s/it] [I 2025-05-16 01:10:02,474] Trial 93 finished with value: 0.6361454732451504 and parameters: {'C': 50.80028086873453, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 92%|█████████▏| 92/100 [4:17:38<16:46, 125.87s/it][I 2025-05-16 01:10:02,707] Trial 92 finished with value: 0.5723087340874323 and parameters: {'C': 0.00028496103808829974, 'kernel': 'linear', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 93%|█████████▎| 93/100 [4:17:38<10:17, 88.18s/it] [I 2025-05-16 01:11:29,248] Trial 49 finished with value: 0.60349657104221 and parameters: {'C': 38.54716217320751, 'kernel': 'linear', 'degree': 4, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 94%|█████████▍| 94/100 [4:19:05<08:46, 87.69s/it][I 2025-05-16 01:13:19,379] Trial 94 finished with value: 0.6260414738011479 and parameters: {'C': 15.899472550888193, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 95%|█████████▌| 95/100 [4:20:55<07:52, 94.42s/it][I 2025-05-16 01:13:54,446] Trial 95 finished with value: 0.630943083709514 and parameters: {'C': 69.38290694274815, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 96%|█████████▌| 96/100 [4:21:30<05:06, 76.61s/it][I 2025-05-16 01:15:18,386] Trial 96 finished with value: 0.6342216086986966 and parameters: {'C': 60.00176261279292, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 97%|█████████▋| 97/100 [4:22:54<03:56, 78.81s/it][I 2025-05-16 01:17:22,891] Trial 97 finished with value: 0.634706695516288 and parameters: {'C': 58.3408804783331, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 98%|█████████▊| 98/100 [4:24:59<03:05, 92.52s/it][I 2025-05-16 01:17:47,420] Trial 98 finished with value: 0.6352886857155926 and parameters: {'C': 54.39537846055859, 'kernel': 'rbf', 'degree': 2, 'gamma': 'auto'}. Best is trial 27 with value: 0.6381790258759752.\n",
      " 99%|█████████▉| 99/100 [4:25:23<01:12, 72.12s/it][I 2025-05-16 01:19:10,014] Trial 99 finished with value: 0.6263857484846647 and parameters: {'C': 24.553237763550392, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 27 with value: 0.6381790258759752.\n",
      "100%|██████████| 100/100 [4:26:46<00:00, 160.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[svm]  best F1=0.6382   Best hyperparameters: {'C': 14.113323810506161, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'} bootstrap 95% CI=(0.6143, 0.6624)\n"
     ]
    }
   ],
   "source": [
    "svm_results = {}\n",
    "svm_results['bert_without_augmentation'] = optimize_model(\n",
    "    model_name = 'svm',\n",
    "    X_train    = X_bert_reg_tr,\n",
    "    y_train    = y_bert_reg_tr,\n",
    "    X_val      = X_bert_reg_val,\n",
    "    y_val      = y_bert_reg_val,\n",
    "    n_trials = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 04:02:06,168] A new study created in memory with name: no-name-adcad1c0-2e61-40d5-a989-98101c03b836\n",
      "  0%|          | 0/50 [00:00<?, ?it/s][I 2025-05-15 04:15:08,474] Trial 0 finished with value: 0.5988766053831862 and parameters: {'C': 14.294579237140699, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 0 with value: 0.5988766053831862.\n",
      "  2%|▏         | 1/50 [13:02<10:38:52, 782.30s/it][I 2025-05-15 04:24:05,265] Trial 4 finished with value: 0.6339189024611019 and parameters: {'C': 0.26744797760450434, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 4 with value: 0.6339189024611019.\n",
      "  4%|▍         | 2/50 [21:59<8:30:18, 637.88s/it] [I 2025-05-15 04:28:09,161] Trial 6 finished with value: 0.6220872796775561 and parameters: {'C': 0.05849434719062382, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 4 with value: 0.6339189024611019.\n",
      "  6%|▌         | 3/50 [26:02<5:58:45, 457.99s/it][I 2025-05-15 04:31:24,912] Trial 7 finished with value: 0.49127055259419994 and parameters: {'C': 0.05604980615150419, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'scale'}. Best is trial 4 with value: 0.6339189024611019.\n",
      "  8%|▊         | 4/50 [29:18<4:31:44, 354.45s/it][I 2025-05-15 04:35:45,590] Trial 1 finished with value: 0.5993218898277131 and parameters: {'C': 0.0620892615297633, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 4 with value: 0.6339189024611019.\n",
      " 10%|█         | 5/50 [33:39<4:00:28, 320.64s/it][I 2025-05-15 04:39:38,297] Trial 5 finished with value: 0.6112846992365064 and parameters: {'C': 0.2629199021537822, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 4 with value: 0.6339189024611019.\n",
      " 12%|█▏        | 6/50 [37:32<3:33:12, 290.74s/it][I 2025-05-15 04:45:35,881] Trial 9 finished with value: 0.6408520942031074 and parameters: {'C': 3.587660416025804, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 14%|█▍        | 7/50 [43:29<3:44:01, 312.59s/it][I 2025-05-15 04:49:33,852] Trial 8 finished with value: 0.6101837409010097 and parameters: {'C': 0.010389620280236957, 'kernel': 'rbf', 'degree': 4, 'gamma': 'scale'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 16%|█▌        | 8/50 [47:27<3:22:11, 288.84s/it][I 2025-05-15 04:58:28,959] Trial 3 finished with value: 0.5893901510293535 and parameters: {'C': 0.003195296976712751, 'kernel': 'rbf', 'degree': 4, 'gamma': 'auto'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 18%|█▊        | 9/50 [56:22<4:09:58, 365.83s/it][I 2025-05-15 05:02:05,994] Trial 13 finished with value: 0.614210218574113 and parameters: {'C': 0.0384353557093954, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 20%|██        | 10/50 [59:59<3:33:15, 319.89s/it][I 2025-05-15 05:16:30,534] Trial 15 finished with value: 0.6113624840659402 and parameters: {'C': 0.0004492987944258821, 'kernel': 'linear', 'degree': 2, 'gamma': 'scale'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 22%|██▏       | 11/50 [1:14:24<5:16:16, 486.58s/it][I 2025-05-15 05:26:19,721] Trial 10 finished with value: 0.5886118365598529 and parameters: {'C': 0.002890799072290293, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 24%|██▍       | 12/50 [1:24:13<5:27:56, 517.80s/it][I 2025-05-15 05:28:37,427] Trial 2 finished with value: 0.32445488464434863 and parameters: {'C': 0.0005499054153485813, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 26%|██▌       | 13/50 [1:26:31<4:08:18, 402.65s/it][I 2025-05-15 05:38:58,629] Trial 18 finished with value: 0.6387855748841116 and parameters: {'C': 3.7941867023947906, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 28%|██▊       | 14/50 [1:36:52<4:41:11, 468.66s/it][I 2025-05-15 05:40:56,196] Trial 17 finished with value: 0.6350175520222853 and parameters: {'C': 94.53623166390643, 'kernel': 'rbf', 'degree': 3, 'gamma': 'auto'}. Best is trial 9 with value: 0.6408520942031074.\n",
      " 30%|███       | 15/50 [1:38:50<3:31:39, 362.83s/it][I 2025-05-15 05:46:42,430] Trial 19 finished with value: 0.6450623624863275 and parameters: {'C': 2.8949898887397625, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 19 with value: 0.6450623624863275.\n",
      " 32%|███▏      | 16/50 [1:44:36<3:22:46, 357.84s/it][I 2025-05-15 05:48:00,332] Trial 20 finished with value: 0.6465231674765588 and parameters: {'C': 2.3343100849943466, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 34%|███▍      | 17/50 [1:45:54<2:30:30, 273.66s/it][I 2025-05-15 06:04:12,882] Trial 22 finished with value: 0.6374190058486364 and parameters: {'C': 4.988694334693785, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 36%|███▌      | 18/50 [2:02:06<4:17:57, 483.67s/it][I 2025-05-15 06:05:19,640] Trial 21 finished with value: 0.6337781271026479 and parameters: {'C': 7.347609087742398, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 38%|███▊      | 19/50 [2:03:13<3:05:12, 358.45s/it][I 2025-05-15 06:06:40,190] Trial 23 finished with value: 0.6463682988976807 and parameters: {'C': 2.765758730838848, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 40%|████      | 20/50 [2:04:34<2:17:30, 275.02s/it][I 2025-05-15 06:07:40,599] Trial 24 finished with value: 0.6431918517884057 and parameters: {'C': 1.702809539114184, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 42%|████▏     | 21/50 [2:05:34<1:41:47, 210.60s/it][I 2025-05-15 06:24:40,151] Trial 25 finished with value: 0.6405061865048853 and parameters: {'C': 0.9861199168254138, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 44%|████▍     | 22/50 [2:22:33<3:31:34, 453.38s/it][I 2025-05-15 06:28:53,143] Trial 28 finished with value: 0.632983117816092 and parameters: {'C': 0.5872599208466392, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 46%|████▌     | 23/50 [2:26:46<2:56:57, 393.25s/it][I 2025-05-15 06:35:25,384] Trial 16 finished with value: 0.3022222222222222 and parameters: {'C': 0.0001496496395834431, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 48%|████▊     | 24/50 [2:33:19<2:50:16, 392.94s/it][I 2025-05-15 06:43:53,118] Trial 11 finished with value: 0.6081515372712889 and parameters: {'C': 2.101539772279831, 'kernel': 'linear', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 50%|█████     | 25/50 [2:41:46<2:58:04, 427.39s/it][I 2025-05-15 07:03:00,948] Trial 27 finished with value: 0.6048973783255706 and parameters: {'C': 0.6057081333766122, 'kernel': 'linear', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 52%|█████▏    | 26/50 [3:00:54<4:17:24, 643.54s/it][I 2025-05-15 07:07:08,154] Trial 26 finished with value: 0.6066633548455167 and parameters: {'C': 0.6970897801350336, 'kernel': 'linear', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 54%|█████▍    | 27/50 [3:05:01<3:21:06, 524.63s/it][I 2025-05-15 07:15:51,220] Trial 29 finished with value: 0.6264568035893716 and parameters: {'C': 49.046013725147276, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 56%|█████▌    | 28/50 [3:13:45<3:12:11, 524.16s/it][I 2025-05-15 07:29:25,993] Trial 32 finished with value: 0.6282755724377418 and parameters: {'C': 30.3125924016586, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 58%|█████▊    | 29/50 [3:27:19<3:33:58, 611.35s/it][I 2025-05-15 07:42:51,094] Trial 36 finished with value: 0.5993153294364802 and parameters: {'C': 14.695605505496262, 'kernel': 'sigmoid', 'degree': 2, 'gamma': 'auto'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 60%|██████    | 30/50 [3:40:44<3:43:09, 669.48s/it][I 2025-05-15 07:48:56,207] Trial 35 finished with value: 0.6334517064444072 and parameters: {'C': 12.799876364681309, 'kernel': 'rbf', 'degree': 2, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 62%|██████▏   | 31/50 [3:46:50<3:03:05, 578.17s/it][I 2025-05-15 07:49:46,825] Trial 33 finished with value: 0.6257964445190172 and parameters: {'C': 34.84796984172379, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 64%|██████▍   | 32/50 [3:47:40<2:05:58, 419.90s/it][I 2025-05-15 07:50:52,108] Trial 34 finished with value: 0.6282823334338257 and parameters: {'C': 31.43368237528126, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 66%|██████▌   | 33/50 [3:48:45<1:28:49, 313.51s/it][I 2025-05-15 08:07:25,119] Trial 38 finished with value: 0.6449669167001042 and parameters: {'C': 1.603724708155816, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 68%|██████▊   | 34/50 [4:05:18<2:17:57, 517.36s/it][I 2025-05-15 08:09:39,961] Trial 39 finished with value: 0.6457130358705161 and parameters: {'C': 1.9381022000629777, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 70%|███████   | 35/50 [4:07:33<1:40:39, 402.61s/it][I 2025-05-15 08:13:58,012] Trial 40 finished with value: 0.4415720185854414 and parameters: {'C': 1.4548304936949468, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 72%|███████▏  | 36/50 [4:11:51<1:23:49, 359.24s/it][I 2025-05-15 08:14:38,214] Trial 37 finished with value: 0.6335958269763975 and parameters: {'C': 10.48312368768293, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 74%|███████▍  | 37/50 [4:12:32<57:05, 263.53s/it]  [I 2025-05-15 08:39:20,480] Trial 41 finished with value: 0.607171781231052 and parameters: {'C': 0.18446938678989605, 'kernel': 'sigmoid', 'degree': 3, 'gamma': 'auto'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 76%|███████▌  | 38/50 [4:37:14<2:05:49, 629.15s/it][I 2025-05-15 08:41:14,274] Trial 42 finished with value: 0.6076132959628651 and parameters: {'C': 0.19976350727182826, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 78%|███████▊  | 39/50 [4:39:08<1:26:59, 474.54s/it][I 2025-05-15 08:45:55,979] Trial 43 finished with value: 0.6072121785893378 and parameters: {'C': 0.29515661636605517, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 80%|████████  | 40/50 [4:43:49<1:09:26, 416.69s/it][I 2025-05-15 08:46:00,260] Trial 44 finished with value: 0.6096598165563683 and parameters: {'C': 0.23842559336395194, 'kernel': 'sigmoid', 'degree': 4, 'gamma': 'auto'}. Best is trial 20 with value: 0.6465231674765588.\n",
      " 82%|████████▏ | 41/50 [4:43:54<43:56, 292.97s/it]  "
     ]
    }
   ],
   "source": [
    "svm_results['bert_with_augmentation'] = optimize_model(\n",
    "    model_name = 'svm',\n",
    "    X_train    = X_bert_aug_tr,\n",
    "    y_train    = y_bert_aug_tr,\n",
    "    X_val      = X_bert_aug_val,\n",
    "    y_val      = y_bert_aug_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['tfidf_without_augmentation'] = optimize_model(\n",
    "    model_name = 'svm',\n",
    "    X_train    = X_tfidf_reg_tr,\n",
    "    y_train    = y_tfidf_reg_tr,\n",
    "    X_val      = X_tfidf_reg_val,\n",
    "    y_val      = y_tfidf_reg_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dvg-ZLIiyunH"
   },
   "outputs": [],
   "source": [
    "svm_results['tfidf_with_augmentation'] = optimize_model(\n",
    "    model_name = 'svm',\n",
    "    X_train    = X_tfidf_aug_tr,\n",
    "    y_train    = y_tfidf_aug_tr,\n",
    "    X_val      = X_tfidf_aug_val,\n",
    "    y_val      = y_tfidf_aug_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM results:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Best Eval Score</th>\n",
       "      <th>CI (95%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_without_augmentation</td>\n",
       "      <td>{'C': 14.113323810506161, 'kernel': 'rbf', 'de...</td>\n",
       "      <td>0.638179</td>\n",
       "      <td>(0.6142822431331255, 0.6624198682227005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Experiment  \\\n",
       "0  bert_without_augmentation   \n",
       "\n",
       "                                     Best Parameters  Best Eval Score  \\\n",
       "0  {'C': 14.113323810506161, 'kernel': 'rbf', 'de...         0.638179   \n",
       "\n",
       "                                   CI (95%)  \n",
       "0  (0.6142822431331255, 0.6624198682227005)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"SVM results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Eval Score': value[1],\n",
    "        'CI (95%)': value[2]\n",
    "    }\n",
    "    for key, value in svm_results.items()\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 16:40:23,531] A new study created in memory with name: no-name-d54c6898-92de-4c8c-a120-ed40d3d5c6a4\n",
      "  0%|          | 0/100 [00:00<?, ?it/s][I 2025-05-15 16:40:43,521] Trial 1 finished with value: 0.5872804061025436 and parameters: {'n_estimators': 129, 'learning_rate': 0.008665749332990566, 'booster': 'gblinear', 'max_depth': 3, 'min_child_weight': 10, 'colsample_bytree': 0.9518770512514789, 'subsample': 0.5930836128270734, 'reg_alpha': 0.00023049735300880039, 'reg_lambda': 0.002832714474683314, 'gamma': 0.33945450614017925}. Best is trial 1 with value: 0.5872804061025436.\n",
      "  1%|          | 1/100 [00:19<32:59, 19.99s/it][I 2025-05-15 16:41:12,617] Trial 2 finished with value: 0.5971101781226416 and parameters: {'n_estimators': 112, 'learning_rate': 0.02697426543063567, 'booster': 'gbtree', 'max_depth': 3, 'min_child_weight': 2, 'colsample_bytree': 0.8869256703299871, 'subsample': 0.6379367146114844, 'reg_alpha': 7.709908337574803, 'reg_lambda': 1.3138231424348024e-08, 'gamma': 0.00017632512892779982, 'grow_policy': 'depthwise'}. Best is trial 2 with value: 0.5971101781226416.\n",
      "  2%|▏         | 2/100 [00:49<41:24, 25.35s/it][I 2025-05-15 16:46:19,560] Trial 0 finished with value: 0.6164844029111269 and parameters: {'n_estimators': 58, 'learning_rate': 0.008108704714040746, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.6577040310879744, 'subsample': 0.7626695611953221, 'reg_alpha': 0.0009995675935801191, 'reg_lambda': 1.685315932337047e-08, 'gamma': 0.011790596600849908, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.6164844029111269.\n",
      "  3%|▎         | 3/100 [05:56<4:08:51, 153.93s/it][I 2025-05-15 16:46:42,940] Trial 5 finished with value: 0.0 and parameters: {'n_estimators': 97, 'learning_rate': 0.004212607155942624, 'booster': 'gblinear', 'max_depth': 8, 'min_child_weight': 6, 'colsample_bytree': 0.5701371181814779, 'subsample': 0.8795404420633208, 'reg_alpha': 1.2941255563007648, 'reg_lambda': 2.0272032552492592e-06, 'gamma': 4.878728104052074e-07}. Best is trial 0 with value: 0.6164844029111269.\n",
      "  4%|▍         | 4/100 [06:19<2:43:50, 102.41s/it][I 2025-05-15 16:47:22,817] Trial 6 finished with value: 0.5014774912340233 and parameters: {'n_estimators': 88, 'learning_rate': 0.03640827454821241, 'booster': 'gblinear', 'max_depth': 10, 'min_child_weight': 2, 'colsample_bytree': 0.7871095236912997, 'subsample': 0.9494138748339473, 'reg_alpha': 2.003121664263914e-06, 'reg_lambda': 0.653284954969691, 'gamma': 0.0009992977813449901}. Best is trial 0 with value: 0.6164844029111269.\n",
      "  5%|▌         | 5/100 [06:59<2:06:25, 79.84s/it] [I 2025-05-15 16:48:44,401] Trial 7 finished with value: 0.5116079212853406 and parameters: {'n_estimators': 171, 'learning_rate': 0.0011157960176122633, 'booster': 'gblinear', 'max_depth': 3, 'min_child_weight': 2, 'colsample_bytree': 0.7482808810344205, 'subsample': 0.5715319984429734, 'reg_alpha': 3.229098324821864e-07, 'reg_lambda': 0.11688385093835155, 'gamma': 0.5953211173698263}. Best is trial 0 with value: 0.6164844029111269.\n",
      "  6%|▌         | 6/100 [08:20<2:06:00, 80.43s/it][I 2025-05-15 16:51:13,190] Trial 8 finished with value: 0.623786590757762 and parameters: {'n_estimators': 153, 'learning_rate': 0.0846881811989101, 'booster': 'gbtree', 'max_depth': 3, 'min_child_weight': 4, 'colsample_bytree': 0.9494684039906975, 'subsample': 0.5850249596489656, 'reg_alpha': 8.610802971724334e-08, 'reg_lambda': 0.0001290886157774786, 'gamma': 8.688533013956427e-05, 'grow_policy': 'lossguide'}. Best is trial 8 with value: 0.623786590757762.\n",
      "  7%|▋         | 7/100 [10:49<2:39:18, 102.78s/it][I 2025-05-15 16:52:49,469] Trial 9 finished with value: 0.5923885817460253 and parameters: {'n_estimators': 121, 'learning_rate': 0.018170782034835447, 'booster': 'gbtree', 'max_depth': 3, 'min_child_weight': 7, 'colsample_bytree': 0.5350953722285562, 'subsample': 0.7430247470545546, 'reg_alpha': 0.008799250085791506, 'reg_lambda': 1.1747461967759858, 'gamma': 0.018905859992090265, 'grow_policy': 'depthwise'}. Best is trial 8 with value: 0.623786590757762.\n",
      "  8%|▊         | 8/100 [12:25<2:34:25, 100.71s/it][I 2025-05-15 16:53:31,211] Trial 10 finished with value: 0.5206586125088777 and parameters: {'n_estimators': 104, 'learning_rate': 0.007545668095711611, 'booster': 'gblinear', 'max_depth': 8, 'min_child_weight': 4, 'colsample_bytree': 0.5589492857872026, 'subsample': 0.7954407419673128, 'reg_alpha': 0.008625787529930935, 'reg_lambda': 0.0010530398521839245, 'gamma': 3.198447758816101e-05}. Best is trial 8 with value: 0.623786590757762.\n",
      "  9%|▉         | 9/100 [13:07<2:04:47, 82.28s/it] [I 2025-05-15 16:59:17,885] Trial 11 finished with value: 0.6034985401080246 and parameters: {'n_estimators': 130, 'learning_rate': 0.0010709589173489818, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 5, 'colsample_bytree': 0.5288038139668547, 'subsample': 0.9628720030452542, 'reg_alpha': 0.0012607460542633266, 'reg_lambda': 9.455230007477297e-08, 'gamma': 1.384220528815066e-07, 'grow_policy': 'depthwise'}. Best is trial 8 with value: 0.623786590757762.\n",
      " 10%|█         | 10/100 [18:54<4:05:51, 163.90s/it][I 2025-05-15 17:08:06,930] Trial 4 finished with value: 0.5923770816360897 and parameters: {'n_estimators': 176, 'learning_rate': 0.01004306368438545, 'booster': 'dart', 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.7588239135137207, 'subsample': 0.531836241999183, 'reg_alpha': 5.761296908816768e-07, 'reg_lambda': 5.27497521306963e-08, 'gamma': 0.0091711572122159, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.003012626111768904, 'skip_drop': 0.0019211841594406604}. Best is trial 8 with value: 0.623786590757762.\n",
      " 11%|█         | 11/100 [27:43<6:48:53, 275.65s/it][I 2025-05-15 17:10:09,957] Trial 13 finished with value: 0.6229090909090909 and parameters: {'n_estimators': 56, 'learning_rate': 0.17945561266513485, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.662342417512398, 'subsample': 0.6931555308244273, 'reg_alpha': 3.306965518246018e-05, 'reg_lambda': 1.1026938016806497e-05, 'gamma': 2.4305435210136635e-05, 'grow_policy': 'lossguide'}. Best is trial 8 with value: 0.623786590757762.\n",
      " 12%|█▏        | 12/100 [29:46<5:36:11, 229.22s/it][I 2025-05-15 17:10:13,159] Trial 3 finished with value: 0.6147352902012166 and parameters: {'n_estimators': 178, 'learning_rate': 0.01098019117479045, 'booster': 'dart', 'max_depth': 6, 'min_child_weight': 4, 'colsample_bytree': 0.7184675571806425, 'subsample': 0.6602243771545231, 'reg_alpha': 3.881118009729147e-06, 'reg_lambda': 0.03833772540772091, 'gamma': 1.37881990836007e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.010992741439645577, 'skip_drop': 0.008312404937453628}. Best is trial 8 with value: 0.623786590757762.\n",
      " 13%|█▎        | 13/100 [29:49<3:53:05, 160.75s/it][I 2025-05-15 17:13:47,457] Trial 14 finished with value: 0.635306721518716 and parameters: {'n_estimators': 155, 'learning_rate': 0.19734846031881773, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'colsample_bytree': 0.6653016279442904, 'subsample': 0.6693297277133574, 'reg_alpha': 1.1718112882952548e-08, 'reg_lambda': 8.761678390499264e-06, 'gamma': 5.079816969214663e-06, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.635306721518716.\n",
      " 14%|█▍        | 14/100 [33:23<4:13:35, 176.93s/it][I 2025-05-15 17:14:05,428] Trial 15 finished with value: 0.6283491163520116 and parameters: {'n_estimators': 152, 'learning_rate': 0.18048450950855255, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'colsample_bytree': 0.9979109815745685, 'subsample': 0.6865477658259496, 'reg_alpha': 3.77722502450483e-08, 'reg_lambda': 2.0225236580749332e-05, 'gamma': 3.865171374054613e-06, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.635306721518716.\n",
      " 15%|█▌        | 15/100 [33:41<3:02:46, 129.01s/it][I 2025-05-15 17:17:25,647] Trial 16 finished with value: 0.632822301827451 and parameters: {'n_estimators': 148, 'learning_rate': 0.19429534562210682, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 9, 'colsample_bytree': 0.8707372221732099, 'subsample': 0.5024897558797798, 'reg_alpha': 1.0977069459834209e-08, 'reg_lambda': 4.501473304119053e-05, 'gamma': 1.9910271620603034e-06, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.635306721518716.\n",
      " 16%|█▌        | 16/100 [37:02<3:30:37, 150.45s/it][I 2025-05-15 17:18:03,158] Trial 17 finished with value: 0.6355658495785884 and parameters: {'n_estimators': 155, 'learning_rate': 0.19404889987105178, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 9, 'colsample_bytree': 0.8527624023314502, 'subsample': 0.812552640111678, 'reg_alpha': 1.730170149418481e-08, 'reg_lambda': 3.7047012155966777e-06, 'gamma': 1.2690862416780742e-06, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 17%|█▋        | 17/100 [37:39<2:41:08, 116.49s/it][I 2025-05-15 17:33:14,822] Trial 12 finished with value: 0.6275989941655954 and parameters: {'n_estimators': 198, 'learning_rate': 0.18976160258882044, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 9, 'colsample_bytree': 0.9852055064798362, 'subsample': 0.5161107206025795, 'reg_alpha': 1.5420917983680726e-08, 'reg_lambda': 3.473979111291e-05, 'gamma': 9.043864109635447e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0002238102575377151, 'skip_drop': 0.0020373377769580354}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 18%|█▊        | 18/100 [52:51<8:05:45, 355.43s/it][I 2025-05-15 17:37:00,044] Trial 18 finished with value: 0.6224161845799925 and parameters: {'n_estimators': 150, 'learning_rate': 0.08242680658013518, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 10, 'colsample_bytree': 0.8140416686846315, 'subsample': 0.5251092049338868, 'reg_alpha': 1.029990793432119e-08, 'reg_lambda': 3.9782722209907214e-07, 'gamma': 1.141464761642998e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 4.5394871814574366e-08, 'skip_drop': 1.23041029076082e-08}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 19%|█▉        | 19/100 [56:36<7:07:02, 316.32s/it][I 2025-05-15 17:39:57,306] Trial 20 finished with value: 0.6232147933316301 and parameters: {'n_estimators': 189, 'learning_rate': 0.06595243090866208, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 10, 'colsample_bytree': 0.815336135362752, 'subsample': 0.8373014733240367, 'reg_alpha': 1.0729029968877535e-05, 'reg_lambda': 1.1055033435380262e-06, 'gamma': 2.004154710543848e-08, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 20%|██        | 20/100 [59:33<6:06:05, 274.57s/it][I 2025-05-15 17:42:25,019] Trial 22 finished with value: 0.6211376404494382 and parameters: {'n_estimators': 168, 'learning_rate': 0.05231812040564751, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 8, 'colsample_bytree': 0.6644673582858438, 'subsample': 0.8907365593896768, 'reg_alpha': 1.994046989659878e-07, 'reg_lambda': 2.8758856603708212e-06, 'gamma': 7.774646453765327e-07, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 21%|██        | 21/100 [1:02:01<5:11:22, 236.49s/it][I 2025-05-15 17:42:45,643] Trial 21 finished with value: 0.623601782452559 and parameters: {'n_estimators': 190, 'learning_rate': 0.08074628818535252, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 8, 'colsample_bytree': 0.6224278731294376, 'subsample': 0.8428071327991566, 'reg_alpha': 1.8211253707772593e-05, 'reg_lambda': 1.222763567269266e-06, 'gamma': 2.1460196107249103e-08, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 22%|██▏       | 22/100 [1:02:22<3:43:13, 171.71s/it][I 2025-05-15 17:44:38,166] Trial 23 finished with value: 0.6227236116998729 and parameters: {'n_estimators': 145, 'learning_rate': 0.12605618519694228, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 9, 'colsample_bytree': 0.8760709193659726, 'subsample': 0.7333364601547809, 'reg_alpha': 9.462790077948228e-08, 'reg_lambda': 0.0001420137532776475, 'gamma': 2.245545612771784e-06, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 23%|██▎       | 23/100 [1:04:14<3:17:33, 153.95s/it][I 2025-05-15 17:44:58,911] Trial 24 finished with value: 0.6232288596291198 and parameters: {'n_estimators': 146, 'learning_rate': 0.1197464531516352, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 9, 'colsample_bytree': 0.8653834376786476, 'subsample': 0.7543864603387032, 'reg_alpha': 1.0153448146569503e-08, 'reg_lambda': 0.0001979931957722665, 'gamma': 4.819874324777517e-06, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 24%|██▍       | 24/100 [1:04:35<2:24:22, 113.98s/it][I 2025-05-15 17:46:31,656] Trial 25 finished with value: 0.6288967774653083 and parameters: {'n_estimators': 136, 'learning_rate': 0.12059574424228973, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 9, 'colsample_bytree': 0.8707857940105851, 'subsample': 0.8086289543870969, 'reg_alpha': 1.059214651585659e-08, 'reg_lambda': 0.0005661826361090967, 'gamma': 6.850030962047027e-07, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 25%|██▌       | 25/100 [1:06:08<2:14:30, 107.61s/it][I 2025-05-15 17:49:12,862] Trial 19 finished with value: 0.6249985701212537 and parameters: {'n_estimators': 200, 'learning_rate': 0.07417666128979268, 'booster': 'dart', 'max_depth': 6, 'min_child_weight': 10, 'colsample_bytree': 0.8303388879147324, 'subsample': 0.8390356289225409, 'reg_alpha': 1.0993850046250665e-05, 'reg_lambda': 1.0937384152419326e-06, 'gamma': 1.0034178149808846e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.8966202480454853e-08, 'skip_drop': 1.8500186780599713e-08}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 26%|██▌       | 26/100 [1:08:49<2:32:32, 123.69s/it][I 2025-05-15 17:51:19,318] Trial 26 finished with value: 0.6288865015166836 and parameters: {'n_estimators': 160, 'learning_rate': 0.0431279215807448, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'colsample_bytree': 0.9091460557781119, 'subsample': 0.6217838913048296, 'reg_alpha': 6.888480027993643e-07, 'reg_lambda': 1.0070314550832787e-05, 'gamma': 1.457969327274373e-07, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 27%|██▋       | 27/100 [1:10:55<2:31:29, 124.52s/it][I 2025-05-15 17:52:10,965] Trial 27 finished with value: 0.6211349191833748 and parameters: {'n_estimators': 162, 'learning_rate': 0.043949116306562785, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'colsample_bytree': 0.7051614311078597, 'subsample': 0.6336968791129152, 'reg_alpha': 1.2169639180090568e-06, 'reg_lambda': 0.006551828860974727, 'gamma': 0.0003667928637243466, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 28%|██▊       | 28/100 [1:11:47<2:03:11, 102.66s/it][I 2025-05-15 17:53:35,115] Trial 30 finished with value: 0.6318710481788228 and parameters: {'n_estimators': 138, 'learning_rate': 0.12674440042221688, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'colsample_bytree': 0.6073717578584273, 'subsample': 0.6928372224743128, 'reg_alpha': 6.498896370028835e-08, 'reg_lambda': 1.878690467840947e-07, 'gamma': 2.6821181855263788e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 29%|██▉       | 29/100 [1:13:11<1:54:54, 97.10s/it] [I 2025-05-15 17:53:41,768] Trial 29 finished with value: 0.6000015267933142 and parameters: {'n_estimators': 138, 'learning_rate': 0.002730496406114031, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'colsample_bytree': 0.7129286863644886, 'subsample': 0.7063629448312597, 'reg_alpha': 1.3581341545739821e-06, 'reg_lambda': 0.006702604838898347, 'gamma': 0.0006764760839482354, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 30%|███       | 30/100 [1:13:18<1:21:37, 69.97s/it][I 2025-05-15 17:54:21,680] Trial 32 finished with value: 0.6037327528774261 and parameters: {'n_estimators': 74, 'learning_rate': 0.02834496663555147, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 9, 'colsample_bytree': 0.9394911719569773, 'subsample': 0.9945437643569022, 'reg_alpha': 0.18588634825553046, 'reg_lambda': 2.7170298570603763e-05, 'gamma': 2.046149445166648e-06, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 31%|███       | 31/100 [1:13:58<1:10:05, 60.95s/it][I 2025-05-15 17:55:04,184] Trial 28 finished with value: 0.6255537160979736 and parameters: {'n_estimators': 161, 'learning_rate': 0.038213611779923336, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'colsample_bytree': 0.9110766052588809, 'subsample': 0.6323312552368423, 'reg_alpha': 1.6683883563795333e-06, 'reg_lambda': 8.972792842312344e-06, 'gamma': 0.00022095585683748267, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 32%|███▏      | 32/100 [1:14:40<1:02:48, 55.42s/it][I 2025-05-15 17:55:31,618] Trial 33 finished with value: 0.6281100270492901 and parameters: {'n_estimators': 119, 'learning_rate': 0.12222563691919297, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'colsample_bytree': 0.6100812914161338, 'subsample': 0.7716011497788923, 'reg_alpha': 6.052549494358208e-08, 'reg_lambda': 1.7638226501701144e-07, 'gamma': 3.0069205396139268e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 33%|███▎      | 33/100 [1:15:08<52:30, 47.02s/it]  [I 2025-05-15 17:56:28,148] Trial 34 finished with value: 0.6323510190628543 and parameters: {'n_estimators': 123, 'learning_rate': 0.12969821686724245, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 8, 'colsample_bytree': 0.6154391132665469, 'subsample': 0.5626014946927196, 'reg_alpha': 6.117558010038962e-08, 'reg_lambda': 6.123225332413899e-08, 'gamma': 2.2460177242789932e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 34%|███▍      | 34/100 [1:16:04<54:51, 49.88s/it][I 2025-05-15 17:57:20,918] Trial 35 finished with value: 0.6314008679094194 and parameters: {'n_estimators': 136, 'learning_rate': 0.1498750307545736, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 9, 'colsample_bytree': 0.6180547685781825, 'subsample': 0.5644100229384985, 'reg_alpha': 5.6777967218710385e-08, 'reg_lambda': 3.969829683135869e-08, 'gamma': 1.039057774764229e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 35%|███▌      | 35/100 [1:16:57<54:58, 50.74s/it][I 2025-05-15 17:58:10,972] Trial 36 finished with value: 0.6213210858656764 and parameters: {'n_estimators': 129, 'learning_rate': 0.1916853014509498, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 9, 'colsample_bytree': 0.6353859085993355, 'subsample': 0.5422874300003816, 'reg_alpha': 2.9987810345057455e-08, 'reg_lambda': 2.8189728091713553e-08, 'gamma': 1.4322503925348489e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 36%|███▌      | 36/100 [1:17:47<53:54, 50.54s/it][I 2025-05-15 17:58:40,238] Trial 38 finished with value: 0.6051054052844376 and parameters: {'n_estimators': 111, 'learning_rate': 0.10117777300337076, 'booster': 'gblinear', 'max_depth': 4, 'min_child_weight': 10, 'colsample_bytree': 0.6873999926400255, 'subsample': 0.6106226876948155, 'reg_alpha': 1.919600266013811e-07, 'reg_lambda': 6.553709466459899e-05, 'gamma': 2.647053407090216e-07}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 37%|███▋      | 37/100 [1:18:16<46:21, 44.15s/it][I 2025-05-15 17:58:54,612] Trial 37 finished with value: 0.6329576019662857 and parameters: {'n_estimators': 115, 'learning_rate': 0.19279249101796078, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 10, 'colsample_bytree': 0.6775265956568763, 'subsample': 0.5023373021785612, 'reg_alpha': 2.292453576105098e-07, 'reg_lambda': 2.9938109233410184e-08, 'gamma': 1.196717430244587e-06, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 38%|███▊      | 38/100 [1:18:31<36:23, 35.22s/it][I 2025-05-15 18:07:47,813] Trial 39 finished with value: 0.6161618553222723 and parameters: {'n_estimators': 114, 'learning_rate': 0.017691739581944955, 'booster': 'dart', 'max_depth': 10, 'min_child_weight': 6, 'colsample_bytree': 0.7469310125114301, 'subsample': 0.5036008317894003, 'reg_alpha': 2.5083841202904174e-07, 'reg_lambda': 4.003984404759005e-06, 'gamma': 0.003094146436947244, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.9795652343243471, 'skip_drop': 0.5736133410462999}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 39%|███▉      | 39/100 [1:27:24<3:07:41, 184.61s/it][I 2025-05-15 18:08:07,397] Trial 41 finished with value: 0.599249816097905 and parameters: {'n_estimators': 88, 'learning_rate': 0.06212400648837592, 'booster': 'gblinear', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.7939095296523452, 'subsample': 0.8965363792601251, 'reg_alpha': 0.000407365553205828, 'reg_lambda': 1.0643165041217334e-08, 'gamma': 5.470717441084099e-08}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 40%|████      | 40/100 [1:27:43<2:15:06, 135.11s/it][I 2025-05-15 18:10:38,336] Trial 42 finished with value: 0.6242100288564125 and parameters: {'n_estimators': 182, 'learning_rate': 0.09436070986093169, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 10, 'colsample_bytree': 0.7778094460530582, 'subsample': 0.6628634722398441, 'reg_alpha': 4.750762516141472e-06, 'reg_lambda': 4.287386689022523e-07, 'gamma': 1.4346793127267737e-06, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 41%|████      | 41/100 [1:30:14<2:17:31, 139.86s/it][I 2025-05-15 18:11:44,593] Trial 43 finished with value: 0.6203110392749374 and parameters: {'n_estimators': 100, 'learning_rate': 0.15574363558509247, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 9, 'colsample_bytree': 0.5830116880757107, 'subsample': 0.5582636447726529, 'reg_alpha': 2.6085931956464175e-08, 'reg_lambda': 4.0216548506257465e-07, 'gamma': 8.087107862932913e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 42%|████▏     | 42/100 [1:31:21<1:53:51, 117.78s/it][I 2025-05-15 18:13:24,696] Trial 40 finished with value: 0.6181031769249854 and parameters: {'n_estimators': 181, 'learning_rate': 0.06405388882976748, 'booster': 'dart', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.7678056071615802, 'subsample': 0.6639056067180175, 'reg_alpha': 5.668313824984621e-05, 'reg_lambda': 4.6886263359596e-06, 'gamma': 2.1248820844780573e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.5592900562300637, 'skip_drop': 3.872696874907874e-06}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 43%|████▎     | 43/100 [1:33:01<1:46:51, 112.47s/it][I 2025-05-15 18:13:27,598] Trial 44 finished with value: 0.6351090408578053 and parameters: {'n_estimators': 128, 'learning_rate': 0.19600931861794677, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 8, 'colsample_bytree': 0.6883439965414277, 'subsample': 0.5911589800940573, 'reg_alpha': 4.062935878884047e-07, 'reg_lambda': 6.235132149308717, 'gamma': 3.892719519432876e-06, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 44%|████▍     | 44/100 [1:33:04<1:14:17, 79.60s/it] [I 2025-05-15 18:15:06,141] Trial 45 finished with value: 0.6319248524346497 and parameters: {'n_estimators': 125, 'learning_rate': 0.1506743926063379, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 8, 'colsample_bytree': 0.6890495906864587, 'subsample': 0.594275780609662, 'reg_alpha': 1.520065429721731e-07, 'reg_lambda': 5.126895607299636e-08, 'gamma': 4.2950403113253777e-07, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 45%|████▌     | 45/100 [1:34:42<1:18:10, 85.28s/it][I 2025-05-15 18:15:59,917] Trial 46 finished with value: 0.6331282256428346 and parameters: {'n_estimators': 170, 'learning_rate': 0.19120633878010157, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.6825426403844206, 'subsample': 0.5001354779631608, 'reg_alpha': 4.1896345526176534e-07, 'reg_lambda': 7.552457564127642, 'gamma': 3.0485400146174915e-07, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 46%|████▌     | 46/100 [1:35:36<1:08:14, 75.83s/it][I 2025-05-15 18:16:37,763] Trial 48 finished with value: 0.18661441728657457 and parameters: {'n_estimators': 173, 'learning_rate': 0.00635327537481122, 'booster': 'gblinear', 'max_depth': 7, 'min_child_weight': 5, 'colsample_bytree': 0.7265820119152687, 'subsample': 0.5402829974122851, 'reg_alpha': 4.0523654541775343e-07, 'reg_lambda': 8.463837835884108, 'gamma': 9.341956741380977e-08}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 47%|████▋     | 47/100 [1:36:14<56:55, 64.44s/it]  [I 2025-05-15 18:18:05,432] Trial 47 finished with value: 0.632562662057044 and parameters: {'n_estimators': 171, 'learning_rate': 0.18929321992407114, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.6501526019942033, 'subsample': 0.5001375188848478, 'reg_alpha': 4.533769545546679e-07, 'reg_lambda': 5.722291435380087, 'gamma': 5.795415189559999e-06, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 48%|████▊     | 48/100 [1:37:41<1:01:53, 71.41s/it][I 2025-05-15 18:19:02,560] Trial 31 finished with value: 0.6198198118973394 and parameters: {'n_estimators': 78, 'learning_rate': 0.003830042419672049, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 9, 'colsample_bytree': 0.9276361405655967, 'subsample': 0.9148273348885203, 'reg_alpha': 0.00010126889658859276, 'reg_lambda': 5.6367565195963955e-05, 'gamma': 9.251891068916959e-07, 'grow_policy': 'lossguide'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 49%|████▉     | 49/100 [1:38:39<57:03, 67.12s/it]  [I 2025-05-15 18:20:07,320] Trial 49 finished with value: 0.6050997835095087 and parameters: {'n_estimators': 155, 'learning_rate': 0.0016932285678004158, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 5, 'colsample_bytree': 0.6798712891922112, 'subsample': 0.6011884563490973, 'reg_alpha': 3.518250217819985e-06, 'reg_lambda': 3.2678498659112076, 'gamma': 2.7678406105552416e-07, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 50%|█████     | 50/100 [1:39:43<55:20, 66.41s/it][I 2025-05-15 18:21:48,108] Trial 50 finished with value: 0.631155303030303 and parameters: {'n_estimators': 155, 'learning_rate': 0.0995001944667921, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 5, 'colsample_bytree': 0.5025603293926492, 'subsample': 0.716877240332042, 'reg_alpha': 3.2483288447537275e-06, 'reg_lambda': 0.2780028516645441, 'gamma': 2.8707050013584005e-07, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 51%|█████     | 51/100 [1:41:24<1:02:39, 76.73s/it][I 2025-05-15 18:22:54,963] Trial 52 finished with value: 0.6341064525721083 and parameters: {'n_estimators': 106, 'learning_rate': 0.09376942869090965, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 3, 'colsample_bytree': 0.7388940597227392, 'subsample': 0.5833919796311111, 'reg_alpha': 2.5018205494829973e-08, 'reg_lambda': 0.22980155677713263, 'gamma': 5.839060361830125e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 52%|█████▏    | 52/100 [1:42:31<59:00, 73.77s/it]  [I 2025-05-15 18:23:29,827] Trial 51 finished with value: 0.6280535548201501 and parameters: {'n_estimators': 157, 'learning_rate': 0.09795837476460328, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 5, 'colsample_bytree': 0.7349336962025588, 'subsample': 0.5904221862133276, 'reg_alpha': 7.35464202681219e-07, 'reg_lambda': 0.26677182307989455, 'gamma': 2.9215067773457985e-07, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 53%|█████▎    | 53/100 [1:43:06<48:38, 62.09s/it][I 2025-05-15 18:24:48,142] Trial 53 finished with value: 0.6273173445806723 and parameters: {'n_estimators': 165, 'learning_rate': 0.15584253541581208, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 7, 'colsample_bytree': 0.7330453192133638, 'subsample': 0.5335271175163063, 'reg_alpha': 2.486477018252736e-08, 'reg_lambda': 0.044145603580440354, 'gamma': 3.5580121190934816e-06, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 54%|█████▍    | 54/100 [1:44:24<51:20, 66.96s/it][I 2025-05-15 18:26:11,697] Trial 54 finished with value: 0.6229178919959114 and parameters: {'n_estimators': 106, 'learning_rate': 0.15590999722981574, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 2, 'colsample_bytree': 0.7418974546235262, 'subsample': 0.5306735575340341, 'reg_alpha': 2.789639385444129e-08, 'reg_lambda': 1.860911473706162, 'gamma': 0.0396714563300465, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 55%|█████▌    | 55/100 [1:45:48<53:57, 71.94s/it][I 2025-05-15 18:26:14,533] Trial 55 finished with value: 0.6306976010101011 and parameters: {'n_estimators': 104, 'learning_rate': 0.15540843025283163, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 3, 'colsample_bytree': 0.7018957669427935, 'subsample': 0.5376945684755352, 'reg_alpha': 2.6936361220004718e-08, 'reg_lambda': 1.6153746971661143, 'gamma': 7.393747445282109e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 56%|█████▌    | 56/100 [1:45:51<37:33, 51.21s/it][I 2025-05-15 18:28:09,651] Trial 56 finished with value: 0.6310465187336667 and parameters: {'n_estimators': 112, 'learning_rate': 0.15501420347775322, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 3, 'colsample_bytree': 0.7000046984567028, 'subsample': 0.5804731252461558, 'reg_alpha': 2.2620589454005596e-08, 'reg_lambda': 2.1527770414131258, 'gamma': 6.722049886510321e-05, 'grow_policy': 'depthwise'}. Best is trial 17 with value: 0.6355658495785884.\n",
      " 57%|█████▋    | 57/100 [1:47:46<50:26, 70.38s/it][I 2025-05-15 18:28:31,009] Trial 57 finished with value: 0.6399801203273199 and parameters: {'n_estimators': 97, 'learning_rate': 0.19893565401854227, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 3, 'colsample_bytree': 0.698884472870104, 'subsample': 0.5802363751849828, 'reg_alpha': 1.1264120790978405e-07, 'reg_lambda': 0.7242980309966568, 'gamma': 1.0153525598898357e-05, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 58%|█████▊    | 58/100 [1:48:07<38:58, 55.67s/it][I 2025-05-15 18:28:48,799] Trial 60 finished with value: 0.50609307930564 and parameters: {'n_estimators': 65, 'learning_rate': 0.10669179708929075, 'booster': 'gblinear', 'max_depth': 7, 'min_child_weight': 4, 'colsample_bytree': 0.6509044310180816, 'subsample': 0.6505691357621658, 'reg_alpha': 9.772197924791477e-08, 'reg_lambda': 0.6336984014290757, 'gamma': 6.984267724611908e-06}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 59%|█████▉    | 59/100 [1:48:25<30:16, 44.31s/it][I 2025-05-15 18:31:24,060] Trial 61 finished with value: 0.630679877979375 and parameters: {'n_estimators': 88, 'learning_rate': 0.08376588365601637, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 2, 'colsample_bytree': 0.8407997203803782, 'subsample': 0.6180289878518727, 'reg_alpha': 9.671980987390661, 'reg_lambda': 0.6363143781173292, 'gamma': 1.3813637906260981e-05, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 60%|██████    | 60/100 [1:51:00<51:43, 77.59s/it][I 2025-05-15 18:34:54,337] Trial 58 finished with value: 0.6267613382189221 and parameters: {'n_estimators': 94, 'learning_rate': 0.1085194000327477, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 3, 'colsample_bytree': 0.6551406421105603, 'subsample': 0.5768470844946397, 'reg_alpha': 1.287171004915882e-07, 'reg_lambda': 0.6286015821941883, 'gamma': 6.203481896347474e-08, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 2.2147735279970244e-06, 'skip_drop': 3.026962778475232e-06}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 61%|██████    | 61/100 [1:54:30<1:16:18, 117.40s/it][I 2025-05-15 18:36:54,332] Trial 59 finished with value: 0.624996584046129 and parameters: {'n_estimators': 94, 'learning_rate': 0.10655816492407726, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 2, 'colsample_bytree': 0.6507570531387444, 'subsample': 0.7809725222342792, 'reg_alpha': 1.1182872854942719e-07, 'reg_lambda': 0.7465679549515034, 'gamma': 7.851402029973497e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 2.022304306484379e-06, 'skip_drop': 3.908874623237355e-06}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 62%|██████▏   | 62/100 [1:56:30<1:14:50, 118.18s/it][I 2025-05-15 18:37:27,148] Trial 63 finished with value: 0.626223744865563 and parameters: {'n_estimators': 116, 'learning_rate': 0.19774819486548628, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 3, 'colsample_bytree': 0.7599418218595609, 'subsample': 0.7756763464021643, 'reg_alpha': 2.767622706166713e-07, 'reg_lambda': 4.154361687439963, 'gamma': 1.2911663213258696e-06, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 63%|██████▎   | 63/100 [1:57:03<57:05, 92.57s/it]   [I 2025-05-15 18:39:12,483] Trial 64 finished with value: 0.628773202825406 and parameters: {'n_estimators': 117, 'learning_rate': 0.1999863246174749, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 3, 'colsample_bytree': 0.6722117619690681, 'subsample': 0.5522578448007611, 'reg_alpha': 2.747158209666659e-07, 'reg_lambda': 9.963863202774963, 'gamma': 9.755463380999778e-07, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 64%|██████▍   | 64/100 [1:58:48<57:50, 96.40s/it][I 2025-05-15 18:39:36,170] Trial 65 finished with value: 0.624458474709731 and parameters: {'n_estimators': 107, 'learning_rate': 0.13460639299179922, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 4, 'colsample_bytree': 0.6804232536583343, 'subsample': 0.5176304335245995, 'reg_alpha': 0.0028134825091532525, 'reg_lambda': 0.04073941667930287, 'gamma': 3.6839647464548745e-06, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 65%|██████▌   | 65/100 [1:59:12<43:30, 74.59s/it][I 2025-05-15 18:40:02,692] Trial 62 finished with value: 0.627748378654319 and parameters: {'n_estimators': 94, 'learning_rate': 0.0745305174502477, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 4, 'colsample_bytree': 0.7562534247167233, 'subsample': 0.7970651874788237, 'reg_alpha': 0.031777484694941134, 'reg_lambda': 0.06596634999822723, 'gamma': 4.490058289862176e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 3.3792301084584034e-06, 'skip_drop': 1.7179338283603558e-06}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 66%|██████▌   | 66/100 [1:59:39<34:05, 60.17s/it][I 2025-05-15 18:41:49,718] Trial 66 finished with value: 0.6362272023446438 and parameters: {'n_estimators': 142, 'learning_rate': 0.13124629485674272, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 4, 'colsample_bytree': 0.7191994303585673, 'subsample': 0.5169372395995621, 'reg_alpha': 7.571647218167405e-07, 'reg_lambda': 0.07051232371250148, 'gamma': 3.639089164440081e-06, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 67%|██████▋   | 67/100 [2:01:26<40:49, 74.22s/it][I 2025-05-15 18:42:09,392] Trial 68 finished with value: 0.6285932643152568 and parameters: {'n_estimators': 132, 'learning_rate': 0.1687580371515762, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 6, 'colsample_bytree': 0.7159816976827179, 'subsample': 0.852592510419363, 'reg_alpha': 1.019141793843171e-06, 'reg_lambda': 0.014972789323752681, 'gamma': 0.0001351304025479634, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 68%|██████▊   | 68/100 [2:01:45<30:51, 57.86s/it][I 2025-05-15 18:43:35,393] Trial 67 finished with value: 0.6298929836995039 and parameters: {'n_estimators': 144, 'learning_rate': 0.1737108545324674, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 4, 'colsample_bytree': 0.7188034526612055, 'subsample': 0.6436633972939702, 'reg_alpha': 9.183704034901727e-07, 'reg_lambda': 0.0014861349536704302, 'gamma': 4.336921302866284e-05, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 69%|██████▉   | 69/100 [2:03:11<34:15, 66.30s/it][I 2025-05-15 18:45:49,938] Trial 69 finished with value: 0.634788189987163 and parameters: {'n_estimators': 146, 'learning_rate': 0.13908795206697422, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 6, 'colsample_bytree': 0.7165883092944241, 'subsample': 0.5728182475084258, 'reg_alpha': 1.028226261349824e-06, 'reg_lambda': 0.014152710075095457, 'gamma': 0.0001931931964874353, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 70%|███████   | 70/100 [2:05:26<43:23, 86.77s/it][I 2025-05-15 18:47:57,818] Trial 70 finished with value: 0.6257805327481916 and parameters: {'n_estimators': 147, 'learning_rate': 0.05266157492150115, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 3, 'colsample_bytree': 0.7933171597067429, 'subsample': 0.818154495189242, 'reg_alpha': 1.4729300747556254e-08, 'reg_lambda': 0.15800138728500984, 'gamma': 5.771522324488543e-07, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 71%|███████   | 71/100 [2:07:34<47:54, 99.11s/it][I 2025-05-15 18:50:42,280] Trial 71 finished with value: 0.6196810976175026 and parameters: {'n_estimators': 144, 'learning_rate': 0.011698123868913847, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 3, 'colsample_bytree': 0.5934891967387534, 'subsample': 0.8226055306451018, 'reg_alpha': 1.5606162404964423e-08, 'reg_lambda': 0.2394891936440219, 'gamma': 1.7012512101910928e-05, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 72%|███████▏  | 72/100 [2:10:18<55:23, 118.71s/it][I 2025-05-15 18:51:34,011] Trial 73 finished with value: 0.6301611446874034 and parameters: {'n_estimators': 142, 'learning_rate': 0.1387641653725957, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 7, 'colsample_bytree': 0.6901610763834923, 'subsample': 0.5781467945078009, 'reg_alpha': 4.77821437710643e-08, 'reg_lambda': 0.10451546788544365, 'gamma': 0.00024654672207616383, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 73%|███████▎  | 73/100 [2:11:10<44:22, 98.62s/it] [I 2025-05-15 18:52:39,108] Trial 72 finished with value: 0.6179338391448789 and parameters: {'n_estimators': 140, 'learning_rate': 0.011119964784863205, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 3, 'colsample_bytree': 0.793567424562042, 'subsample': 0.6070040745673718, 'reg_alpha': 4.9410648158573745e-08, 'reg_lambda': 0.021910328881985252, 'gamma': 0.0002740113072588227, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 74%|███████▍  | 74/100 [2:12:15<38:22, 88.56s/it][I 2025-05-15 18:54:09,485] Trial 74 finished with value: 0.6378116131514853 and parameters: {'n_estimators': 141, 'learning_rate': 0.13463873914488173, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 7, 'colsample_bytree': 0.6370274973988607, 'subsample': 0.6084424709595819, 'reg_alpha': 6.815570495800802e-06, 'reg_lambda': 0.08783174493945135, 'gamma': 0.0013681673490052173, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 75%|███████▌  | 75/100 [2:13:45<37:07, 89.11s/it][I 2025-05-15 18:56:20,414] Trial 75 finished with value: 0.6300772665032737 and parameters: {'n_estimators': 133, 'learning_rate': 0.12800776332275193, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 5, 'colsample_bytree': 0.7008089205494995, 'subsample': 0.6784112282370961, 'reg_alpha': 5.050687132337908e-07, 'reg_lambda': 3.1177016039255463, 'gamma': 0.0005255011365004618, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 76%|███████▌  | 76/100 [2:15:56<40:39, 101.65s/it][I 2025-05-15 18:56:54,146] Trial 76 finished with value: 0.6308645453596349 and parameters: {'n_estimators': 132, 'learning_rate': 0.12155580423622059, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 6, 'colsample_bytree': 0.7059966507373748, 'subsample': 0.5504853209117297, 'reg_alpha': 5.103696939078489e-07, 'reg_lambda': 1.1379025386086143, 'gamma': 0.0008279439782181993, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 77%|███████▋  | 77/100 [2:16:30<31:09, 81.28s/it] [I 2025-05-15 18:59:19,346] Trial 77 finished with value: 0.6321869408434946 and parameters: {'n_estimators': 133, 'learning_rate': 0.11595321952469499, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 7, 'colsample_bytree': 0.7042311697838739, 'subsample': 0.6773962814961524, 'reg_alpha': 7.838666485303074e-06, 'reg_lambda': 0.006256217996199853, 'gamma': 0.0011891682302274614, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 78%|███████▊  | 78/100 [2:18:55<36:49, 100.45s/it][I 2025-05-15 18:59:45,267] Trial 80 finished with value: 0.5858723141373385 and parameters: {'n_estimators': 150, 'learning_rate': 0.08916764264337516, 'booster': 'gblinear', 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.6431119029453144, 'subsample': 0.6214982214075166, 'reg_alpha': 1.7256594450638808e-05, 'reg_lambda': 0.01741358460720992, 'gamma': 0.00013594355254945524}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 79%|███████▉  | 79/100 [2:19:21<27:19, 78.09s/it] [I 2025-05-15 19:01:07,933] Trial 78 finished with value: 0.629220019039927 and parameters: {'n_estimators': 152, 'learning_rate': 0.08833531513368574, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 7, 'colsample_bytree': 0.6309878161815315, 'subsample': 0.6299282720229102, 'reg_alpha': 6.163820471443164e-06, 'reg_lambda': 0.005323154848877566, 'gamma': 0.002753268885049907, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 80%|████████  | 80/100 [2:20:44<26:29, 79.47s/it][I 2025-05-15 19:01:45,601] Trial 79 finished with value: 0.629758263585529 and parameters: {'n_estimators': 149, 'learning_rate': 0.08324626607235264, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.6317831918109816, 'subsample': 0.6211002078818544, 'reg_alpha': 7.266303219809966e-06, 'reg_lambda': 0.08482257136300994, 'gamma': 0.001587961878298426, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 81%|████████  | 81/100 [2:21:22<21:11, 66.93s/it][I 2025-05-15 19:02:27,645] Trial 82 finished with value: 0.6286052714117916 and parameters: {'n_estimators': 159, 'learning_rate': 0.07324409725374689, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 8, 'colsample_bytree': 0.6672904704609569, 'subsample': 0.5677427846848772, 'reg_alpha': 1.6157856041460411e-06, 'reg_lambda': 0.08947732401433736, 'gamma': 0.012069189966298343, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 82%|████████▏ | 82/100 [2:22:04<17:50, 59.46s/it][I 2025-05-15 19:05:14,596] Trial 83 finished with value: 0.6296232680636665 and parameters: {'n_estimators': 166, 'learning_rate': 0.1682725339639736, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.6663939320953817, 'subsample': 0.5196051678890727, 'reg_alpha': 1.5446484233480503e-06, 'reg_lambda': 0.1694489928148782, 'gamma': 2.806290783848351e-06, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 83%|████████▎ | 83/100 [2:24:51<25:59, 91.71s/it][I 2025-05-15 19:05:40,821] Trial 84 finished with value: 0.6321795172752299 and parameters: {'n_estimators': 127, 'learning_rate': 0.1375488285629869, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.9734200494887313, 'subsample': 0.5176672384232595, 'reg_alpha': 2.2750847308822936e-06, 'reg_lambda': 0.35953065222393654, 'gamma': 2.5674706548629225e-06, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 84%|████████▍ | 84/100 [2:25:17<19:13, 72.06s/it][I 2025-05-15 19:06:43,401] Trial 81 finished with value: 0.6187923291946417 and parameters: {'n_estimators': 122, 'learning_rate': 0.024436063224574596, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.6323312057662206, 'subsample': 0.6351325717975158, 'reg_alpha': 1.754016562018189e-06, 'reg_lambda': 0.08674038600870364, 'gamma': 0.0023806409254738424, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 85%|████████▌ | 85/100 [2:26:19<17:18, 69.22s/it][I 2025-05-15 19:07:30,041] Trial 85 finished with value: 0.6350106673766291 and parameters: {'n_estimators': 128, 'learning_rate': 0.1395336970569001, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 7, 'colsample_bytree': 0.7244400476976065, 'subsample': 0.5936871174595817, 'reg_alpha': 7.818199306044414e-08, 'reg_lambda': 0.467133429874786, 'gamma': 0.06417518410427026, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 86%|████████▌ | 86/100 [2:27:06<14:34, 62.44s/it][I 2025-05-15 19:07:56,631] Trial 86 finished with value: 0.6339874877233629 and parameters: {'n_estimators': 122, 'learning_rate': 0.17221827085886454, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.7223041385690445, 'subsample': 0.5990355078017588, 'reg_alpha': 8.556160859850665e-08, 'reg_lambda': 1.9914483256114116e-06, 'gamma': 1.1127608391496282e-05, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 87%|████████▋ | 87/100 [2:27:33<11:11, 51.69s/it][I 2025-05-15 19:09:31,099] Trial 88 finished with value: 0.6337898430270719 and parameters: {'n_estimators': 137, 'learning_rate': 0.13868441144653032, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.72492206450174, 'subsample': 0.5945686905089614, 'reg_alpha': 8.273313662497824e-08, 'reg_lambda': 0.4457916214476196, 'gamma': 1.220694013643121e-05, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 88%|████████▊ | 88/100 [2:29:07<12:54, 64.52s/it][I 2025-05-15 19:09:57,920] Trial 89 finished with value: 0.6354289176378345 and parameters: {'n_estimators': 138, 'learning_rate': 0.14081252762856494, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.7498145555890151, 'subsample': 0.5862094009973516, 'reg_alpha': 1.664483900153314e-07, 'reg_lambda': 0.0003188672314762417, 'gamma': 0.27237554296211125, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 89%|████████▉ | 89/100 [2:29:34<09:45, 53.21s/it][I 2025-05-15 19:10:00,693] Trial 87 finished with value: 0.6330808801005587 and parameters: {'n_estimators': 164, 'learning_rate': 0.17383357086524728, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.7274544643230849, 'subsample': 0.5504935062549448, 'reg_alpha': 8.522593253685e-08, 'reg_lambda': 0.0003445073975742116, 'gamma': 0.0048304940269756555, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 90%|█████████ | 90/100 [2:29:37<06:20, 38.08s/it][I 2025-05-15 19:14:51,743] Trial 90 finished with value: 0.6344439728353142 and parameters: {'n_estimators': 142, 'learning_rate': 0.14163153741696574, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 7, 'colsample_bytree': 0.774984936637397, 'subsample': 0.5710829076963373, 'reg_alpha': 1.774057515364306e-08, 'reg_lambda': 0.0005224155570746889, 'gamma': 0.08382512579471355, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 91%|█████████ | 91/100 [2:34:28<17:05, 113.97s/it][I 2025-05-15 19:15:21,469] Trial 92 finished with value: 0.6271680911680911 and parameters: {'n_estimators': 128, 'learning_rate': 0.1420537412585503, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.7801603166710164, 'subsample': 0.6511337113572759, 'reg_alpha': 1.3927641369933973e-07, 'reg_lambda': 0.0011260547262809922, 'gamma': 0.09843019613127513, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 92%|█████████▏| 92/100 [2:34:57<11:49, 88.70s/it] [I 2025-05-15 19:15:40,686] Trial 91 finished with value: 0.6245511245511246 and parameters: {'n_estimators': 140, 'learning_rate': 0.11532120097645057, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.7774004633093342, 'subsample': 0.5652593106990563, 'reg_alpha': 1.8210258656143965e-07, 'reg_lambda': 0.001042027525762619, 'gamma': 0.17200878829878544, 'grow_policy': 'lossguide'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 93%|█████████▎| 93/100 [2:35:17<07:54, 67.85s/it][I 2025-05-15 19:16:40,327] Trial 93 finished with value: 0.6272484858800335 and parameters: {'n_estimators': 128, 'learning_rate': 0.14662609693447012, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.8515269284673076, 'subsample': 0.5695201867178578, 'reg_alpha': 1.5962453720663725e-07, 'reg_lambda': 0.0018540737857232921, 'gamma': 0.12155448080511569, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 94%|█████████▍| 94/100 [2:36:16<06:32, 65.39s/it][I 2025-05-15 19:17:28,509] Trial 94 finished with value: 0.626166119796693 and parameters: {'n_estimators': 142, 'learning_rate': 0.11487993160151737, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.8289563996169611, 'subsample': 0.5695836730530983, 'reg_alpha': 3.7150021620809103e-08, 'reg_lambda': 0.00010193543926808656, 'gamma': 0.9521670550716946, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 95%|█████████▌| 95/100 [2:37:04<05:01, 60.23s/it][I 2025-05-15 19:23:09,271] Trial 95 finished with value: 0.6304882492257242 and parameters: {'n_estimators': 135, 'learning_rate': 0.16514934424847166, 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.8166198274730948, 'subsample': 0.6105297649093181, 'reg_alpha': 1.4697080758939654e-08, 'reg_lambda': 0.00019513494372078253, 'gamma': 0.7200764141164068, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 96%|█████████▌| 96/100 [2:42:45<09:37, 144.39s/it][I 2025-05-15 19:23:43,181] Trial 96 finished with value: 0.6246024108409429 and parameters: {'n_estimators': 141, 'learning_rate': 0.1745515778648311, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 8, 'colsample_bytree': 0.8043154060699669, 'subsample': 0.5878395525734739, 'reg_alpha': 2.030624714491361e-08, 'reg_lambda': 1.6608960765273634e-05, 'gamma': 0.6857812571544669, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 97%|█████████▋| 97/100 [2:43:19<05:33, 111.24s/it][I 2025-05-15 19:24:10,726] Trial 98 finished with value: 0.6216235911387485 and parameters: {'n_estimators': 52, 'learning_rate': 0.1754572470413705, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 8, 'colsample_bytree': 0.893888033206433, 'subsample': 0.5811574008643603, 'reg_alpha': 4.284689366908291e-08, 'reg_lambda': 0.0005398647507232935, 'gamma': 0.30070486061555807, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 98%|█████████▊| 98/100 [2:43:47<02:52, 86.13s/it] [I 2025-05-15 19:24:13,320] Trial 99 finished with value: 0.584616106186389 and parameters: {'n_estimators': 154, 'learning_rate': 0.19961072845861866, 'booster': 'gblinear', 'max_depth': 4, 'min_child_weight': 9, 'colsample_bytree': 0.7503187577363606, 'subsample': 0.6027218547502855, 'reg_alpha': 6.701012649580042e-07, 'reg_lambda': 0.02599140016879049, 'gamma': 0.3703556571142299}. Best is trial 57 with value: 0.6399801203273199.\n",
      " 99%|█████████▉| 99/100 [2:43:49<01:01, 61.07s/it][I 2025-05-15 19:24:14,219] Trial 97 finished with value: 0.6209927014042868 and parameters: {'n_estimators': 52, 'learning_rate': 0.1685729806845848, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.8967177997933493, 'subsample': 0.6100195714280845, 'reg_alpha': 1.357943914019322e-08, 'reg_lambda': 1.6425123718008422e-05, 'gamma': 0.3397277699115031, 'grow_policy': 'depthwise'}. Best is trial 57 with value: 0.6399801203273199.\n",
      "100%|██████████| 100/100 [2:43:50<00:00, 43.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xgboost]  best F1=0.6400   Best hyperparameters: {'n_estimators': 97, 'learning_rate': 0.19893565401854227, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 3, 'colsample_bytree': 0.698884472870104, 'subsample': 0.5802363751849828, 'reg_alpha': 1.1264120790978405e-07, 'reg_lambda': 0.7242980309966568, 'gamma': 1.0153525598898357e-05, 'grow_policy': 'depthwise'} bootstrap 95% CI=(0.6136, 0.6633)\n"
     ]
    }
   ],
   "source": [
    "xgb_results = {}\n",
    "xgb_results['bert_without_augmentation'] = optimize_model(\n",
    "    model_name = 'xgboost',\n",
    "    X_train    = X_bert_reg_tr,\n",
    "    y_train    = y_bert_reg_tr,\n",
    "    X_val      = X_bert_reg_val,\n",
    "    y_val      = y_bert_reg_val,\n",
    "    n_trials = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['bert_with_augmentation'] = optimize_model(\n",
    "    model_name = 'xgboost',\n",
    "    X_train    = X_bert_aug_tr,\n",
    "    y_train    = y_bert_aug_tr,\n",
    "    X_val      = X_bert_aug_val,\n",
    "    y_val      = y_bert_aug_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 13:45:37,701] A new study created in memory with name: no-name-c2b88e3f-1372-4634-ac26-54b8b04482c5\n",
      "  0%|          | 0/50 [00:00<?, ?it/s][I 2025-05-15 13:45:54,275] Trial 0 finished with value: 0.0 and parameters: {'n_estimators': 146, 'learning_rate': 0.15020861438022654, 'booster': 'gblinear', 'max_depth': 10, 'min_child_weight': 4, 'colsample_bytree': 0.8467262422988333, 'subsample': 0.8104585153371822, 'reg_alpha': 0.009353432542695172, 'reg_lambda': 0.0006814361471557467, 'gamma': 0.00016276417702419104}. Best is trial 0 with value: 0.0.\n",
      "  2%|▏         | 1/50 [00:16<13:32, 16.57s/it][I 2025-05-15 13:46:00,494] Trial 2 finished with value: 0.0 and parameters: {'n_estimators': 194, 'learning_rate': 0.06725107414171043, 'booster': 'gblinear', 'max_depth': 6, 'min_child_weight': 8, 'colsample_bytree': 0.6337009009790975, 'subsample': 0.5620620904155708, 'reg_alpha': 0.1067107676093894, 'reg_lambda': 0.001083519849260911, 'gamma': 7.114473067716964e-06}. Best is trial 0 with value: 0.0.\n",
      "  4%|▍         | 2/50 [00:22<08:23, 10.48s/it][I 2025-05-15 13:46:09,860] Trial 1 finished with value: 0.47645063033180496 and parameters: {'n_estimators': 51, 'learning_rate': 0.007121690042207904, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 2, 'colsample_bytree': 0.7720242506734982, 'subsample': 0.8285004784725359, 'reg_alpha': 2.659440600117815e-05, 'reg_lambda': 0.008550426378598567, 'gamma': 4.2230119405972744e-08, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.47645063033180496.\n",
      "  6%|▌         | 3/50 [00:32<07:48,  9.97s/it][I 2025-05-15 13:46:12,048] Trial 3 finished with value: 0.006657789613848202 and parameters: {'n_estimators': 156, 'learning_rate': 0.0017299675153441111, 'booster': 'gblinear', 'max_depth': 6, 'min_child_weight': 3, 'colsample_bytree': 0.6393407388660548, 'subsample': 0.9022241786074146, 'reg_alpha': 0.0017742593366773137, 'reg_lambda': 5.762996311831929e-08, 'gamma': 0.12541778158922107}. Best is trial 1 with value: 0.47645063033180496.\n",
      "  8%|▊         | 4/50 [00:34<05:17,  6.90s/it][I 2025-05-15 13:50:25,464] Trial 5 finished with value: 0.5563611826769721 and parameters: {'n_estimators': 104, 'learning_rate': 0.04904632648691196, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 7, 'colsample_bytree': 0.732922186187241, 'subsample': 0.8579037024387224, 'reg_alpha': 0.0322400651932168, 'reg_lambda': 3.9600252759312255e-08, 'gamma': 0.002287085535077612, 'grow_policy': 'lossguide'}. Best is trial 5 with value: 0.5563611826769721.\n",
      " 10%|█         | 5/50 [04:47<1:11:50, 95.80s/it][I 2025-05-15 13:50:50,377] Trial 7 finished with value: 0.24340154895624966 and parameters: {'n_estimators': 73, 'learning_rate': 0.01815729008414753, 'booster': 'gblinear', 'max_depth': 8, 'min_child_weight': 6, 'colsample_bytree': 0.9763943913569514, 'subsample': 0.8907396592724458, 'reg_alpha': 8.32078470377114e-05, 'reg_lambda': 0.00013344927495246095, 'gamma': 3.862022625855418e-07}. Best is trial 5 with value: 0.5563611826769721.\n",
      " 12%|█▏        | 6/50 [05:12<52:34, 71.69s/it]  [I 2025-05-15 13:56:50,815] Trial 4 finished with value: 0.4974520619910691 and parameters: {'n_estimators': 112, 'learning_rate': 0.02199235017963, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 9, 'colsample_bytree': 0.5383040754952296, 'subsample': 0.9911650185818038, 'reg_alpha': 2.2526874610269198e-06, 'reg_lambda': 2.9676718640643052e-05, 'gamma': 2.960215973742963e-07, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.03007685113153493, 'skip_drop': 0.00012123323905164569}. Best is trial 5 with value: 0.5563611826769721.\n",
      " 14%|█▍        | 7/50 [11:13<1:59:01, 166.09s/it][I 2025-05-15 13:57:39,164] Trial 9 finished with value: 0.3786324786324786 and parameters: {'n_estimators': 126, 'learning_rate': 0.035334803372892, 'booster': 'gblinear', 'max_depth': 7, 'min_child_weight': 6, 'colsample_bytree': 0.6470367693703603, 'subsample': 0.6782460025737505, 'reg_alpha': 9.040555727510759e-07, 'reg_lambda': 0.00013603184185472344, 'gamma': 3.0592242508761844e-06}. Best is trial 5 with value: 0.5563611826769721.\n",
      " 16%|█▌        | 8/50 [12:01<1:30:01, 128.61s/it][I 2025-05-15 14:00:10,834] Trial 10 finished with value: 0.5529114523218056 and parameters: {'n_estimators': 119, 'learning_rate': 0.08908449516665229, 'booster': 'gbtree', 'max_depth': 6, 'min_child_weight': 8, 'colsample_bytree': 0.6746313284411964, 'subsample': 0.9535001418672957, 'reg_alpha': 0.2683841042219996, 'reg_lambda': 0.010159659394094024, 'gamma': 0.0004518048401638462, 'grow_policy': 'lossguide'}. Best is trial 5 with value: 0.5563611826769721.\n",
      " 18%|█▊        | 9/50 [14:33<1:32:48, 135.82s/it][I 2025-05-15 14:02:37,754] Trial 11 finished with value: 0.5730713802775089 and parameters: {'n_estimators': 175, 'learning_rate': 0.15142792946501002, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.9022632511364057, 'subsample': 0.7200415579028555, 'reg_alpha': 3.825156967763569e-06, 'reg_lambda': 1.5131125863012986e-05, 'gamma': 0.15691021668291794, 'grow_policy': 'depthwise'}. Best is trial 11 with value: 0.5730713802775089.\n",
      " 20%|██        | 10/50 [17:00<1:32:49, 139.25s/it][I 2025-05-15 14:07:27,127] Trial 6 finished with value: 0.3900258750662157 and parameters: {'n_estimators': 175, 'learning_rate': 0.0019452657821097607, 'booster': 'dart', 'max_depth': 3, 'min_child_weight': 5, 'colsample_bytree': 0.8198507865047258, 'subsample': 0.5267597023219492, 'reg_alpha': 0.02381680490093453, 'reg_lambda': 0.00793243436635255, 'gamma': 4.999500405599313e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.8065615394457635e-05, 'skip_drop': 9.401229683759649e-07}. Best is trial 11 with value: 0.5730713802775089.\n",
      " 22%|██▏       | 11/50 [21:49<2:00:22, 185.19s/it][I 2025-05-15 14:08:08,503] Trial 8 finished with value: 0.4963896280911274 and parameters: {'n_estimators': 149, 'learning_rate': 0.02305207299055645, 'booster': 'dart', 'max_depth': 5, 'min_child_weight': 2, 'colsample_bytree': 0.7489427131818045, 'subsample': 0.9444301801313084, 'reg_alpha': 5.447589906089989e-08, 'reg_lambda': 5.21744386260753, 'gamma': 0.08301549548105087, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 7.526363056497285e-07, 'skip_drop': 3.826458585367108e-08}. Best is trial 11 with value: 0.5730713802775089.\n",
      " 24%|██▍       | 12/50 [22:30<1:29:34, 141.44s/it][I 2025-05-15 14:09:19,707] Trial 13 finished with value: 0.558381153729991 and parameters: {'n_estimators': 83, 'learning_rate': 0.15461718531470203, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.9500597451070996, 'subsample': 0.6985573945159058, 'reg_alpha': 1.050469773054428e-08, 'reg_lambda': 1.098516085343235e-08, 'gamma': 0.19017437334454473, 'grow_policy': 'lossguide'}. Best is trial 11 with value: 0.5730713802775089.\n",
      " 26%|██▌       | 13/50 [23:42<1:14:06, 120.16s/it][I 2025-05-15 14:09:35,758] Trial 14 finished with value: 0.5738094159081198 and parameters: {'n_estimators': 87, 'learning_rate': 0.19441238151963433, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.9491774216821505, 'subsample': 0.6913134208873974, 'reg_alpha': 1.3695356695603866, 'reg_lambda': 1.0128082668856633e-08, 'gamma': 0.004885115563208422, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 28%|██▊       | 14/50 [23:58<53:14, 88.73s/it]   [I 2025-05-15 14:11:36,268] Trial 16 finished with value: 0.506929431348036 and parameters: {'n_estimators': 199, 'learning_rate': 0.005544271433352836, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.9206250549210943, 'subsample': 0.6250337841113374, 'reg_alpha': 6.331001993308264, 'reg_lambda': 9.950278062171325e-07, 'gamma': 0.006061730836266256, 'grow_policy': 'depthwise'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 30%|███       | 15/50 [25:58<57:20, 98.30s/it][I 2025-05-15 14:11:44,574] Trial 15 finished with value: 0.5565321914206296 and parameters: {'n_estimators': 85, 'learning_rate': 0.12244178717048165, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.9706148727786343, 'subsample': 0.7023496745013817, 'reg_alpha': 1.9929428313996457e-08, 'reg_lambda': 1.541445141629906e-06, 'gamma': 0.010913044977076921, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 32%|███▏      | 16/50 [26:06<40:21, 71.21s/it][I 2025-05-15 14:13:10,273] Trial 17 finished with value: 0.5671849759963966 and parameters: {'n_estimators': 88, 'learning_rate': 0.1884397156217344, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'colsample_bytree': 0.8696491875949094, 'subsample': 0.755556590312289, 'reg_alpha': 7.617420260915997, 'reg_lambda': 1.845239148448846e-06, 'gamma': 0.00826203791481572, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 34%|███▍      | 17/50 [27:32<41:33, 75.57s/it][I 2025-05-15 14:13:27,883] Trial 18 finished with value: 0.5240499457111836 and parameters: {'n_estimators': 174, 'learning_rate': 0.008205627095258104, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 8, 'colsample_bytree': 0.8779288905815464, 'subsample': 0.7685025368757652, 'reg_alpha': 3.6049092402597878, 'reg_lambda': 7.600174844584015e-07, 'gamma': 0.789354824559264, 'grow_policy': 'depthwise'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 36%|███▌      | 18/50 [27:50<31:00, 58.15s/it][I 2025-05-15 14:13:48,484] Trial 19 finished with value: 0.5152765292581654 and parameters: {'n_estimators': 53, 'learning_rate': 0.008377294661745947, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.9103210134340055, 'subsample': 0.6053071137544437, 'reg_alpha': 5.665725663145242e-06, 'reg_lambda': 0.5196962284467174, 'gamma': 2.598105656264518e-05, 'grow_policy': 'depthwise'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 38%|███▊      | 19/50 [28:10<24:13, 46.87s/it][I 2025-05-15 14:13:59,803] Trial 20 finished with value: 0.539045007831066 and parameters: {'n_estimators': 52, 'learning_rate': 0.07541640007654599, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 9, 'colsample_bytree': 0.9145966547598359, 'subsample': 0.6193898447637091, 'reg_alpha': 0.0005355573310426735, 'reg_lambda': 0.6737640976468747, 'gamma': 0.0009040562288187881, 'grow_policy': 'depthwise'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 40%|████      | 20/50 [28:22<18:05, 36.20s/it][I 2025-05-15 14:15:02,241] Trial 21 finished with value: 0.5292887261224173 and parameters: {'n_estimators': 133, 'learning_rate': 0.07011525327754935, 'booster': 'gbtree', 'max_depth': 4, 'min_child_weight': 9, 'colsample_bytree': 0.9272746735247657, 'subsample': 0.6494087103680446, 'reg_alpha': 0.0002624786901372605, 'reg_lambda': 1.0076637701714554e-05, 'gamma': 0.000665346739430614, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 42%|████▏     | 21/50 [29:24<21:18, 44.07s/it][I 2025-05-15 14:18:02,539] Trial 23 finished with value: 0.5619149696320114 and parameters: {'n_estimators': 91, 'learning_rate': 0.15708407874739386, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.8223137819261841, 'subsample': 0.7405237975605256, 'reg_alpha': 0.8550329257864511, 'reg_lambda': 2.1461040952235115e-07, 'gamma': 0.020242577781488604, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5738094159081198.\n",
      " 44%|████▍     | 22/50 [32:24<39:38, 84.96s/it][I 2025-05-15 14:21:06,955] Trial 24 finished with value: 0.5755486683379261 and parameters: {'n_estimators': 100, 'learning_rate': 0.17662056229949788, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'colsample_bytree': 0.8674138113778415, 'subsample': 0.7490880792231122, 'reg_alpha': 2.153279790329752e-07, 'reg_lambda': 8.11699084911849e-06, 'gamma': 0.03252682282616966, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 46%|████▌     | 23/50 [35:29<51:39, 114.80s/it][I 2025-05-15 14:23:39,559] Trial 22 finished with value: 0.4921293784288901 and parameters: {'n_estimators': 134, 'learning_rate': 0.041294267175362755, 'booster': 'dart', 'max_depth': 4, 'min_child_weight': 7, 'colsample_bytree': 0.7993943371703893, 'subsample': 0.6549853548739157, 'reg_alpha': 1.395863326083225e-07, 'reg_lambda': 1.3217073809044943e-05, 'gamma': 0.030324351532225926, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.963091364955867, 'skip_drop': 0.8044326243753332}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 48%|████▊     | 24/50 [38:01<54:39, 126.15s/it][I 2025-05-15 14:25:14,656] Trial 12 finished with value: 0.3982825465840025 and parameters: {'n_estimators': 197, 'learning_rate': 0.006391414000575812, 'booster': 'dart', 'max_depth': 3, 'min_child_weight': 9, 'colsample_bytree': 0.9971870413876969, 'subsample': 0.6867991940766397, 'reg_alpha': 6.042045032843168e-08, 'reg_lambda': 1.2609247630830942, 'gamma': 0.6282166921580009, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 5.3937072752824684e-08, 'skip_drop': 1.6715199758427456e-08}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 50%|█████     | 25/50 [39:36<48:40, 116.83s/it][I 2025-05-15 14:25:20,908] Trial 25 finished with value: 0.5465743527576943 and parameters: {'n_estimators': 106, 'learning_rate': 0.03909645455607458, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 7, 'colsample_bytree': 0.9968820444567237, 'subsample': 0.7897544558579883, 'reg_alpha': 2.6420273877564165e-07, 'reg_lambda': 1.3472014450084867e-05, 'gamma': 0.6114188790113023, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 52%|█████▏    | 26/50 [39:43<33:27, 83.65s/it] [I 2025-05-15 14:25:22,628] Trial 26 finished with value: 0.567583496234177 and parameters: {'n_estimators': 105, 'learning_rate': 0.10618093767074828, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.9995156005877726, 'subsample': 0.7898929733887846, 'reg_alpha': 7.432111693181457e-07, 'reg_lambda': 8.292634242976252e-06, 'gamma': 0.5804909701450077, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 54%|█████▍    | 27/50 [39:44<22:38, 59.07s/it][I 2025-05-15 14:25:52,529] Trial 29 finished with value: 0.5560719624932624 and parameters: {'n_estimators': 62, 'learning_rate': 0.18900860499459773, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.8783407256804187, 'subsample': 0.7157639009304172, 'reg_alpha': 1.077304281688394e-05, 'reg_lambda': 1.6475961226463162e-07, 'gamma': 0.04439536164002213, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 56%|█████▌    | 28/50 [40:14<18:27, 50.32s/it][I 2025-05-15 14:26:03,161] Trial 28 finished with value: 0.5520277993453078 and parameters: {'n_estimators': 67, 'learning_rate': 0.09613808674483527, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.87289560083868, 'subsample': 0.7296184564551323, 'reg_alpha': 5.53391505610608e-07, 'reg_lambda': 2.481949827453932e-07, 'gamma': 0.03733110635039897, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 58%|█████▊    | 29/50 [40:25<13:26, 38.41s/it][I 2025-05-15 14:26:18,369] Trial 30 finished with value: 0.5649345337607428 and parameters: {'n_estimators': 75, 'learning_rate': 0.11457625851455094, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 9, 'colsample_bytree': 0.9423461069927491, 'subsample': 0.7319458968683726, 'reg_alpha': 2.545024227327346e-05, 'reg_lambda': 0.09034610724286485, 'gamma': 0.0047691313641516535, 'grow_policy': 'depthwise'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 60%|██████    | 30/50 [40:40<10:29, 31.45s/it][I 2025-05-15 14:26:39,051] Trial 27 finished with value: 0.5619709167658928 and parameters: {'n_estimators': 104, 'learning_rate': 0.11148591071559592, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.8814555807160619, 'subsample': 0.7350252437632264, 'reg_alpha': 5.781380168244294e-07, 'reg_lambda': 1.3747800131658416e-07, 'gamma': 0.05979676573685108, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 62%|██████▏   | 31/50 [41:01<08:56, 28.22s/it][I 2025-05-15 14:26:42,628] Trial 32 finished with value: 0.006657789613848202 and parameters: {'n_estimators': 166, 'learning_rate': 0.002950189953931165, 'booster': 'gblinear', 'max_depth': 10, 'min_child_weight': 5, 'colsample_bytree': 0.8359989232657845, 'subsample': 0.5896278569169293, 'reg_alpha': 0.0016997567343572152, 'reg_lambda': 0.0003178917894264374, 'gamma': 0.0001113464133324378}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 64%|██████▍   | 32/50 [41:04<06:15, 20.84s/it][I 2025-05-15 14:26:49,344] Trial 31 finished with value: 0.5067508146021953 and parameters: {'n_estimators': 96, 'learning_rate': 0.003770289613123553, 'booster': 'gbtree', 'max_depth': 8, 'min_child_weight': 4, 'colsample_bytree': 0.8418260848076073, 'subsample': 0.8307942044162684, 'reg_alpha': 5.8539163397537244e-05, 'reg_lambda': 0.0007752272770559284, 'gamma': 0.0001438729953260133, 'grow_policy': 'depthwise'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 66%|██████▌   | 33/50 [41:11<04:42, 16.59s/it][I 2025-05-15 14:27:22,413] Trial 34 finished with value: 0.5722958567754481 and parameters: {'n_estimators': 97, 'learning_rate': 0.1301803296207534, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'colsample_bytree': 0.9616470507755385, 'subsample': 0.818290175782978, 'reg_alpha': 2.3519370858789513e-06, 'reg_lambda': 4.6880125887817015e-06, 'gamma': 0.3007059293852833, 'grow_policy': 'lossguide'}. Best is trial 24 with value: 0.5755486683379261.\n",
      " 68%|██████▊   | 34/50 [41:44<05:44, 21.54s/it][I 2025-05-15 14:27:29,157] Trial 33 finished with value: 0.5768590281548764 and parameters: {'n_estimators': 96, 'learning_rate': 0.13671810205729562, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 9, 'colsample_bytree': 0.9544259373139633, 'subsample': 0.7821265873367009, 'reg_alpha': 1.8675227772962615e-06, 'reg_lambda': 0.0003869335467411198, 'gamma': 0.23432819427336474, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 70%|███████   | 35/50 [41:51<04:16, 17.10s/it][I 2025-05-15 14:28:14,937] Trial 36 finished with value: 0.5490716456433954 and parameters: {'n_estimators': 97, 'learning_rate': 0.06431072417012172, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 8, 'colsample_bytree': 0.9548558982795168, 'subsample': 0.8219996146870947, 'reg_alpha': 2.9803196040375583e-06, 'reg_lambda': 5.337503636092418e-05, 'gamma': 0.2596499824148328, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 72%|███████▏  | 36/50 [42:37<06:00, 25.72s/it][I 2025-05-15 14:28:52,350] Trial 38 finished with value: 0.011126038481731985 and parameters: {'n_estimators': 185, 'learning_rate': 0.1909450303318281, 'booster': 'gblinear', 'max_depth': 10, 'min_child_weight': 8, 'colsample_bytree': 0.7779670987312453, 'subsample': 0.7692911260637202, 'reg_alpha': 1.2091067970097051e-05, 'reg_lambda': 0.004268564453616184, 'gamma': 0.12324875765611314}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 74%|███████▍  | 37/50 [43:14<06:19, 29.21s/it][I 2025-05-15 14:29:03,282] Trial 35 finished with value: 0.5162378432056878 and parameters: {'n_estimators': 118, 'learning_rate': 0.0010075762332035182, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 9, 'colsample_bytree': 0.9590021067060281, 'subsample': 0.7951223244592824, 'reg_alpha': 2.5007116498991812e-06, 'reg_lambda': 5.038039156845159e-06, 'gamma': 0.20921555073498022, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 76%|███████▌  | 38/50 [43:25<04:44, 23.73s/it][I 2025-05-15 14:29:11,870] Trial 37 finished with value: 0.5638185937893375 and parameters: {'n_estimators': 117, 'learning_rate': 0.060470412404746564, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 8, 'colsample_bytree': 0.7978399847756554, 'subsample': 0.7807534558964655, 'reg_alpha': 4.51520717095764e-06, 'reg_lambda': 4.330935646759411e-05, 'gamma': 0.002415525020929535, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 78%|███████▊  | 39/50 [43:34<03:31, 19.19s/it][I 2025-05-15 14:34:30,162] Trial 40 finished with value: 0.538168939724115 and parameters: {'n_estimators': 79, 'learning_rate': 0.05561131670077743, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 7, 'colsample_bytree': 0.7119372683228224, 'subsample': 0.661140604212981, 'reg_alpha': 0.011660574129317995, 'reg_lambda': 0.002119933506754914, 'gamma': 0.0018077365359172122, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.000802200379557017, 'skip_drop': 0.4914853166451848}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 80%|████████  | 40/50 [48:52<18:09, 108.92s/it][I 2025-05-15 14:35:11,661] Trial 42 finished with value: 0.29891582551442347 and parameters: {'n_estimators': 143, 'learning_rate': 0.013069357541153908, 'booster': 'gblinear', 'max_depth': 8, 'min_child_weight': 9, 'colsample_bytree': 0.9072108841200025, 'subsample': 0.8650655322078036, 'reg_alpha': 1.231759765148712e-07, 'reg_lambda': 0.00022509845245155858, 'gamma': 3.2470907838596404e-05}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 82%|████████▏ | 41/50 [49:33<13:18, 88.69s/it] [I 2025-05-15 14:35:15,192] Trial 39 finished with value: 0.455364112190707 and parameters: {'n_estimators': 115, 'learning_rate': 0.0011520784276416013, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 9, 'colsample_bytree': 0.5620987466795875, 'subsample': 0.7937006523017149, 'reg_alpha': 1.8419968181377065e-07, 'reg_lambda': 0.0029013586288304948, 'gamma': 0.0033702847566762202, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 84%|████████▍ | 42/50 [49:37<08:25, 63.14s/it][I 2025-05-15 14:36:50,040] Trial 43 finished with value: 0.5682873262840129 and parameters: {'n_estimators': 96, 'learning_rate': 0.1405145866018453, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 9, 'colsample_bytree': 0.9318701814733551, 'subsample': 0.8471022876015508, 'reg_alpha': 8.06233031866023e-05, 'reg_lambda': 1.1725686788934652e-08, 'gamma': 0.25512556584330204, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 86%|████████▌ | 43/50 [51:12<08:28, 72.66s/it][I 2025-05-15 14:37:02,601] Trial 44 finished with value: 0.574200445052782 and parameters: {'n_estimators': 98, 'learning_rate': 0.13890809091256798, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.9006496127130835, 'subsample': 0.8537720146670198, 'reg_alpha': 0.00012607508282131877, 'reg_lambda': 1.1382066304595715e-08, 'gamma': 0.017973566030419105, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 88%|████████▊ | 44/50 [51:24<05:27, 54.63s/it][I 2025-05-15 14:38:25,899] Trial 45 finished with value: 0.5636792452830188 and parameters: {'n_estimators': 126, 'learning_rate': 0.08967474947838223, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 10, 'colsample_bytree': 0.9727880949460024, 'subsample': 0.8760627716201287, 'reg_alpha': 1.0285527488810742e-06, 'reg_lambda': 7.685862102201805e-05, 'gamma': 0.07476543017132291, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 90%|█████████ | 45/50 [52:48<05:16, 63.23s/it][I 2025-05-15 14:38:47,428] Trial 46 finished with value: 0.5542442358723374 and parameters: {'n_estimators': 127, 'learning_rate': 0.08073518000790032, 'booster': 'gbtree', 'max_depth': 7, 'min_child_weight': 10, 'colsample_bytree': 0.8592040421559527, 'subsample': 0.9096183198100257, 'reg_alpha': 0.00033386681882739206, 'reg_lambda': 4.418877985175473e-08, 'gamma': 0.013580734384854966, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 92%|█████████▏| 46/50 [53:09<03:22, 50.72s/it][I 2025-05-15 14:39:46,059] Trial 47 finished with value: 0.5573518129693935 and parameters: {'n_estimators': 109, 'learning_rate': 0.08234091687282624, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.8907044820756612, 'subsample': 0.9218772356377058, 'reg_alpha': 0.00025645338338204813, 'reg_lambda': 4.724468748763091e-08, 'gamma': 0.017007230473450204, 'grow_policy': 'depthwise'}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 94%|█████████▍| 47/50 [54:08<02:39, 53.09s/it][I 2025-05-15 14:40:00,373] Trial 41 finished with value: 0.5349603872407207 and parameters: {'n_estimators': 141, 'learning_rate': 0.03133269224294978, 'booster': 'dart', 'max_depth': 7, 'min_child_weight': 7, 'colsample_bytree': 0.5234998327169249, 'subsample': 0.8567266117605052, 'reg_alpha': 0.007218339077994699, 'reg_lambda': 0.0014899205562057306, 'gamma': 0.0023015787818436596, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.0009234366242854077, 'skip_drop': 0.5423559301061506}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 96%|█████████▌| 48/50 [54:22<01:22, 41.46s/it][I 2025-05-15 14:41:55,719] Trial 48 finished with value: 0.558433697607472 and parameters: {'n_estimators': 68, 'learning_rate': 0.15147934737214644, 'booster': 'dart', 'max_depth': 9, 'min_child_weight': 10, 'colsample_bytree': 0.8939130800079391, 'subsample': 0.6831424193758917, 'reg_alpha': 0.001992585609558875, 'reg_lambda': 0.031085263734106108, 'gamma': 0.00032122317182116454, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.0668102561049117e-08, 'skip_drop': 0.0006626003624637752}. Best is trial 33 with value: 0.5768590281548764.\n",
      " 98%|█████████▊| 49/50 [56:18<01:03, 63.62s/it][I 2025-05-15 14:42:29,716] Trial 49 finished with value: 0.5691279316365214 and parameters: {'n_estimators': 68, 'learning_rate': 0.15909532773958465, 'booster': 'dart', 'max_depth': 10, 'min_child_weight': 3, 'colsample_bytree': 0.901059167960681, 'subsample': 0.5073808232680426, 'reg_alpha': 0.09848856822843015, 'reg_lambda': 1.9635566740890767e-08, 'gamma': 0.00028620788980138033, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 4.757753779775496e-05, 'skip_drop': 0.0010785295394273818}. Best is trial 33 with value: 0.5768590281548764.\n",
      "100%|██████████| 50/50 [56:52<00:00, 54.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xgboost]  best F1=0.5769   Best hyperparameters: {'n_estimators': 96, 'learning_rate': 0.13671810205729562, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 9, 'colsample_bytree': 0.9544259373139633, 'subsample': 0.7821265873367009, 'reg_alpha': 1.8675227772962615e-06, 'reg_lambda': 0.0003869335467411198, 'gamma': 0.23432819427336474, 'grow_policy': 'lossguide'} bootstrap 95% CI=(0.5520, 0.6039)\n"
     ]
    }
   ],
   "source": [
    "xgb_results['tfidf_without_augmentation'] = optimize_model(\n",
    "    model_name = 'xgboost',\n",
    "    X_train    = X_tfidf_reg_tr,\n",
    "    y_train    = y_tfidf_reg_tr,\n",
    "    X_val      = X_tfidf_reg_val,\n",
    "    y_val      = y_tfidf_reg_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ze5Iu-hQyvkX"
   },
   "outputs": [],
   "source": [
    "xgb_results['tfidf_with_augmentation'] = optimize_model(\n",
    "    model_name = 'xgboost',\n",
    "    X_train    = X_tfidf_aug_tr,\n",
    "    y_train    = y_tfidf_aug_tr,\n",
    "    X_val      = X_tfidf_aug_val,\n",
    "    y_val      = y_tfidf_aug_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost results:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Best Eval Score</th>\n",
       "      <th>CI (95%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_without_augmentation</td>\n",
       "      <td>{'n_estimators': 97, 'learning_rate': 0.198935...</td>\n",
       "      <td>0.63998</td>\n",
       "      <td>(0.6136302135287747, 0.6633417826405086)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Experiment  \\\n",
       "0  bert_without_augmentation   \n",
       "\n",
       "                                     Best Parameters  Best Eval Score  \\\n",
       "0  {'n_estimators': 97, 'learning_rate': 0.198935...          0.63998   \n",
       "\n",
       "                                   CI (95%)  \n",
       "0  (0.6136302135287747, 0.6633417826405086)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"XGBoost results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Eval Score': value[1],\n",
    "        'CI (95%)': value[2]\n",
    "    }\n",
    "    for key, value in xgb_results.items()\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the potential a network classification head shown, we'll attempt to optimise it to train-val loss, with adjustments to learning process.\n",
    "\n",
    "This is a 1-block flow to train / optimize the process for a DNN on the embedding dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Status]: Building datasets and dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Comments: 100%|██████████| 43214/43214 [00:01<00:00, 28953.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextDataset] rows: train=29690, val=4213, test=8656\n",
      "[EmbeddingDataset]: Loading precomputed embeddings from Data\\cache\\distilbert_embeddings_regular.pkl...\n",
      "[EmbeddingDataset Status]: Embedding generation complete.\n",
      "[Dataloader Status]: Preparing the dataloader...\n",
      "[DL] EmbeddingDataset ready. X shape (29690, 768), y len 29690\n",
      "[Dataloader Status]: Preparing the dataloader...\n",
      "[DL] EmbeddingDataset ready. X shape (4213, 768), y len 4213\n",
      "[Dataloader Status]: Preparing the dataloader...\n",
      "[DL] EmbeddingDataset ready. X shape (8656, 768), y len 8656\n",
      "[Model Fit Status]: Training direct DNN...\n",
      "Epoch 1/30: Train Loss: 0.4243 | Val Loss: 0.6811 | F1: 0.7041 | ↪️ Saved new best model.\n",
      "Epoch 2/30: Train Loss: 0.3061 | Val Loss: 0.7275 | F1: 0.7196 | ↪️ Saved new best model.\n",
      "Epoch 3/30: Train Loss: 0.2775 | Val Loss: 0.7467 | F1: 0.7122\n",
      "Epoch 4/30: Train Loss: 0.2633 | Val Loss: 0.7217 | F1: 0.6999\n",
      "Epoch 5/30: Train Loss: 0.2524 | Val Loss: 0.8336 | F1: 0.7253 | ↪️ Saved new best model.\n",
      "Epoch 6/30: Train Loss: 0.2412 | Val Loss: 0.8684 | F1: 0.7237\n",
      "Epoch 7/30: Train Loss: 0.2318 | Val Loss: 0.9148 | F1: 0.7231\n",
      "Epoch 8/30: Train Loss: 0.2205 | Val Loss: 0.8629 | F1: 0.7236\n",
      "Epoch 9/30: Train Loss: 0.1989 | Val Loss: 0.8426 | F1: 0.7251\n",
      "Epoch 10/30: Train Loss: 0.1851 | Val Loss: 0.8669 | F1: 0.7264 | ↪️ Saved new best model.\n",
      "Epoch 11/30: Train Loss: 0.1822 | Val Loss: 0.9829 | F1: 0.7276 | ↪️ Saved new best model.\n",
      "Epoch 12/30: Train Loss: 0.1751 | Val Loss: 0.8905 | F1: 0.7214\n",
      "Epoch 13/30: Train Loss: 0.1724 | Val Loss: 0.8957 | F1: 0.7120\n",
      "Epoch 14/30: Train Loss: 0.1638 | Val Loss: 1.0031 | F1: 0.7187\n",
      "Epoch 15/30: Train Loss: 0.1446 | Val Loss: 1.0003 | F1: 0.7284 | ↪️ Saved new best model.\n",
      "Epoch 16/30: Train Loss: 0.1346 | Val Loss: 1.0708 | F1: 0.7284\n",
      "Epoch 17/30: Train Loss: 0.1290 | Val Loss: 1.0793 | F1: 0.7272\n",
      "Epoch 18/30: Train Loss: 0.1272 | Val Loss: 1.1180 | F1: 0.7265\n",
      "Epoch 19/30: Train Loss: 0.1196 | Val Loss: 1.0590 | F1: 0.7228\n",
      "Epoch 20/30: Train Loss: 0.1187 | Val Loss: 1.1480 | F1: 0.7255\n",
      "Epoch 21/30: Train Loss: 0.1130 | Val Loss: 1.1052 | F1: 0.7163\n",
      "Epoch 22/30: Train Loss: 0.1062 | Val Loss: 1.1434 | F1: 0.7248\n",
      "Epoch 23/30: Train Loss: 0.1084 | Val Loss: 1.1505 | F1: 0.7244\n",
      "Epoch 24/30: Train Loss: 0.1035 | Val Loss: 1.1583 | F1: 0.7247\n",
      "Epoch 25/30: Train Loss: 0.1034 | Val Loss: 1.1671 | F1: 0.7217\n",
      "Epoch 26/30: Train Loss: 0.1009 | Val Loss: 1.1808 | F1: 0.7260\n",
      "Epoch 27/30: Train Loss: 0.1015 | Val Loss: 1.1593 | F1: 0.7241\n",
      "Epoch 28/30: Train Loss: 0.0997 | Val Loss: 1.1681 | F1: 0.7230\n",
      "Epoch 29/30: Train Loss: 0.0984 | Val Loss: 1.1650 | F1: 0.7238\n",
      "Epoch 30/30: Train Loss: 0.0986 | Val Loss: 1.2014 | F1: 0.7240\n",
      "\n",
      "[Evaluation on Test Set]\n",
      "[Debug] Glimpse of true labels: [      5514453319680 4602680627353878528          1059926094\n",
      "          1058930698                   0 4428349979278442496\n",
      "                   0          1069989592                   0\n",
      "                   0] Length: 8656\n",
      "\n",
      "Confusion Matrix (Truth Table):\n",
      "        Pred 0  Pred 1  Pred 2\n",
      "True 0     329     290    1252\n",
      "True 1       0       0       0\n",
      "True 2       0       0       0\n",
      "\n",
      "F1-Score (Micro): 0.0380\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  0       0.21      0.18      0.19      1871\n",
      "                  1       0.00      0.00      0.00         0\n",
      "                  2       0.00      0.00      0.00         0\n",
      "          972585984       0.00      0.00      0.00         1\n",
      "          974678016       0.00      0.00      0.00         1\n",
      "          978663424       0.00      0.00      0.00         1\n",
      "          981579776       0.00      0.00      0.00         1\n",
      "          982038528       0.00      0.00      0.00         1\n",
      "          983467008       0.00      0.00      0.00         1\n",
      "          983539200       0.00      0.00      0.00         1\n",
      "          983674368       0.00      0.00      0.00         1\n",
      "          984459264       0.00      0.00      0.00         1\n",
      "          988415744       0.00      0.00      0.00         1\n",
      "          988632576       0.00      0.00      0.00         1\n",
      "          993862016       0.00      0.00      0.00         1\n",
      "          994011360       0.00      0.00      0.00         1\n",
      "          995635200       0.00      0.00      0.00         1\n",
      "          997806080       0.00      0.00      0.00         1\n",
      "          998846976       0.00      0.00      0.00         1\n",
      "          999211264       0.00      0.00      0.00         1\n",
      "         1001225216       0.00      0.00      0.00         1\n",
      "         1006215296       0.00      0.00      0.00         1\n",
      "         1006503552       0.00      0.00      0.00         1\n",
      "         1008222656       0.00      0.00      0.00         1\n",
      "         1008892636       0.00      0.00      0.00         1\n",
      "         1010124672       0.00      0.00      0.00         1\n",
      "         1010429632       0.00      0.00      0.00         1\n",
      "         1010557040       0.00      0.00      0.00         1\n",
      "         1010573376       0.00      0.00      0.00         1\n",
      "         1010749248       0.00      0.00      0.00         1\n",
      "         1011486528       0.00      0.00      0.00         1\n",
      "         1013048608       0.00      0.00      0.00         1\n",
      "         1013121344       0.00      0.00      0.00         1\n",
      "         1013414912       0.00      0.00      0.00         1\n",
      "         1013475648       0.00      0.00      0.00         1\n",
      "         1014376768       0.00      0.00      0.00         1\n",
      "         1014848864       0.00      0.00      0.00         1\n",
      "         1014857760       0.00      0.00      0.00         1\n",
      "         1015025280       0.00      0.00      0.00         1\n",
      "         1015343616       0.00      0.00      0.00         1\n",
      "         1015428928       0.00      0.00      0.00         1\n",
      "         1015518784       0.00      0.00      0.00         1\n",
      "         1015824960       0.00      0.00      0.00         1\n",
      "         1016621964       0.00      0.00      0.00         1\n",
      "         1017559984       0.00      0.00      0.00         1\n",
      "         1017911200       0.00      0.00      0.00         1\n",
      "         1018543420       0.00      0.00      0.00         1\n",
      "         1018768448       0.00      0.00      0.00         1\n",
      "         1018927264       0.00      0.00      0.00         1\n",
      "         1019232640       0.00      0.00      0.00         1\n",
      "         1019356704       0.00      0.00      0.00         1\n",
      "         1019574176       0.00      0.00      0.00         1\n",
      "         1020120896       0.00      0.00      0.00         1\n",
      "         1020572064       0.00      0.00      0.00         1\n",
      "         1020780576       0.00      0.00      0.00         1\n",
      "         1020854696       0.00      0.00      0.00         1\n",
      "         1020941952       0.00      0.00      0.00         1\n",
      "         1021024352       0.00      0.00      0.00         1\n",
      "         1021064800       0.00      0.00      0.00         1\n",
      "         1021288768       0.00      0.00      0.00         1\n",
      "         1021711488       0.00      0.00      0.00         1\n",
      "         1022116064       0.00      0.00      0.00         1\n",
      "         1022220496       0.00      0.00      0.00         1\n",
      "         1022591480       0.00      0.00      0.00         1\n",
      "         1022928576       0.00      0.00      0.00         1\n",
      "         1023046400       0.00      0.00      0.00         1\n",
      "         1023126720       0.00      0.00      0.00         1\n",
      "         1023235360       0.00      0.00      0.00         1\n",
      "         1023363680       0.00      0.00      0.00         1\n",
      "         1023389024       0.00      0.00      0.00         1\n",
      "         1023546752       0.00      0.00      0.00         1\n",
      "         1023556224       0.00      0.00      0.00         1\n",
      "         1023642024       0.00      0.00      0.00         1\n",
      "         1024058564       0.00      0.00      0.00         1\n",
      "         1024230144       0.00      0.00      0.00         1\n",
      "         1024800616       0.00      0.00      0.00         1\n",
      "         1024967236       0.00      0.00      0.00         1\n",
      "         1025274010       0.00      0.00      0.00         1\n",
      "         1025507696       0.00      0.00      0.00         1\n",
      "         1025521588       0.00      0.00      0.00         1\n",
      "         1025696720       0.00      0.00      0.00         1\n",
      "         1025940936       0.00      0.00      0.00         1\n",
      "         1026337504       0.00      0.00      0.00         1\n",
      "         1026450944       0.00      0.00      0.00         1\n",
      "         1026505392       0.00      0.00      0.00         1\n",
      "         1026814752       0.00      0.00      0.00         1\n",
      "         1026847296       0.00      0.00      0.00         1\n",
      "         1027104592       0.00      0.00      0.00         1\n",
      "         1027243776       0.00      0.00      0.00         1\n",
      "         1027261920       0.00      0.00      0.00         1\n",
      "         1027292960       0.00      0.00      0.00         1\n",
      "         1027315952       0.00      0.00      0.00         1\n",
      "         1027349104       0.00      0.00      0.00         1\n",
      "         1027525232       0.00      0.00      0.00         1\n",
      "         1027712944       0.00      0.00      0.00         1\n",
      "         1027716192       0.00      0.00      0.00         1\n",
      "         1027718368       0.00      0.00      0.00         1\n",
      "         1027829696       0.00      0.00      0.00         1\n",
      "         1028055606       0.00      0.00      0.00         1\n",
      "         1028152128       0.00      0.00      0.00         1\n",
      "         1028297124       0.00      0.00      0.00         1\n",
      "         1028340160       0.00      0.00      0.00         1\n",
      "         1028410656       0.00      0.00      0.00         1\n",
      "         1028535344       0.00      0.00      0.00         1\n",
      "         1028610304       0.00      0.00      0.00         1\n",
      "         1028809488       0.00      0.00      0.00         1\n",
      "         1028998032       0.00      0.00      0.00         1\n",
      "         1029031552       0.00      0.00      0.00         1\n",
      "         1029059712       0.00      0.00      0.00         1\n",
      "         1029233536       0.00      0.00      0.00         1\n",
      "         1029248952       0.00      0.00      0.00         1\n",
      "         1029613152       0.00      0.00      0.00         1\n",
      "         1029679104       0.00      0.00      0.00         1\n",
      "         1029731232       0.00      0.00      0.00         1\n",
      "         1029784016       0.00      0.00      0.00         1\n",
      "         1030031422       0.00      0.00      0.00         1\n",
      "         1030095392       0.00      0.00      0.00         1\n",
      "         1030195216       0.00      0.00      0.00         1\n",
      "         1030408416       0.00      0.00      0.00         1\n",
      "         1030444224       0.00      0.00      0.00         1\n",
      "         1030477440       0.00      0.00      0.00         1\n",
      "         1030486144       0.00      0.00      0.00         1\n",
      "         1030545240       0.00      0.00      0.00         1\n",
      "         1030744736       0.00      0.00      0.00         1\n",
      "         1030772064       0.00      0.00      0.00         1\n",
      "         1031009952       0.00      0.00      0.00         1\n",
      "         1031056896       0.00      0.00      0.00         1\n",
      "         1031137504       0.00      0.00      0.00         1\n",
      "         1031875224       0.00      0.00      0.00         1\n",
      "         1031911876       0.00      0.00      0.00         1\n",
      "         1031920968       0.00      0.00      0.00         1\n",
      "         1031994112       0.00      0.00      0.00         1\n",
      "         1032002744       0.00      0.00      0.00         1\n",
      "         1032039768       0.00      0.00      0.00         1\n",
      "         1032055552       0.00      0.00      0.00         1\n",
      "         1032258832       0.00      0.00      0.00         1\n",
      "         1032288088       0.00      0.00      0.00         1\n",
      "         1032326296       0.00      0.00      0.00         1\n",
      "         1032365434       0.00      0.00      0.00         1\n",
      "         1032394608       0.00      0.00      0.00         1\n",
      "         1032448390       0.00      0.00      0.00         1\n",
      "         1032517352       0.00      0.00      0.00         1\n",
      "         1032561016       0.00      0.00      0.00         1\n",
      "         1032605228       0.00      0.00      0.00         1\n",
      "         1032666192       0.00      0.00      0.00         1\n",
      "         1032856728       0.00      0.00      0.00         1\n",
      "         1032931504       0.00      0.00      0.00         1\n",
      "         1032947274       0.00      0.00      0.00         1\n",
      "         1032954384       0.00      0.00      0.00         1\n",
      "         1033003990       0.00      0.00      0.00         1\n",
      "         1033013464       0.00      0.00      0.00         1\n",
      "         1033016232       0.00      0.00      0.00         1\n",
      "         1033159416       0.00      0.00      0.00         1\n",
      "         1033179796       0.00      0.00      0.00         1\n",
      "         1033210498       0.00      0.00      0.00         1\n",
      "         1033219680       0.00      0.00      0.00         1\n",
      "         1033257196       0.00      0.00      0.00         1\n",
      "         1033281872       0.00      0.00      0.00         1\n",
      "         1033318544       0.00      0.00      0.00         1\n",
      "         1033339414       0.00      0.00      0.00         1\n",
      "         1033388184       0.00      0.00      0.00         1\n",
      "         1033436144       0.00      0.00      0.00         1\n",
      "         1033602848       0.00      0.00      0.00         1\n",
      "         1033624656       0.00      0.00      0.00         1\n",
      "         1033755680       0.00      0.00      0.00         1\n",
      "         1033839528       0.00      0.00      0.00         1\n",
      "         1033955906       0.00      0.00      0.00         1\n",
      "         1033957952       0.00      0.00      0.00         1\n",
      "         1033967624       0.00      0.00      0.00         1\n",
      "         1033978264       0.00      0.00      0.00         1\n",
      "         1034047330       0.00      0.00      0.00         1\n",
      "         1034060136       0.00      0.00      0.00         1\n",
      "         1034113264       0.00      0.00      0.00         1\n",
      "         1034211664       0.00      0.00      0.00         1\n",
      "         1034228040       0.00      0.00      0.00         1\n",
      "         1034283514       0.00      0.00      0.00         1\n",
      "         1034298816       0.00      0.00      0.00         1\n",
      "         1034344496       0.00      0.00      0.00         1\n",
      "         1034439968       0.00      0.00      0.00         1\n",
      "         1034451280       0.00      0.00      0.00         1\n",
      "         1034485520       0.00      0.00      0.00         1\n",
      "         1034564776       0.00      0.00      0.00         1\n",
      "         1034689624       0.00      0.00      0.00         1\n",
      "         1034758184       0.00      0.00      0.00         1\n",
      "         1034831904       0.00      0.00      0.00         1\n",
      "         1034954866       0.00      0.00      0.00         1\n",
      "         1034962264       0.00      0.00      0.00         1\n",
      "         1035083568       0.00      0.00      0.00         1\n",
      "         1035157720       0.00      0.00      0.00         1\n",
      "         1035199546       0.00      0.00      0.00         1\n",
      "         1035308660       0.00      0.00      0.00         1\n",
      "         1035323872       0.00      0.00      0.00         1\n",
      "         1035358912       0.00      0.00      0.00         1\n",
      "         1035399728       0.00      0.00      0.00         1\n",
      "         1035497888       0.00      0.00      0.00         1\n",
      "         1035546436       0.00      0.00      0.00         1\n",
      "         1035569512       0.00      0.00      0.00         1\n",
      "         1035633142       0.00      0.00      0.00         1\n",
      "         1035666748       0.00      0.00      0.00         1\n",
      "         1035678816       0.00      0.00      0.00         1\n",
      "         1035709620       0.00      0.00      0.00         1\n",
      "         1035715384       0.00      0.00      0.00         1\n",
      "         1035731496       0.00      0.00      0.00         1\n",
      "         1035824896       0.00      0.00      0.00         1\n",
      "         1035876880       0.00      0.00      0.00         1\n",
      "         1036024728       0.00      0.00      0.00         1\n",
      "         1036093020       0.00      0.00      0.00         1\n",
      "         1036502608       0.00      0.00      0.00         1\n",
      "         1036659664       0.00      0.00      0.00         1\n",
      "         1036820096       0.00      0.00      0.00         1\n",
      "         1036855392       0.00      0.00      0.00         1\n",
      "         1036923276       0.00      0.00      0.00         1\n",
      "         1036927592       0.00      0.00      0.00         1\n",
      "         1037060552       0.00      0.00      0.00         1\n",
      "         1037106864       0.00      0.00      0.00         1\n",
      "         1037112304       0.00      0.00      0.00         1\n",
      "         1037161792       0.00      0.00      0.00         1\n",
      "         1037223936       0.00      0.00      0.00         1\n",
      "         1037228512       0.00      0.00      0.00         1\n",
      "         1037295048       0.00      0.00      0.00         1\n",
      "         1037316992       0.00      0.00      0.00         1\n",
      "         1037337216       0.00      0.00      0.00         1\n",
      "         1037355508       0.00      0.00      0.00         1\n",
      "         1037563624       0.00      0.00      0.00         1\n",
      "         1037655520       0.00      0.00      0.00         1\n",
      "         1037786240       0.00      0.00      0.00         1\n",
      "         1037801768       0.00      0.00      0.00         1\n",
      "         1037851184       0.00      0.00      0.00         1\n",
      "         1037868168       0.00      0.00      0.00         1\n",
      "         1037964944       0.00      0.00      0.00         1\n",
      "         1037982124       0.00      0.00      0.00         1\n",
      "         1038020280       0.00      0.00      0.00         1\n",
      "         1038144256       0.00      0.00      0.00         1\n",
      "         1038311208       0.00      0.00      0.00         1\n",
      "         1038388464       0.00      0.00      0.00         1\n",
      "         1038392928       0.00      0.00      0.00         1\n",
      "         1038458576       0.00      0.00      0.00         1\n",
      "         1038527540       0.00      0.00      0.00         1\n",
      "         1038594984       0.00      0.00      0.00         1\n",
      "         1038731560       0.00      0.00      0.00         1\n",
      "         1038842256       0.00      0.00      0.00         1\n",
      "         1038881412       0.00      0.00      0.00         1\n",
      "         1038908016       0.00      0.00      0.00         1\n",
      "         1038992152       0.00      0.00      0.00         1\n",
      "         1039108984       0.00      0.00      0.00         1\n",
      "         1039127096       0.00      0.00      0.00         1\n",
      "         1039221728       0.00      0.00      0.00         1\n",
      "         1039250887       0.00      0.00      0.00         1\n",
      "         1039276656       0.00      0.00      0.00         1\n",
      "         1039287464       0.00      0.00      0.00         1\n",
      "         1039345296       0.00      0.00      0.00         1\n",
      "         1039385856       0.00      0.00      0.00         1\n",
      "         1039437963       0.00      0.00      0.00         1\n",
      "         1039455200       0.00      0.00      0.00         1\n",
      "         1039573080       0.00      0.00      0.00         1\n",
      "         1039744956       0.00      0.00      0.00         1\n",
      "         1039772392       0.00      0.00      0.00         1\n",
      "         1039860096       0.00      0.00      0.00         1\n",
      "         1039900364       0.00      0.00      0.00         1\n",
      "         1039935152       0.00      0.00      0.00         1\n",
      "         1039942514       0.00      0.00      0.00         1\n",
      "         1039984216       0.00      0.00      0.00         1\n",
      "         1040071296       0.00      0.00      0.00         1\n",
      "         1040177243       0.00      0.00      0.00         1\n",
      "         1040178012       0.00      0.00      0.00         1\n",
      "         1040181448       0.00      0.00      0.00         1\n",
      "         1040218384       0.00      0.00      0.00         1\n",
      "         1040231144       0.00      0.00      0.00         1\n",
      "         1040242483       0.00      0.00      0.00         1\n",
      "         1040257945       0.00      0.00      0.00         1\n",
      "         1040273200       0.00      0.00      0.00         1\n",
      "         1040321446       0.00      0.00      0.00         1\n",
      "         1040325714       0.00      0.00      0.00         1\n",
      "         1040342896       0.00      0.00      0.00         1\n",
      "         1040411388       0.00      0.00      0.00         1\n",
      "         1040421176       0.00      0.00      0.00         1\n",
      "         1040450454       0.00      0.00      0.00         1\n",
      "         1040471736       0.00      0.00      0.00         1\n",
      "         1040482380       0.00      0.00      0.00         1\n",
      "         1040565374       0.00      0.00      0.00         1\n",
      "         1040621500       0.00      0.00      0.00         1\n",
      "         1040647360       0.00      0.00      0.00         1\n",
      "         1040647530       0.00      0.00      0.00         1\n",
      "         1040671252       0.00      0.00      0.00         1\n",
      "         1040684242       0.00      0.00      0.00         1\n",
      "         1040690891       0.00      0.00      0.00         1\n",
      "         1040781700       0.00      0.00      0.00         1\n",
      "         1040810678       0.00      0.00      0.00         1\n",
      "         1040849952       0.00      0.00      0.00         1\n",
      "         1040878874       0.00      0.00      0.00         1\n",
      "         1040898824       0.00      0.00      0.00         1\n",
      "         1041038076       0.00      0.00      0.00         1\n",
      "         1041046056       0.00      0.00      0.00         1\n",
      "         1041048276       0.00      0.00      0.00         1\n",
      "         1041073296       0.00      0.00      0.00         1\n",
      "         1041131814       0.00      0.00      0.00         1\n",
      "         1041141716       0.00      0.00      0.00         1\n",
      "         1041155808       0.00      0.00      0.00         1\n",
      "         1041172680       0.00      0.00      0.00         1\n",
      "         1041223596       0.00      0.00      0.00         1\n",
      "         1041232566       0.00      0.00      0.00         1\n",
      "         1041253408       0.00      0.00      0.00         1\n",
      "         1041320820       0.00      0.00      0.00         1\n",
      "         1041349538       0.00      0.00      0.00         1\n",
      "         1041375408       0.00      0.00      0.00         1\n",
      "         1041376608       0.00      0.00      0.00         1\n",
      "         1041390004       0.00      0.00      0.00         1\n",
      "         1041396530       0.00      0.00      0.00         1\n",
      "         1041396870       0.00      0.00      0.00         1\n",
      "         1041427440       0.00      0.00      0.00         1\n",
      "         1041447056       0.00      0.00      0.00         1\n",
      "         1041447488       0.00      0.00      0.00         1\n",
      "         1041562870       0.00      0.00      0.00         1\n",
      "         1041567832       0.00      0.00      0.00         1\n",
      "         1041578552       0.00      0.00      0.00         1\n",
      "         1041586600       0.00      0.00      0.00         1\n",
      "         1041591248       0.00      0.00      0.00         1\n",
      "         1041596272       0.00      0.00      0.00         1\n",
      "         1041598322       0.00      0.00      0.00         1\n",
      "         1041640196       0.00      0.00      0.00         1\n",
      "         1041659952       0.00      0.00      0.00         1\n",
      "         1041669988       0.00      0.00      0.00         1\n",
      "         1041698107       0.00      0.00      0.00         1\n",
      "         1041709924       0.00      0.00      0.00         1\n",
      "         1041748168       0.00      0.00      0.00         1\n",
      "         1041750091       0.00      0.00      0.00         1\n",
      "         1041751593       0.00      0.00      0.00         1\n",
      "         1041762976       0.00      0.00      0.00         1\n",
      "         1041851112       0.00      0.00      0.00         1\n",
      "         1041852872       0.00      0.00      0.00         1\n",
      "         1041863584       0.00      0.00      0.00         1\n",
      "         1041894952       0.00      0.00      0.00         1\n",
      "         1041903624       0.00      0.00      0.00         1\n",
      "         1041933616       0.00      0.00      0.00         1\n",
      "         1041942522       0.00      0.00      0.00         1\n",
      "         1042001290       0.00      0.00      0.00         1\n",
      "         1042043108       0.00      0.00      0.00         1\n",
      "         1042098915       0.00      0.00      0.00         1\n",
      "         1042108940       0.00      0.00      0.00         1\n",
      "         1042126040       0.00      0.00      0.00         1\n",
      "         1042130202       0.00      0.00      0.00         1\n",
      "         1042182616       0.00      0.00      0.00         1\n",
      "         1042183704       0.00      0.00      0.00         1\n",
      "         1042189248       0.00      0.00      0.00         1\n",
      "         1042191352       0.00      0.00      0.00         1\n",
      "         1042191528       0.00      0.00      0.00         1\n",
      "         1042198012       0.00      0.00      0.00         1\n",
      "         1042205548       0.00      0.00      0.00         1\n",
      "         1042209332       0.00      0.00      0.00         1\n",
      "         1042218134       0.00      0.00      0.00         1\n",
      "         1042225492       0.00      0.00      0.00         1\n",
      "         1042247581       0.00      0.00      0.00         1\n",
      "         1042252568       0.00      0.00      0.00         1\n",
      "         1042268826       0.00      0.00      0.00         1\n",
      "         1042299428       0.00      0.00      0.00         1\n",
      "         1042357532       0.00      0.00      0.00         1\n",
      "         1042372252       0.00      0.00      0.00         1\n",
      "         1042392776       0.00      0.00      0.00         1\n",
      "         1042411779       0.00      0.00      0.00         1\n",
      "         1042412135       0.00      0.00      0.00         1\n",
      "         1042430516       0.00      0.00      0.00         1\n",
      "         1042447518       0.00      0.00      0.00         1\n",
      "         1042468512       0.00      0.00      0.00         1\n",
      "         1042490392       0.00      0.00      0.00         1\n",
      "         1042521024       0.00      0.00      0.00         1\n",
      "         1042527385       0.00      0.00      0.00         1\n",
      "         1042554352       0.00      0.00      0.00         1\n",
      "         1042563857       0.00      0.00      0.00         1\n",
      "         1042602608       0.00      0.00      0.00         1\n",
      "         1042626584       0.00      0.00      0.00         1\n",
      "         1042639138       0.00      0.00      0.00         1\n",
      "         1042639284       0.00      0.00      0.00         1\n",
      "         1042687505       0.00      0.00      0.00         1\n",
      "         1042733776       0.00      0.00      0.00         1\n",
      "         1042736285       0.00      0.00      0.00         1\n",
      "         1042739892       0.00      0.00      0.00         1\n",
      "         1042781796       0.00      0.00      0.00         1\n",
      "         1042785116       0.00      0.00      0.00         1\n",
      "         1042841034       0.00      0.00      0.00         1\n",
      "         1042875150       0.00      0.00      0.00         1\n",
      "         1042888982       0.00      0.00      0.00         1\n",
      "         1042960028       0.00      0.00      0.00         1\n",
      "         1043021240       0.00      0.00      0.00         1\n",
      "         1043034640       0.00      0.00      0.00         1\n",
      "         1043098678       0.00      0.00      0.00         1\n",
      "         1043104200       0.00      0.00      0.00         1\n",
      "         1043123572       0.00      0.00      0.00         1\n",
      "         1043172556       0.00      0.00      0.00         1\n",
      "         1043175924       0.00      0.00      0.00         1\n",
      "         1043202078       0.00      0.00      0.00         1\n",
      "         1043217224       0.00      0.00      0.00         1\n",
      "         1043224320       0.00      0.00      0.00         1\n",
      "         1043246744       0.00      0.00      0.00         1\n",
      "         1043254032       0.00      0.00      0.00         1\n",
      "         1043260656       0.00      0.00      0.00         1\n",
      "         1043295288       0.00      0.00      0.00         1\n",
      "         1043355112       0.00      0.00      0.00         1\n",
      "         1043445952       0.00      0.00      0.00         1\n",
      "         1043446834       0.00      0.00      0.00         1\n",
      "         1043446979       0.00      0.00      0.00         1\n",
      "         1043457978       0.00      0.00      0.00         1\n",
      "         1043511688       0.00      0.00      0.00         1\n",
      "         1043580158       0.00      0.00      0.00         1\n",
      "         1043602761       0.00      0.00      0.00         1\n",
      "         1043653976       0.00      0.00      0.00         1\n",
      "         1043686376       0.00      0.00      0.00         1\n",
      "         1043715068       0.00      0.00      0.00         1\n",
      "         1043738380       0.00      0.00      0.00         1\n",
      "         1043797840       0.00      0.00      0.00         1\n",
      "         1043855412       0.00      0.00      0.00         1\n",
      "         1043900924       0.00      0.00      0.00         1\n",
      "         1043905562       0.00      0.00      0.00         1\n",
      "         1043925832       0.00      0.00      0.00         1\n",
      "         1043957876       0.00      0.00      0.00         1\n",
      "         1044066576       0.00      0.00      0.00         1\n",
      "         1044088032       0.00      0.00      0.00         1\n",
      "         1044088256       0.00      0.00      0.00         1\n",
      "         1044105783       0.00      0.00      0.00         1\n",
      "         1044253320       0.00      0.00      0.00         1\n",
      "         1044253528       0.00      0.00      0.00         1\n",
      "         1044298652       0.00      0.00      0.00         1\n",
      "         1044302572       0.00      0.00      0.00         1\n",
      "         1044333630       0.00      0.00      0.00         1\n",
      "         1044341260       0.00      0.00      0.00         1\n",
      "         1044344044       0.00      0.00      0.00         1\n",
      "         1044349079       0.00      0.00      0.00         1\n",
      "         1044372219       0.00      0.00      0.00         1\n",
      "         1044373498       0.00      0.00      0.00         1\n",
      "         1044445272       0.00      0.00      0.00         1\n",
      "         1044509808       0.00      0.00      0.00         1\n",
      "         1044603992       0.00      0.00      0.00         1\n",
      "         1044657475       0.00      0.00      0.00         1\n",
      "         1044668424       0.00      0.00      0.00         1\n",
      "         1044675144       0.00      0.00      0.00         1\n",
      "         1044700696       0.00      0.00      0.00         1\n",
      "         1044837124       0.00      0.00      0.00         1\n",
      "         1044988664       0.00      0.00      0.00         1\n",
      "         1045035968       0.00      0.00      0.00         1\n",
      "         1045118532       0.00      0.00      0.00         1\n",
      "         1045118832       0.00      0.00      0.00         1\n",
      "         1045129532       0.00      0.00      0.00         1\n",
      "         1045178948       0.00      0.00      0.00         1\n",
      "         1045179213       0.00      0.00      0.00         1\n",
      "         1045247632       0.00      0.00      0.00         1\n",
      "         1045253002       0.00      0.00      0.00         1\n",
      "         1045267256       0.00      0.00      0.00         1\n",
      "         1045287788       0.00      0.00      0.00         1\n",
      "         1045302908       0.00      0.00      0.00         1\n",
      "         1045336286       0.00      0.00      0.00         1\n",
      "         1045431514       0.00      0.00      0.00         1\n",
      "         1045500744       0.00      0.00      0.00         1\n",
      "         1045577552       0.00      0.00      0.00         1\n",
      "         1045632584       0.00      0.00      0.00         1\n",
      "         1045672160       0.00      0.00      0.00         1\n",
      "         1045693532       0.00      0.00      0.00         1\n",
      "         1045713320       0.00      0.00      0.00         1\n",
      "         1045714637       0.00      0.00      0.00         1\n",
      "         1045754048       0.00      0.00      0.00         1\n",
      "         1045776100       0.00      0.00      0.00         1\n",
      "         1045779464       0.00      0.00      0.00         1\n",
      "         1045796661       0.00      0.00      0.00         1\n",
      "         1045845288       0.00      0.00      0.00         1\n",
      "         1045880176       0.00      0.00      0.00         1\n",
      "         1045893690       0.00      0.00      0.00         1\n",
      "         1045895928       0.00      0.00      0.00         1\n",
      "         1045953660       0.00      0.00      0.00         1\n",
      "         1045984265       0.00      0.00      0.00         1\n",
      "         1046030952       0.00      0.00      0.00         1\n",
      "         1046048234       0.00      0.00      0.00         1\n",
      "         1046071751       0.00      0.00      0.00         1\n",
      "         1046084895       0.00      0.00      0.00         1\n",
      "         1046095850       0.00      0.00      0.00         1\n",
      "         1046119544       0.00      0.00      0.00         1\n",
      "         1046148088       0.00      0.00      0.00         1\n",
      "         1046160728       0.00      0.00      0.00         1\n",
      "         1046180360       0.00      0.00      0.00         1\n",
      "         1046231679       0.00      0.00      0.00         1\n",
      "         1046265488       0.00      0.00      0.00         1\n",
      "         1046298828       0.00      0.00      0.00         1\n",
      "         1046325964       0.00      0.00      0.00         1\n",
      "         1046371414       0.00      0.00      0.00         1\n",
      "         1046431012       0.00      0.00      0.00         1\n",
      "         1046439212       0.00      0.00      0.00         1\n",
      "         1046475020       0.00      0.00      0.00         1\n",
      "         1046505756       0.00      0.00      0.00         1\n",
      "         1046537248       0.00      0.00      0.00         1\n",
      "         1046553292       0.00      0.00      0.00         1\n",
      "         1046557352       0.00      0.00      0.00         1\n",
      "         1046571001       0.00      0.00      0.00         1\n",
      "         1046631567       0.00      0.00      0.00         1\n",
      "         1046659244       0.00      0.00      0.00         1\n",
      "         1046685744       0.00      0.00      0.00         1\n",
      "         1046705956       0.00      0.00      0.00         1\n",
      "         1046712208       0.00      0.00      0.00         1\n",
      "         1046722197       0.00      0.00      0.00         1\n",
      "         1046748938       0.00      0.00      0.00         1\n",
      "         1046795104       0.00      0.00      0.00         1\n",
      "         1046828128       0.00      0.00      0.00         1\n",
      "         1046840824       0.00      0.00      0.00         1\n",
      "         1046859640       0.00      0.00      0.00         1\n",
      "         1046860988       0.00      0.00      0.00         1\n",
      "         1046861916       0.00      0.00      0.00         1\n",
      "         1046879198       0.00      0.00      0.00         1\n",
      "         1046905074       0.00      0.00      0.00         1\n",
      "         1046973172       0.00      0.00      0.00         1\n",
      "         1046995936       0.00      0.00      0.00         1\n",
      "         1047008160       0.00      0.00      0.00         1\n",
      "         1047047572       0.00      0.00      0.00         1\n",
      "         1047049905       0.00      0.00      0.00         1\n",
      "         1047133223       0.00      0.00      0.00         1\n",
      "         1047252192       0.00      0.00      0.00         1\n",
      "         1047275060       0.00      0.00      0.00         1\n",
      "         1047348100       0.00      0.00      0.00         1\n",
      "         1047386520       0.00      0.00      0.00         1\n",
      "         1047448569       0.00      0.00      0.00         1\n",
      "         1047490678       0.00      0.00      0.00         1\n",
      "         1047510094       0.00      0.00      0.00         1\n",
      "         1047529884       0.00      0.00      0.00         1\n",
      "         1047543932       0.00      0.00      0.00         1\n",
      "         1047579580       0.00      0.00      0.00         1\n",
      "         1047584236       0.00      0.00      0.00         1\n",
      "         1047595032       0.00      0.00      0.00         1\n",
      "         1047599320       0.00      0.00      0.00         1\n",
      "         1047599910       0.00      0.00      0.00         1\n",
      "         1047605308       0.00      0.00      0.00         1\n",
      "         1047631535       0.00      0.00      0.00         1\n",
      "         1047643287       0.00      0.00      0.00         1\n",
      "         1047714537       0.00      0.00      0.00         1\n",
      "         1047735408       0.00      0.00      0.00         1\n",
      "         1047743952       0.00      0.00      0.00         1\n",
      "         1047772276       0.00      0.00      0.00         1\n",
      "         1047799948       0.00      0.00      0.00         1\n",
      "         1047833536       0.00      0.00      0.00         1\n",
      "         1047838268       0.00      0.00      0.00         1\n",
      "         1047912225       0.00      0.00      0.00         1\n",
      "         1047930184       0.00      0.00      0.00         1\n",
      "         1047941240       0.00      0.00      0.00         1\n",
      "         1047946012       0.00      0.00      0.00         1\n",
      "         1047952324       0.00      0.00      0.00         1\n",
      "         1048006712       0.00      0.00      0.00         1\n",
      "         1048013042       0.00      0.00      0.00         1\n",
      "         1048060642       0.00      0.00      0.00         1\n",
      "         1048112964       0.00      0.00      0.00         1\n",
      "         1048150902       0.00      0.00      0.00         1\n",
      "         1048166712       0.00      0.00      0.00         1\n",
      "         1048198944       0.00      0.00      0.00         1\n",
      "         1048211694       0.00      0.00      0.00         1\n",
      "         1048303320       0.00      0.00      0.00         1\n",
      "         1048324546       0.00      0.00      0.00         1\n",
      "         1048347138       0.00      0.00      0.00         1\n",
      "         1048350861       0.00      0.00      0.00         1\n",
      "         1048373912       0.00      0.00      0.00         1\n",
      "         1048387968       0.00      0.00      0.00         1\n",
      "         1048399916       0.00      0.00      0.00         1\n",
      "         1048409312       0.00      0.00      0.00         1\n",
      "         1048415074       0.00      0.00      0.00         1\n",
      "         1048415196       0.00      0.00      0.00         1\n",
      "         1048465186       0.00      0.00      0.00         1\n",
      "         1048467272       0.00      0.00      0.00         1\n",
      "         1048482310       0.00      0.00      0.00         1\n",
      "         1048502836       0.00      0.00      0.00         1\n",
      "         1048525456       0.00      0.00      0.00         1\n",
      "         1048582218       0.00      0.00      0.00         1\n",
      "         1048587312       0.00      0.00      0.00         1\n",
      "         1048633687       0.00      0.00      0.00         1\n",
      "         1048634114       0.00      0.00      0.00         1\n",
      "         1048644564       0.00      0.00      0.00         1\n",
      "         1048645932       0.00      0.00      0.00         1\n",
      "         1048676392       0.00      0.00      0.00         1\n",
      "         1048719528       0.00      0.00      0.00         1\n",
      "         1048744892       0.00      0.00      0.00         1\n",
      "         1048793868       0.00      0.00      0.00         1\n",
      "         1048795366       0.00      0.00      0.00         1\n",
      "         1048808502       0.00      0.00      0.00         1\n",
      "         1048814222       0.00      0.00      0.00         1\n",
      "         1048824173       0.00      0.00      0.00         1\n",
      "         1048863884       0.00      0.00      0.00         1\n",
      "         1048870086       0.00      0.00      0.00         1\n",
      "         1048883340       0.00      0.00      0.00         1\n",
      "         1048884632       0.00      0.00      0.00         1\n",
      "         1048902022       0.00      0.00      0.00         1\n",
      "         1048923846       0.00      0.00      0.00         1\n",
      "         1048929082       0.00      0.00      0.00         1\n",
      "         1048929310       0.00      0.00      0.00         1\n",
      "         1048946098       0.00      0.00      0.00         1\n",
      "         1048946315       0.00      0.00      0.00         1\n",
      "         1048952134       0.00      0.00      0.00         1\n",
      "         1048957808       0.00      0.00      0.00         1\n",
      "         1049013270       0.00      0.00      0.00         1\n",
      "         1049023550       0.00      0.00      0.00         1\n",
      "         1049039204       0.00      0.00      0.00         1\n",
      "         1049044253       0.00      0.00      0.00         1\n",
      "         1049063808       0.00      0.00      0.00         1\n",
      "         1049075384       0.00      0.00      0.00         1\n",
      "         1049117282       0.00      0.00      0.00         1\n",
      "         1049129306       0.00      0.00      0.00         1\n",
      "         1049167124       0.00      0.00      0.00         1\n",
      "         1049180004       0.00      0.00      0.00         1\n",
      "         1049187867       0.00      0.00      0.00         1\n",
      "         1049207586       0.00      0.00      0.00         1\n",
      "         1049219056       0.00      0.00      0.00         1\n",
      "         1049229136       0.00      0.00      0.00         1\n",
      "         1049257946       0.00      0.00      0.00         1\n",
      "         1049262088       0.00      0.00      0.00         1\n",
      "         1049281988       0.00      0.00      0.00         1\n",
      "         1049318600       0.00      0.00      0.00         1\n",
      "         1049336346       0.00      0.00      0.00         1\n",
      "         1049338839       0.00      0.00      0.00         1\n",
      "         1049363102       0.00      0.00      0.00         1\n",
      "         1049366095       0.00      0.00      0.00         1\n",
      "         1049369208       0.00      0.00      0.00         1\n",
      "         1049407480       0.00      0.00      0.00         1\n",
      "         1049411238       0.00      0.00      0.00         1\n",
      "         1049429852       0.00      0.00      0.00         1\n",
      "         1049435870       0.00      0.00      0.00         1\n",
      "         1049449114       0.00      0.00      0.00         1\n",
      "         1049453506       0.00      0.00      0.00         1\n",
      "         1049456070       0.00      0.00      0.00         1\n",
      "         1049458480       0.00      0.00      0.00         1\n",
      "         1049472411       0.00      0.00      0.00         1\n",
      "         1049480664       0.00      0.00      0.00         1\n",
      "         1049494328       0.00      0.00      0.00         1\n",
      "         1049516904       0.00      0.00      0.00         1\n",
      "         1049518658       0.00      0.00      0.00         1\n",
      "         1049564055       0.00      0.00      0.00         1\n",
      "         1049603288       0.00      0.00      0.00         1\n",
      "         1049608208       0.00      0.00      0.00         1\n",
      "         1049612554       0.00      0.00      0.00         1\n",
      "         1049619208       0.00      0.00      0.00         1\n",
      "         1049627955       0.00      0.00      0.00         1\n",
      "         1049632794       0.00      0.00      0.00         1\n",
      "         1049636526       0.00      0.00      0.00         1\n",
      "         1049678596       0.00      0.00      0.00         1\n",
      "         1049685234       0.00      0.00      0.00         1\n",
      "         1049705248       0.00      0.00      0.00         1\n",
      "         1049709384       0.00      0.00      0.00         1\n",
      "         1049726822       0.00      0.00      0.00         1\n",
      "         1049730370       0.00      0.00      0.00         1\n",
      "         1049753475       0.00      0.00      0.00         1\n",
      "         1049783813       0.00      0.00      0.00         1\n",
      "         1049792073       0.00      0.00      0.00         1\n",
      "         1049797034       0.00      0.00      0.00         1\n",
      "         1049802920       0.00      0.00      0.00         1\n",
      "         1049805977       0.00      0.00      0.00         1\n",
      "         1049806372       0.00      0.00      0.00         1\n",
      "         1049817440       0.00      0.00      0.00         1\n",
      "         1049847342       0.00      0.00      0.00         1\n",
      "         1049859196       0.00      0.00      0.00         1\n",
      "         1049903916       0.00      0.00      0.00         1\n",
      "         1049925722       0.00      0.00      0.00         1\n",
      "         1049925735       0.00      0.00      0.00         1\n",
      "         1049967131       0.00      0.00      0.00         1\n",
      "         1049986202       0.00      0.00      0.00         1\n",
      "         1049991850       0.00      0.00      0.00         1\n",
      "         1049994396       0.00      0.00      0.00         1\n",
      "         1050016602       0.00      0.00      0.00         1\n",
      "         1050027540       0.00      0.00      0.00         1\n",
      "         1050032223       0.00      0.00      0.00         1\n",
      "         1050036976       0.00      0.00      0.00         1\n",
      "         1050050506       0.00      0.00      0.00         1\n",
      "         1050052032       0.00      0.00      0.00         1\n",
      "         1050082040       0.00      0.00      0.00         1\n",
      "         1050088251       0.00      0.00      0.00         1\n",
      "         1050088585       0.00      0.00      0.00         1\n",
      "         1050089373       0.00      0.00      0.00         1\n",
      "         1050124833       0.00      0.00      0.00         1\n",
      "         1050135340       0.00      0.00      0.00         1\n",
      "         1050155096       0.00      0.00      0.00         1\n",
      "         1050207029       0.00      0.00      0.00         1\n",
      "         1050214546       0.00      0.00      0.00         1\n",
      "         1050218885       0.00      0.00      0.00         1\n",
      "         1050224762       0.00      0.00      0.00         1\n",
      "         1050228604       0.00      0.00      0.00         1\n",
      "         1050244044       0.00      0.00      0.00         1\n",
      "         1050258983       0.00      0.00      0.00         1\n",
      "         1050278281       0.00      0.00      0.00         1\n",
      "         1050278764       0.00      0.00      0.00         1\n",
      "         1050313095       0.00      0.00      0.00         1\n",
      "         1050320003       0.00      0.00      0.00         1\n",
      "         1050343116       0.00      0.00      0.00         1\n",
      "         1050353032       0.00      0.00      0.00         1\n",
      "         1050355222       0.00      0.00      0.00         1\n",
      "         1050367034       0.00      0.00      0.00         1\n",
      "         1050416046       0.00      0.00      0.00         1\n",
      "         1050423814       0.00      0.00      0.00         1\n",
      "         1050430100       0.00      0.00      0.00         1\n",
      "         1050455948       0.00      0.00      0.00         1\n",
      "         1050466554       0.00      0.00      0.00         1\n",
      "         1050478646       0.00      0.00      0.00         1\n",
      "         1050483979       0.00      0.00      0.00         1\n",
      "         1050517410       0.00      0.00      0.00         1\n",
      "         1050520393       0.00      0.00      0.00         1\n",
      "         1050523063       0.00      0.00      0.00         1\n",
      "         1050531205       0.00      0.00      0.00         1\n",
      "         1050567372       0.00      0.00      0.00         1\n",
      "         1050573353       0.00      0.00      0.00         1\n",
      "         1050580868       0.00      0.00      0.00         1\n",
      "         1050617766       0.00      0.00      0.00         1\n",
      "         1050640592       0.00      0.00      0.00         1\n",
      "         1050655970       0.00      0.00      0.00         1\n",
      "         1050665390       0.00      0.00      0.00         1\n",
      "         1050678560       0.00      0.00      0.00         1\n",
      "         1050692052       0.00      0.00      0.00         1\n",
      "         1050736514       0.00      0.00      0.00         1\n",
      "         1050741656       0.00      0.00      0.00         1\n",
      "         1050745345       0.00      0.00      0.00         1\n",
      "         1050758843       0.00      0.00      0.00         1\n",
      "         1050820432       0.00      0.00      0.00         1\n",
      "         1050874036       0.00      0.00      0.00         1\n",
      "         1050881394       0.00      0.00      0.00         1\n",
      "         1050881998       0.00      0.00      0.00         1\n",
      "         1050886147       0.00      0.00      0.00         1\n",
      "         1050886858       0.00      0.00      0.00         1\n",
      "         1050891062       0.00      0.00      0.00         1\n",
      "         1050904492       0.00      0.00      0.00         1\n",
      "         1050931224       0.00      0.00      0.00         1\n",
      "         1050934788       0.00      0.00      0.00         1\n",
      "         1050938683       0.00      0.00      0.00         1\n",
      "         1050953369       0.00      0.00      0.00         1\n",
      "         1050955035       0.00      0.00      0.00         1\n",
      "         1050956434       0.00      0.00      0.00         1\n",
      "         1050968674       0.00      0.00      0.00         1\n",
      "         1050986047       0.00      0.00      0.00         1\n",
      "         1050997276       0.00      0.00      0.00         1\n",
      "         1051006404       0.00      0.00      0.00         1\n",
      "         1051021015       0.00      0.00      0.00         1\n",
      "         1051021586       0.00      0.00      0.00         1\n",
      "         1051057050       0.00      0.00      0.00         1\n",
      "         1051080758       0.00      0.00      0.00         1\n",
      "         1051115014       0.00      0.00      0.00         1\n",
      "         1051154294       0.00      0.00      0.00         1\n",
      "         1051168650       0.00      0.00      0.00         1\n",
      "         1051191684       0.00      0.00      0.00         1\n",
      "         1051193737       0.00      0.00      0.00         1\n",
      "         1051200226       0.00      0.00      0.00         1\n",
      "         1051229660       0.00      0.00      0.00         1\n",
      "         1051235150       0.00      0.00      0.00         1\n",
      "         1051241450       0.00      0.00      0.00         1\n",
      "         1051266416       0.00      0.00      0.00         1\n",
      "         1051274398       0.00      0.00      0.00         1\n",
      "         1051274684       0.00      0.00      0.00         1\n",
      "         1051283008       0.00      0.00      0.00         1\n",
      "         1051284670       0.00      0.00      0.00         1\n",
      "         1051290268       0.00      0.00      0.00         1\n",
      "         1051296904       0.00      0.00      0.00         1\n",
      "         1051316905       0.00      0.00      0.00         1\n",
      "         1051331906       0.00      0.00      0.00         1\n",
      "         1051342400       0.00      0.00      0.00         1\n",
      "         1051382345       0.00      0.00      0.00         1\n",
      "         1051388293       0.00      0.00      0.00         1\n",
      "         1051417776       0.00      0.00      0.00         1\n",
      "         1051429908       0.00      0.00      0.00         1\n",
      "         1051453850       0.00      0.00      0.00         1\n",
      "         1051565660       0.00      0.00      0.00         1\n",
      "         1051571142       0.00      0.00      0.00         1\n",
      "         1051582507       0.00      0.00      0.00         1\n",
      "         1051631569       0.00      0.00      0.00         1\n",
      "         1051634258       0.00      0.00      0.00         1\n",
      "         1051663046       0.00      0.00      0.00         1\n",
      "         1051670412       0.00      0.00      0.00         1\n",
      "         1051674190       0.00      0.00      0.00         1\n",
      "         1051687434       0.00      0.00      0.00         1\n",
      "         1051708582       0.00      0.00      0.00         1\n",
      "         1051711318       0.00      0.00      0.00         1\n",
      "         1051735193       0.00      0.00      0.00         1\n",
      "         1051737846       0.00      0.00      0.00         1\n",
      "         1051750469       0.00      0.00      0.00         1\n",
      "         1051752557       0.00      0.00      0.00         1\n",
      "         1051777516       0.00      0.00      0.00         1\n",
      "         1051782530       0.00      0.00      0.00         1\n",
      "         1051794556       0.00      0.00      0.00         1\n",
      "         1051805744       0.00      0.00      0.00         1\n",
      "         1051813034       0.00      0.00      0.00         1\n",
      "         1051818422       0.00      0.00      0.00         1\n",
      "         1051823312       0.00      0.00      0.00         1\n",
      "         1051836146       0.00      0.00      0.00         1\n",
      "         1051836771       0.00      0.00      0.00         1\n",
      "         1051849668       0.00      0.00      0.00         1\n",
      "         1051864544       0.00      0.00      0.00         1\n",
      "         1051880158       0.00      0.00      0.00         1\n",
      "         1051884146       0.00      0.00      0.00         1\n",
      "         1051886115       0.00      0.00      0.00         1\n",
      "         1051893296       0.00      0.00      0.00         1\n",
      "         1051920670       0.00      0.00      0.00         1\n",
      "         1051940708       0.00      0.00      0.00         1\n",
      "         1051958596       0.00      0.00      0.00         1\n",
      "         1051979168       0.00      0.00      0.00         1\n",
      "         1051982096       0.00      0.00      0.00         1\n",
      "         1052039256       0.00      0.00      0.00         1\n",
      "         1052101199       0.00      0.00      0.00         1\n",
      "         1052105036       0.00      0.00      0.00         1\n",
      "         1052113166       0.00      0.00      0.00         1\n",
      "         1052145395       0.00      0.00      0.00         1\n",
      "         1052151714       0.00      0.00      0.00         1\n",
      "         1052167578       0.00      0.00      0.00         1\n",
      "         1052182435       0.00      0.00      0.00         1\n",
      "         1052184979       0.00      0.00      0.00         1\n",
      "         1052191803       0.00      0.00      0.00         1\n",
      "         1052196318       0.00      0.00      0.00         1\n",
      "         1052228038       0.00      0.00      0.00         1\n",
      "         1052230390       0.00      0.00      0.00         1\n",
      "         1052233012       0.00      0.00      0.00         1\n",
      "         1052265992       0.00      0.00      0.00         1\n",
      "         1052272300       0.00      0.00      0.00         1\n",
      "         1052287554       0.00      0.00      0.00         1\n",
      "         1052312920       0.00      0.00      0.00         1\n",
      "         1052337814       0.00      0.00      0.00         1\n",
      "         1052356277       0.00      0.00      0.00         1\n",
      "         1052363966       0.00      0.00      0.00         1\n",
      "         1052396008       0.00      0.00      0.00         1\n",
      "         1052411653       0.00      0.00      0.00         1\n",
      "         1052421686       0.00      0.00      0.00         1\n",
      "         1052435586       0.00      0.00      0.00         1\n",
      "         1052441814       0.00      0.00      0.00         1\n",
      "         1052458852       0.00      0.00      0.00         1\n",
      "         1052464670       0.00      0.00      0.00         1\n",
      "         1052468780       0.00      0.00      0.00         1\n",
      "         1052504318       0.00      0.00      0.00         1\n",
      "         1052508188       0.00      0.00      0.00         1\n",
      "         1052509500       0.00      0.00      0.00         1\n",
      "         1052519548       0.00      0.00      0.00         1\n",
      "         1052551318       0.00      0.00      0.00         1\n",
      "         1052558900       0.00      0.00      0.00         1\n",
      "         1052594583       0.00      0.00      0.00         1\n",
      "         1052600176       0.00      0.00      0.00         1\n",
      "         1052602458       0.00      0.00      0.00         1\n",
      "         1052658712       0.00      0.00      0.00         1\n",
      "         1052669040       0.00      0.00      0.00         1\n",
      "         1052705645       0.00      0.00      0.00         1\n",
      "         1052767862       0.00      0.00      0.00         1\n",
      "         1052780730       0.00      0.00      0.00         1\n",
      "         1052784102       0.00      0.00      0.00         1\n",
      "         1052793504       0.00      0.00      0.00         1\n",
      "         1052796992       0.00      0.00      0.00         1\n",
      "         1052839126       0.00      0.00      0.00         1\n",
      "         1052843714       0.00      0.00      0.00         1\n",
      "         1052844420       0.00      0.00      0.00         1\n",
      "         1052849728       0.00      0.00      0.00         1\n",
      "         1052858608       0.00      0.00      0.00         1\n",
      "         1052859314       0.00      0.00      0.00         1\n",
      "         1052862778       0.00      0.00      0.00         1\n",
      "         1052862950       0.00      0.00      0.00         1\n",
      "         1052892040       0.00      0.00      0.00         1\n",
      "         1052893346       0.00      0.00      0.00         1\n",
      "         1052902752       0.00      0.00      0.00         1\n",
      "         1052916584       0.00      0.00      0.00         1\n",
      "         1052927434       0.00      0.00      0.00         1\n",
      "         1052932480       0.00      0.00      0.00         1\n",
      "         1052942242       0.00      0.00      0.00         1\n",
      "         1052942640       0.00      0.00      0.00         1\n",
      "         1052953702       0.00      0.00      0.00         1\n",
      "         1052987846       0.00      0.00      0.00         1\n",
      "         1052996874       0.00      0.00      0.00         1\n",
      "         1052998926       0.00      0.00      0.00         1\n",
      "         1053005890       0.00      0.00      0.00         1\n",
      "         1053017724       0.00      0.00      0.00         1\n",
      "         1053021172       0.00      0.00      0.00         1\n",
      "         1053062017       0.00      0.00      0.00         1\n",
      "         1053067750       0.00      0.00      0.00         1\n",
      "         1053093188       0.00      0.00      0.00         1\n",
      "         1053116406       0.00      0.00      0.00         1\n",
      "         1053120004       0.00      0.00      0.00         1\n",
      "         1053155024       0.00      0.00      0.00         1\n",
      "         1053163174       0.00      0.00      0.00         1\n",
      "         1053193316       0.00      0.00      0.00         1\n",
      "         1053194000       0.00      0.00      0.00         1\n",
      "         1053206672       0.00      0.00      0.00         1\n",
      "         1053207546       0.00      0.00      0.00         1\n",
      "         1053213546       0.00      0.00      0.00         1\n",
      "         1053216506       0.00      0.00      0.00         1\n",
      "         1053229168       0.00      0.00      0.00         1\n",
      "         1053232756       0.00      0.00      0.00         1\n",
      "         1053240450       0.00      0.00      0.00         1\n",
      "         1053278159       0.00      0.00      0.00         1\n",
      "         1053293360       0.00      0.00      0.00         1\n",
      "         1053294104       0.00      0.00      0.00         1\n",
      "         1053309708       0.00      0.00      0.00         1\n",
      "         1053321798       0.00      0.00      0.00         1\n",
      "         1053338509       0.00      0.00      0.00         1\n",
      "         1053436421       0.00      0.00      0.00         1\n",
      "         1053495714       0.00      0.00      0.00         1\n",
      "         1053507326       0.00      0.00      0.00         1\n",
      "         1053522618       0.00      0.00      0.00         1\n",
      "         1053544172       0.00      0.00      0.00         1\n",
      "         1053546705       0.00      0.00      0.00         1\n",
      "         1053552908       0.00      0.00      0.00         1\n",
      "         1053568262       0.00      0.00      0.00         1\n",
      "         1053571383       0.00      0.00      0.00         1\n",
      "         1053575990       0.00      0.00      0.00         1\n",
      "         1053592879       0.00      0.00      0.00         1\n",
      "         1053602455       0.00      0.00      0.00         1\n",
      "         1053609434       0.00      0.00      0.00         1\n",
      "         1053619219       0.00      0.00      0.00         1\n",
      "         1053673024       0.00      0.00      0.00         1\n",
      "         1053691693       0.00      0.00      0.00         1\n",
      "         1053731548       0.00      0.00      0.00         1\n",
      "         1053740499       0.00      0.00      0.00         1\n",
      "         1053764712       0.00      0.00      0.00         1\n",
      "         1053772644       0.00      0.00      0.00         1\n",
      "         1053777098       0.00      0.00      0.00         1\n",
      "         1053780818       0.00      0.00      0.00         1\n",
      "         1053788030       0.00      0.00      0.00         1\n",
      "         1053791021       0.00      0.00      0.00         1\n",
      "         1053796142       0.00      0.00      0.00         1\n",
      "         1053810090       0.00      0.00      0.00         1\n",
      "         1053816574       0.00      0.00      0.00         1\n",
      "         1053875352       0.00      0.00      0.00         1\n",
      "         1053897107       0.00      0.00      0.00         1\n",
      "         1053904956       0.00      0.00      0.00         1\n",
      "         1053908018       0.00      0.00      0.00         1\n",
      "         1053910124       0.00      0.00      0.00         1\n",
      "         1054036602       0.00      0.00      0.00         1\n",
      "         1054047522       0.00      0.00      0.00         1\n",
      "         1054060282       0.00      0.00      0.00         1\n",
      "         1054081664       0.00      0.00      0.00         1\n",
      "         1054100431       0.00      0.00      0.00         1\n",
      "         1054113189       0.00      0.00      0.00         1\n",
      "         1054115062       0.00      0.00      0.00         1\n",
      "         1054134141       0.00      0.00      0.00         1\n",
      "         1054148064       0.00      0.00      0.00         1\n",
      "         1054148500       0.00      0.00      0.00         1\n",
      "         1054163788       0.00      0.00      0.00         1\n",
      "         1054166757       0.00      0.00      0.00         1\n",
      "         1054172711       0.00      0.00      0.00         1\n",
      "         1054203082       0.00      0.00      0.00         1\n",
      "         1054212485       0.00      0.00      0.00         1\n",
      "         1054229406       0.00      0.00      0.00         1\n",
      "         1054232488       0.00      0.00      0.00         1\n",
      "         1054267932       0.00      0.00      0.00         1\n",
      "         1054279232       0.00      0.00      0.00         1\n",
      "         1054306448       0.00      0.00      0.00         1\n",
      "         1054309148       0.00      0.00      0.00         1\n",
      "         1054311558       0.00      0.00      0.00         1\n",
      "         1054317838       0.00      0.00      0.00         1\n",
      "         1054335404       0.00      0.00      0.00         1\n",
      "         1054352499       0.00      0.00      0.00         1\n",
      "         1054375220       0.00      0.00      0.00         1\n",
      "         1054385319       0.00      0.00      0.00         1\n",
      "         1054387413       0.00      0.00      0.00         1\n",
      "         1054396744       0.00      0.00      0.00         1\n",
      "         1054425267       0.00      0.00      0.00         1\n",
      "         1054472245       0.00      0.00      0.00         1\n",
      "         1054498334       0.00      0.00      0.00         1\n",
      "         1054526395       0.00      0.00      0.00         1\n",
      "         1054545884       0.00      0.00      0.00         1\n",
      "         1054558045       0.00      0.00      0.00         1\n",
      "         1054581196       0.00      0.00      0.00         1\n",
      "         1054624836       0.00      0.00      0.00         1\n",
      "         1054675683       0.00      0.00      0.00         1\n",
      "         1054690896       0.00      0.00      0.00         1\n",
      "         1054708074       0.00      0.00      0.00         1\n",
      "         1054717689       0.00      0.00      0.00         1\n",
      "         1054718021       0.00      0.00      0.00         1\n",
      "         1054719006       0.00      0.00      0.00         1\n",
      "         1054719321       0.00      0.00      0.00         1\n",
      "         1054731724       0.00      0.00      0.00         1\n",
      "         1054794420       0.00      0.00      0.00         1\n",
      "         1054799288       0.00      0.00      0.00         1\n",
      "         1054818186       0.00      0.00      0.00         1\n",
      "         1054842569       0.00      0.00      0.00         1\n",
      "         1054843635       0.00      0.00      0.00         1\n",
      "         1054848218       0.00      0.00      0.00         1\n",
      "         1054855944       0.00      0.00      0.00         1\n",
      "         1054857676       0.00      0.00      0.00         1\n",
      "         1054867477       0.00      0.00      0.00         1\n",
      "         1054889728       0.00      0.00      0.00         1\n",
      "         1054907684       0.00      0.00      0.00         1\n",
      "         1054912680       0.00      0.00      0.00         1\n",
      "         1054921152       0.00      0.00      0.00         1\n",
      "         1054935065       0.00      0.00      0.00         1\n",
      "         1054941873       0.00      0.00      0.00         1\n",
      "         1054946459       0.00      0.00      0.00         1\n",
      "         1054979442       0.00      0.00      0.00         1\n",
      "         1054981519       0.00      0.00      0.00         1\n",
      "         1054993900       0.00      0.00      0.00         1\n",
      "         1055021851       0.00      0.00      0.00         1\n",
      "         1055047424       0.00      0.00      0.00         1\n",
      "         1055075986       0.00      0.00      0.00         1\n",
      "         1055096842       0.00      0.00      0.00         1\n",
      "         1055101088       0.00      0.00      0.00         1\n",
      "         1055118263       0.00      0.00      0.00         1\n",
      "         1055120414       0.00      0.00      0.00         1\n",
      "         1055124482       0.00      0.00      0.00         1\n",
      "         1055132824       0.00      0.00      0.00         1\n",
      "         1055149236       0.00      0.00      0.00         1\n",
      "         1055150395       0.00      0.00      0.00         1\n",
      "         1055273168       0.00      0.00      0.00         1\n",
      "         1055299220       0.00      0.00      0.00         1\n",
      "         1055299576       0.00      0.00      0.00         1\n",
      "         1055305739       0.00      0.00      0.00         1\n",
      "         1055310612       0.00      0.00      0.00         1\n",
      "         1055316083       0.00      0.00      0.00         1\n",
      "         1055344325       0.00      0.00      0.00         1\n",
      "         1055356978       0.00      0.00      0.00         1\n",
      "         1055369622       0.00      0.00      0.00         1\n",
      "         1055387666       0.00      0.00      0.00         1\n",
      "         1055406769       0.00      0.00      0.00         1\n",
      "         1055413670       0.00      0.00      0.00         1\n",
      "         1055424999       0.00      0.00      0.00         1\n",
      "         1055465799       0.00      0.00      0.00         1\n",
      "         1055480594       0.00      0.00      0.00         1\n",
      "         1055491402       0.00      0.00      0.00         1\n",
      "         1055495214       0.00      0.00      0.00         1\n",
      "         1055578161       0.00      0.00      0.00         1\n",
      "         1055609396       0.00      0.00      0.00         1\n",
      "         1055611262       0.00      0.00      0.00         1\n",
      "         1055613142       0.00      0.00      0.00         1\n",
      "         1055640196       0.00      0.00      0.00         1\n",
      "         1055665081       0.00      0.00      0.00         1\n",
      "         1055724805       0.00      0.00      0.00         1\n",
      "         1055742912       0.00      0.00      0.00         1\n",
      "         1055769738       0.00      0.00      0.00         1\n",
      "         1055773894       0.00      0.00      0.00         1\n",
      "         1055807492       0.00      0.00      0.00         1\n",
      "         1055821019       0.00      0.00      0.00         1\n",
      "         1055822192       0.00      0.00      0.00         1\n",
      "         1055836827       0.00      0.00      0.00         1\n",
      "         1055841794       0.00      0.00      0.00         1\n",
      "         1055850475       0.00      0.00      0.00         1\n",
      "         1055869368       0.00      0.00      0.00         1\n",
      "         1055869670       0.00      0.00      0.00         1\n",
      "         1055870036       0.00      0.00      0.00         1\n",
      "         1055879448       0.00      0.00      0.00         1\n",
      "         1055921013       0.00      0.00      0.00         1\n",
      "         1055934190       0.00      0.00      0.00         1\n",
      "         1055981092       0.00      0.00      0.00         1\n",
      "         1055989129       0.00      0.00      0.00         1\n",
      "         1055996398       0.00      0.00      0.00         1\n",
      "         1056003072       0.00      0.00      0.00         1\n",
      "         1056018007       0.00      0.00      0.00         1\n",
      "         1056022137       0.00      0.00      0.00         1\n",
      "         1056062070       0.00      0.00      0.00         1\n",
      "         1056078399       0.00      0.00      0.00         1\n",
      "         1056080910       0.00      0.00      0.00         1\n",
      "         1056095692       0.00      0.00      0.00         1\n",
      "         1056145550       0.00      0.00      0.00         1\n",
      "         1056160074       0.00      0.00      0.00         1\n",
      "         1056183640       0.00      0.00      0.00         1\n",
      "         1056200578       0.00      0.00      0.00         1\n",
      "         1056203823       0.00      0.00      0.00         1\n",
      "         1056212894       0.00      0.00      0.00         1\n",
      "         1056225198       0.00      0.00      0.00         1\n",
      "         1056232968       0.00      0.00      0.00         1\n",
      "         1056236218       0.00      0.00      0.00         1\n",
      "         1056242308       0.00      0.00      0.00         1\n",
      "         1056248930       0.00      0.00      0.00         1\n",
      "         1056257312       0.00      0.00      0.00         1\n",
      "         1056292845       0.00      0.00      0.00         1\n",
      "         1056299184       0.00      0.00      0.00         1\n",
      "         1056318783       0.00      0.00      0.00         1\n",
      "         1056319575       0.00      0.00      0.00         1\n",
      "         1056336092       0.00      0.00      0.00         1\n",
      "         1056372220       0.00      0.00      0.00         1\n",
      "         1056375378       0.00      0.00      0.00         1\n",
      "         1056379238       0.00      0.00      0.00         1\n",
      "         1056405513       0.00      0.00      0.00         1\n",
      "         1056434491       0.00      0.00      0.00         1\n",
      "         1056450964       0.00      0.00      0.00         1\n",
      "         1056454452       0.00      0.00      0.00         1\n",
      "         1056466733       0.00      0.00      0.00         1\n",
      "         1056505202       0.00      0.00      0.00         1\n",
      "         1056508945       0.00      0.00      0.00         1\n",
      "         1056512574       0.00      0.00      0.00         1\n",
      "         1056532060       0.00      0.00      0.00         1\n",
      "         1056534869       0.00      0.00      0.00         1\n",
      "         1056542862       0.00      0.00      0.00         1\n",
      "         1056563870       0.00      0.00      0.00         1\n",
      "         1056581800       0.00      0.00      0.00         1\n",
      "         1056617066       0.00      0.00      0.00         1\n",
      "         1056618675       0.00      0.00      0.00         1\n",
      "         1056628216       0.00      0.00      0.00         1\n",
      "         1056630031       0.00      0.00      0.00         1\n",
      "         1056634196       0.00      0.00      0.00         1\n",
      "         1056640266       0.00      0.00      0.00         1\n",
      "         1056662890       0.00      0.00      0.00         1\n",
      "         1056674359       0.00      0.00      0.00         1\n",
      "         1056683653       0.00      0.00      0.00         1\n",
      "         1056685484       0.00      0.00      0.00         1\n",
      "         1056685732       0.00      0.00      0.00         1\n",
      "         1056687794       0.00      0.00      0.00         1\n",
      "         1056705587       0.00      0.00      0.00         1\n",
      "         1056715743       0.00      0.00      0.00         1\n",
      "         1056723784       0.00      0.00      0.00         1\n",
      "         1056772912       0.00      0.00      0.00         1\n",
      "         1056779490       0.00      0.00      0.00         1\n",
      "         1056811876       0.00      0.00      0.00         1\n",
      "         1056817796       0.00      0.00      0.00         1\n",
      "         1056827156       0.00      0.00      0.00         1\n",
      "         1056848398       0.00      0.00      0.00         1\n",
      "         1056858572       0.00      0.00      0.00         1\n",
      "         1056891686       0.00      0.00      0.00         1\n",
      "         1056896989       0.00      0.00      0.00         1\n",
      "         1056908816       0.00      0.00      0.00         1\n",
      "         1056983673       0.00      0.00      0.00         1\n",
      "         1056998584       0.00      0.00      0.00         1\n",
      "         1057002014       0.00      0.00      0.00         1\n",
      "         1057009433       0.00      0.00      0.00         1\n",
      "         1057012555       0.00      0.00      0.00         1\n",
      "         1057017073       0.00      0.00      0.00         1\n",
      "         1057026070       0.00      0.00      0.00         1\n",
      "         1057030036       0.00      0.00      0.00         1\n",
      "         1057031186       0.00      0.00      0.00         1\n",
      "         1057039817       0.00      0.00      0.00         1\n",
      "         1057049688       0.00      0.00      0.00         1\n",
      "         1057076920       0.00      0.00      0.00         1\n",
      "         1057081732       0.00      0.00      0.00         1\n",
      "         1057086342       0.00      0.00      0.00         1\n",
      "         1057086988       0.00      0.00      0.00         1\n",
      "         1057095661       0.00      0.00      0.00         1\n",
      "         1057100226       0.00      0.00      0.00         1\n",
      "         1057100350       0.00      0.00      0.00         1\n",
      "         1057104724       0.00      0.00      0.00         1\n",
      "         1057105259       0.00      0.00      0.00         1\n",
      "         1057111886       0.00      0.00      0.00         1\n",
      "         1057118793       0.00      0.00      0.00         1\n",
      "         1057141305       0.00      0.00      0.00         1\n",
      "         1057141940       0.00      0.00      0.00         1\n",
      "         1057166302       0.00      0.00      0.00         1\n",
      "         1057182788       0.00      0.00      0.00         1\n",
      "         1057199673       0.00      0.00      0.00         1\n",
      "         1057213210       0.00      0.00      0.00         1\n",
      "         1057213971       0.00      0.00      0.00         1\n",
      "         1057217925       0.00      0.00      0.00         1\n",
      "         1057259921       0.00      0.00      0.00         1\n",
      "         1057265649       0.00      0.00      0.00         1\n",
      "         1057284250       0.00      0.00      0.00         1\n",
      "         1057296482       0.00      0.00      0.00         1\n",
      "         1057298691       0.00      0.00      0.00         1\n",
      "         1057298700       0.00      0.00      0.00         1\n",
      "         1057311751       0.00      0.00      0.00         1\n",
      "         1057321807       0.00      0.00      0.00         1\n",
      "         1057328141       0.00      0.00      0.00         1\n",
      "         1057332028       0.00      0.00      0.00         1\n",
      "         1057336761       0.00      0.00      0.00         1\n",
      "         1057338432       0.00      0.00      0.00         1\n",
      "         1057342113       0.00      0.00      0.00         1\n",
      "         1057345132       0.00      0.00      0.00         1\n",
      "         1057355655       0.00      0.00      0.00         1\n",
      "         1057366340       0.00      0.00      0.00         1\n",
      "         1057412542       0.00      0.00      0.00         1\n",
      "         1057419228       0.00      0.00      0.00         1\n",
      "         1057436017       0.00      0.00      0.00         1\n",
      "         1057453560       0.00      0.00      0.00         1\n",
      "         1057454664       0.00      0.00      0.00         1\n",
      "         1057459486       0.00      0.00      0.00         1\n",
      "         1057467308       0.00      0.00      0.00         1\n",
      "         1057477748       0.00      0.00      0.00         1\n",
      "         1057479896       0.00      0.00      0.00         1\n",
      "         1057488873       0.00      0.00      0.00         1\n",
      "         1057498098       0.00      0.00      0.00         1\n",
      "         1057521207       0.00      0.00      0.00         1\n",
      "         1057525682       0.00      0.00      0.00         1\n",
      "         1057534070       0.00      0.00      0.00         1\n",
      "         1057534224       0.00      0.00      0.00         1\n",
      "         1057541768       0.00      0.00      0.00         1\n",
      "         1057545079       0.00      0.00      0.00         1\n",
      "         1057553028       0.00      0.00      0.00         1\n",
      "         1057556427       0.00      0.00      0.00         1\n",
      "         1057567241       0.00      0.00      0.00         1\n",
      "         1057580620       0.00      0.00      0.00         1\n",
      "         1057584208       0.00      0.00      0.00         1\n",
      "         1057586342       0.00      0.00      0.00         1\n",
      "         1057593124       0.00      0.00      0.00         1\n",
      "         1057607940       0.00      0.00      0.00         1\n",
      "         1057619981       0.00      0.00      0.00         1\n",
      "         1057628687       0.00      0.00      0.00         1\n",
      "         1057638209       0.00      0.00      0.00         1\n",
      "         1057657662       0.00      0.00      0.00         1\n",
      "         1057665200       0.00      0.00      0.00         1\n",
      "         1057666932       0.00      0.00      0.00         1\n",
      "         1057669540       0.00      0.00      0.00         1\n",
      "         1057682233       0.00      0.00      0.00         1\n",
      "         1057683304       0.00      0.00      0.00         1\n",
      "         1057683960       0.00      0.00      0.00         1\n",
      "         1057686402       0.00      0.00      0.00         1\n",
      "         1057714403       0.00      0.00      0.00         1\n",
      "         1057714926       0.00      0.00      0.00         1\n",
      "         1057738992       0.00      0.00      0.00         1\n",
      "         1057739825       0.00      0.00      0.00         1\n",
      "         1057749694       0.00      0.00      0.00         1\n",
      "         1057760323       0.00      0.00      0.00         1\n",
      "         1057761169       0.00      0.00      0.00         1\n",
      "         1057776203       0.00      0.00      0.00         1\n",
      "         1057785874       0.00      0.00      0.00         1\n",
      "         1057816516       0.00      0.00      0.00         1\n",
      "         1057825980       0.00      0.00      0.00         1\n",
      "         1057827642       0.00      0.00      0.00         1\n",
      "         1057831749       0.00      0.00      0.00         1\n",
      "         1057844504       0.00      0.00      0.00         1\n",
      "         1057857200       0.00      0.00      0.00         1\n",
      "         1057865950       0.00      0.00      0.00         1\n",
      "         1057867098       0.00      0.00      0.00         1\n",
      "         1057872080       0.00      0.00      0.00         1\n",
      "         1057873838       0.00      0.00      0.00         1\n",
      "         1057877883       0.00      0.00      0.00         1\n",
      "         1057879888       0.00      0.00      0.00         1\n",
      "         1057890640       0.00      0.00      0.00         1\n",
      "         1057894503       0.00      0.00      0.00         1\n",
      "         1057899097       0.00      0.00      0.00         1\n",
      "         1057904827       0.00      0.00      0.00         1\n",
      "         1057909864       0.00      0.00      0.00         1\n",
      "         1057915886       0.00      0.00      0.00         1\n",
      "         1057921026       0.00      0.00      0.00         1\n",
      "         1057924252       0.00      0.00      0.00         1\n",
      "         1057943034       0.00      0.00      0.00         1\n",
      "         1057953316       0.00      0.00      0.00         1\n",
      "         1057962399       0.00      0.00      0.00         1\n",
      "         1057969888       0.00      0.00      0.00         1\n",
      "         1057969979       0.00      0.00      0.00         1\n",
      "         1057971803       0.00      0.00      0.00         1\n",
      "         1057982728       0.00      0.00      0.00         1\n",
      "         1057985199       0.00      0.00      0.00         1\n",
      "         1057995851       0.00      0.00      0.00         1\n",
      "         1057999981       0.00      0.00      0.00         1\n",
      "         1058002329       0.00      0.00      0.00         1\n",
      "         1058007662       0.00      0.00      0.00         1\n",
      "         1058011380       0.00      0.00      0.00         1\n",
      "         1058014952       0.00      0.00      0.00         1\n",
      "         1058020593       0.00      0.00      0.00         1\n",
      "         1058027618       0.00      0.00      0.00         1\n",
      "         1058033389       0.00      0.00      0.00         1\n",
      "         1058039422       0.00      0.00      0.00         1\n",
      "         1058051849       0.00      0.00      0.00         1\n",
      "         1058073484       0.00      0.00      0.00         1\n",
      "         1058082266       0.00      0.00      0.00         1\n",
      "         1058112236       0.00      0.00      0.00         1\n",
      "         1058114319       0.00      0.00      0.00         1\n",
      "         1058120955       0.00      0.00      0.00         1\n",
      "         1058129250       0.00      0.00      0.00         1\n",
      "         1058135342       0.00      0.00      0.00         1\n",
      "         1058146466       0.00      0.00      0.00         1\n",
      "         1058155533       0.00      0.00      0.00         1\n",
      "         1058165160       0.00      0.00      0.00         1\n",
      "         1058170600       0.00      0.00      0.00         1\n",
      "         1058209409       0.00      0.00      0.00         1\n",
      "         1058217252       0.00      0.00      0.00         1\n",
      "         1058231330       0.00      0.00      0.00         1\n",
      "         1058246312       0.00      0.00      0.00         1\n",
      "         1058249709       0.00      0.00      0.00         1\n",
      "         1058249758       0.00      0.00      0.00         1\n",
      "         1058251518       0.00      0.00      0.00         1\n",
      "         1058252588       0.00      0.00      0.00         1\n",
      "         1058264593       0.00      0.00      0.00         1\n",
      "         1058268532       0.00      0.00      0.00         1\n",
      "         1058268925       0.00      0.00      0.00         1\n",
      "         1058269663       0.00      0.00      0.00         1\n",
      "         1058270910       0.00      0.00      0.00         1\n",
      "         1058276705       0.00      0.00      0.00         1\n",
      "         1058287475       0.00      0.00      0.00         1\n",
      "         1058291352       0.00      0.00      0.00         1\n",
      "         1058299548       0.00      0.00      0.00         1\n",
      "         1058312794       0.00      0.00      0.00         1\n",
      "         1058313758       0.00      0.00      0.00         1\n",
      "         1058317205       0.00      0.00      0.00         1\n",
      "         1058331232       0.00      0.00      0.00         1\n",
      "         1058332949       0.00      0.00      0.00         1\n",
      "         1058335874       0.00      0.00      0.00         1\n",
      "         1058346681       0.00      0.00      0.00         1\n",
      "         1058349542       0.00      0.00      0.00         1\n",
      "         1058374360       0.00      0.00      0.00         1\n",
      "         1058379844       0.00      0.00      0.00         1\n",
      "         1058380326       0.00      0.00      0.00         1\n",
      "         1058392212       0.00      0.00      0.00         1\n",
      "         1058405246       0.00      0.00      0.00         1\n",
      "         1058407268       0.00      0.00      0.00         1\n",
      "         1058407383       0.00      0.00      0.00         1\n",
      "         1058408692       0.00      0.00      0.00         1\n",
      "         1058426868       0.00      0.00      0.00         1\n",
      "         1058427098       0.00      0.00      0.00         1\n",
      "         1058428035       0.00      0.00      0.00         1\n",
      "         1058447062       0.00      0.00      0.00         1\n",
      "         1058508291       0.00      0.00      0.00         1\n",
      "         1058517284       0.00      0.00      0.00         1\n",
      "         1058558306       0.00      0.00      0.00         1\n",
      "         1058583378       0.00      0.00      0.00         1\n",
      "         1058588604       0.00      0.00      0.00         1\n",
      "         1058591275       0.00      0.00      0.00         1\n",
      "         1058599808       0.00      0.00      0.00         1\n",
      "         1058600908       0.00      0.00      0.00         1\n",
      "         1058612080       0.00      0.00      0.00         1\n",
      "         1058618562       0.00      0.00      0.00         1\n",
      "         1058618982       0.00      0.00      0.00         1\n",
      "         1058627854       0.00      0.00      0.00         1\n",
      "         1058630702       0.00      0.00      0.00         1\n",
      "         1058643222       0.00      0.00      0.00         1\n",
      "         1058647459       0.00      0.00      0.00         1\n",
      "         1058649229       0.00      0.00      0.00         1\n",
      "         1058657760       0.00      0.00      0.00         1\n",
      "         1058668215       0.00      0.00      0.00         1\n",
      "         1058677792       0.00      0.00      0.00         1\n",
      "         1058681522       0.00      0.00      0.00         1\n",
      "         1058687678       0.00      0.00      0.00         1\n",
      "         1058695266       0.00      0.00      0.00         1\n",
      "         1058705436       0.00      0.00      0.00         1\n",
      "         1058708790       0.00      0.00      0.00         1\n",
      "         1058720906       0.00      0.00      0.00         1\n",
      "         1058732474       0.00      0.00      0.00         1\n",
      "         1058735847       0.00      0.00      0.00         1\n",
      "         1058741020       0.00      0.00      0.00         1\n",
      "         1058741220       0.00      0.00      0.00         1\n",
      "         1058746080       0.00      0.00      0.00         1\n",
      "         1058751164       0.00      0.00      0.00         1\n",
      "         1058774674       0.00      0.00      0.00         1\n",
      "         1058788269       0.00      0.00      0.00         1\n",
      "         1058822316       0.00      0.00      0.00         1\n",
      "         1058834267       0.00      0.00      0.00         1\n",
      "         1058837574       0.00      0.00      0.00         1\n",
      "         1058848611       0.00      0.00      0.00         1\n",
      "         1058854593       0.00      0.00      0.00         1\n",
      "         1058877465       0.00      0.00      0.00         1\n",
      "         1058879130       0.00      0.00      0.00         1\n",
      "         1058882177       0.00      0.00      0.00         1\n",
      "         1058885502       0.00      0.00      0.00         1\n",
      "         1058888826       0.00      0.00      0.00         1\n",
      "         1058900328       0.00      0.00      0.00         1\n",
      "         1058910819       0.00      0.00      0.00         1\n",
      "         1058918540       0.00      0.00      0.00         1\n",
      "         1058930698       0.00      0.00      0.00         1\n",
      "         1058935462       0.00      0.00      0.00         1\n",
      "         1058947058       0.00      0.00      0.00         1\n",
      "         1058974452       0.00      0.00      0.00         1\n",
      "         1058985092       0.00      0.00      0.00         1\n",
      "         1058993261       0.00      0.00      0.00         1\n",
      "         1059007358       0.00      0.00      0.00         1\n",
      "         1059044145       0.00      0.00      0.00         1\n",
      "         1059060194       0.00      0.00      0.00         1\n",
      "         1059061954       0.00      0.00      0.00         1\n",
      "         1059077060       0.00      0.00      0.00         1\n",
      "         1059106182       0.00      0.00      0.00         1\n",
      "         1059113312       0.00      0.00      0.00         1\n",
      "         1059114330       0.00      0.00      0.00         1\n",
      "         1059123300       0.00      0.00      0.00         1\n",
      "         1059128125       0.00      0.00      0.00         1\n",
      "         1059130591       0.00      0.00      0.00         1\n",
      "         1059138582       0.00      0.00      0.00         1\n",
      "         1059145861       0.00      0.00      0.00         1\n",
      "         1059146480       0.00      0.00      0.00         1\n",
      "         1059152493       0.00      0.00      0.00         1\n",
      "         1059153334       0.00      0.00      0.00         1\n",
      "         1059155178       0.00      0.00      0.00         1\n",
      "         1059173201       0.00      0.00      0.00         1\n",
      "         1059177523       0.00      0.00      0.00         1\n",
      "         1059177770       0.00      0.00      0.00         1\n",
      "         1059201457       0.00      0.00      0.00         1\n",
      "         1059207716       0.00      0.00      0.00         1\n",
      "         1059213808       0.00      0.00      0.00         1\n",
      "         1059216994       0.00      0.00      0.00         1\n",
      "         1059231022       0.00      0.00      0.00         1\n",
      "         1059233693       0.00      0.00      0.00         1\n",
      "         1059243538       0.00      0.00      0.00         1\n",
      "         1059296088       0.00      0.00      0.00         1\n",
      "         1059301246       0.00      0.00      0.00         1\n",
      "         1059301752       0.00      0.00      0.00         1\n",
      "         1059311568       0.00      0.00      0.00         1\n",
      "         1059345818       0.00      0.00      0.00         1\n",
      "         1059362951       0.00      0.00      0.00         1\n",
      "         1059366740       0.00      0.00      0.00         1\n",
      "         1059371842       0.00      0.00      0.00         1\n",
      "         1059380744       0.00      0.00      0.00         1\n",
      "         1059393173       0.00      0.00      0.00         1\n",
      "         1059403622       0.00      0.00      0.00         1\n",
      "         1059412708       0.00      0.00      0.00         1\n",
      "         1059415505       0.00      0.00      0.00         1\n",
      "         1059425390       0.00      0.00      0.00         1\n",
      "         1059427501       0.00      0.00      0.00         1\n",
      "         1059472447       0.00      0.00      0.00         1\n",
      "         1059475402       0.00      0.00      0.00         1\n",
      "         1059487224       0.00      0.00      0.00         1\n",
      "         1059489462       0.00      0.00      0.00         1\n",
      "         1059493586       0.00      0.00      0.00         1\n",
      "         1059500108       0.00      0.00      0.00         1\n",
      "         1059500519       0.00      0.00      0.00         1\n",
      "         1059513220       0.00      0.00      0.00         1\n",
      "         1059515530       0.00      0.00      0.00         1\n",
      "         1059521874       0.00      0.00      0.00         1\n",
      "         1059526299       0.00      0.00      0.00         1\n",
      "         1059532955       0.00      0.00      0.00         1\n",
      "         1059536745       0.00      0.00      0.00         1\n",
      "         1059558086       0.00      0.00      0.00         1\n",
      "         1059586711       0.00      0.00      0.00         1\n",
      "         1059586787       0.00      0.00      0.00         1\n",
      "         1059602758       0.00      0.00      0.00         1\n",
      "         1059608650       0.00      0.00      0.00         1\n",
      "         1059608716       0.00      0.00      0.00         1\n",
      "         1059610820       0.00      0.00      0.00         1\n",
      "         1059626412       0.00      0.00      0.00         1\n",
      "         1059628506       0.00      0.00      0.00         1\n",
      "         1059631468       0.00      0.00      0.00         1\n",
      "         1059645210       0.00      0.00      0.00         1\n",
      "         1059647760       0.00      0.00      0.00         1\n",
      "         1059655200       0.00      0.00      0.00         1\n",
      "         1059661581       0.00      0.00      0.00         1\n",
      "         1059667072       0.00      0.00      0.00         1\n",
      "         1059677507       0.00      0.00      0.00         1\n",
      "         1059695408       0.00      0.00      0.00         1\n",
      "         1059695691       0.00      0.00      0.00         1\n",
      "         1059697437       0.00      0.00      0.00         1\n",
      "         1059709637       0.00      0.00      0.00         1\n",
      "         1059713260       0.00      0.00      0.00         1\n",
      "         1059723027       0.00      0.00      0.00         1\n",
      "         1059738623       0.00      0.00      0.00         1\n",
      "         1059741073       0.00      0.00      0.00         1\n",
      "         1059746010       0.00      0.00      0.00         1\n",
      "         1059763965       0.00      0.00      0.00         1\n",
      "         1059789558       0.00      0.00      0.00         1\n",
      "         1059793912       0.00      0.00      0.00         1\n",
      "         1059802996       0.00      0.00      0.00         1\n",
      "         1059805217       0.00      0.00      0.00         1\n",
      "         1059810791       0.00      0.00      0.00         1\n",
      "         1059812004       0.00      0.00      0.00         1\n",
      "         1059835360       0.00      0.00      0.00         1\n",
      "         1059844544       0.00      0.00      0.00         1\n",
      "         1059857419       0.00      0.00      0.00         1\n",
      "         1059868820       0.00      0.00      0.00         1\n",
      "         1059875449       0.00      0.00      0.00         1\n",
      "         1059888963       0.00      0.00      0.00         1\n",
      "         1059912664       0.00      0.00      0.00         1\n",
      "         1059915693       0.00      0.00      0.00         1\n",
      "         1059926094       0.00      0.00      0.00         1\n",
      "         1059928800       0.00      0.00      0.00         1\n",
      "         1059937153       0.00      0.00      0.00         1\n",
      "         1059940955       0.00      0.00      0.00         1\n",
      "         1059942527       0.00      0.00      0.00         1\n",
      "         1059977690       0.00      0.00      0.00         1\n",
      "         1059985195       0.00      0.00      0.00         1\n",
      "         1059996722       0.00      0.00      0.00         1\n",
      "         1059999908       0.00      0.00      0.00         1\n",
      "         1060004970       0.00      0.00      0.00         1\n",
      "         1060008860       0.00      0.00      0.00         1\n",
      "         1060037546       0.00      0.00      0.00         1\n",
      "         1060042205       0.00      0.00      0.00         1\n",
      "         1060054724       0.00      0.00      0.00         1\n",
      "         1060059802       0.00      0.00      0.00         1\n",
      "         1060102639       0.00      0.00      0.00         1\n",
      "         1060108611       0.00      0.00      0.00         1\n",
      "         1060114981       0.00      0.00      0.00         1\n",
      "         1060127610       0.00      0.00      0.00         1\n",
      "         1060138316       0.00      0.00      0.00         1\n",
      "         1060145427       0.00      0.00      0.00         1\n",
      "         1060163732       0.00      0.00      0.00         1\n",
      "         1060167410       0.00      0.00      0.00         1\n",
      "         1060171566       0.00      0.00      0.00         1\n",
      "         1060177386       0.00      0.00      0.00         1\n",
      "         1060195932       0.00      0.00      0.00         1\n",
      "         1060196160       0.00      0.00      0.00         1\n",
      "         1060204716       0.00      0.00      0.00         1\n",
      "         1060209444       0.00      0.00      0.00         1\n",
      "         1060231267       0.00      0.00      0.00         1\n",
      "         1060270832       0.00      0.00      0.00         1\n",
      "         1060271542       0.00      0.00      0.00         1\n",
      "         1060274493       0.00      0.00      0.00         1\n",
      "         1060277398       0.00      0.00      0.00         1\n",
      "         1060300213       0.00      0.00      0.00         1\n",
      "         1060301441       0.00      0.00      0.00         1\n",
      "         1060302238       0.00      0.00      0.00         1\n",
      "         1060305585       0.00      0.00      0.00         1\n",
      "         1060306666       0.00      0.00      0.00         1\n",
      "         1060310752       0.00      0.00      0.00         1\n",
      "         1060336724       0.00      0.00      0.00         1\n",
      "         1060345973       0.00      0.00      0.00         1\n",
      "         1060353087       0.00      0.00      0.00         1\n",
      "         1060357034       0.00      0.00      0.00         1\n",
      "         1060357984       0.00      0.00      0.00         1\n",
      "         1060397828       0.00      0.00      0.00         1\n",
      "         1060400401       0.00      0.00      0.00         1\n",
      "         1060403725       0.00      0.00      0.00         1\n",
      "         1060405266       0.00      0.00      0.00         1\n",
      "         1060462511       0.00      0.00      0.00         1\n",
      "         1060470591       0.00      0.00      0.00         1\n",
      "         1060482907       0.00      0.00      0.00         1\n",
      "         1060503749       0.00      0.00      0.00         1\n",
      "         1060507525       0.00      0.00      0.00         1\n",
      "         1060527759       0.00      0.00      0.00         1\n",
      "         1060532667       0.00      0.00      0.00         1\n",
      "         1060536349       0.00      0.00      0.00         1\n",
      "         1060538557       0.00      0.00      0.00         1\n",
      "         1060549669       0.00      0.00      0.00         1\n",
      "         1060553092       0.00      0.00      0.00         1\n",
      "         1060558180       0.00      0.00      0.00         1\n",
      "         1060568653       0.00      0.00      0.00         1\n",
      "         1060582606       0.00      0.00      0.00         1\n",
      "         1060592351       0.00      0.00      0.00         1\n",
      "         1060618644       0.00      0.00      0.00         1\n",
      "         1060620376       0.00      0.00      0.00         1\n",
      "         1060624181       0.00      0.00      0.00         1\n",
      "         1060631124       0.00      0.00      0.00         1\n",
      "         1060646014       0.00      0.00      0.00         1\n",
      "         1060653824       0.00      0.00      0.00         1\n",
      "         1060660686       0.00      0.00      0.00         1\n",
      "         1060662492       0.00      0.00      0.00         1\n",
      "         1060670932       0.00      0.00      0.00         1\n",
      "         1060682060       0.00      0.00      0.00         1\n",
      "         1060686550       0.00      0.00      0.00         1\n",
      "         1060690535       0.00      0.00      0.00         1\n",
      "         1060700794       0.00      0.00      0.00         1\n",
      "         1060706180       0.00      0.00      0.00         1\n",
      "         1060709646       0.00      0.00      0.00         1\n",
      "         1060727888       0.00      0.00      0.00         1\n",
      "         1060728424       0.00      0.00      0.00         1\n",
      "         1060750324       0.00      0.00      0.00         1\n",
      "         1060751891       0.00      0.00      0.00         1\n",
      "         1060771748       0.00      0.00      0.00         1\n",
      "         1060776750       0.00      0.00      0.00         1\n",
      "         1060801656       0.00      0.00      0.00         1\n",
      "         1060805016       0.00      0.00      0.00         1\n",
      "         1060807874       0.00      0.00      0.00         1\n",
      "         1060829156       0.00      0.00      0.00         1\n",
      "         1060844417       0.00      0.00      0.00         1\n",
      "         1060852071       0.00      0.00      0.00         1\n",
      "         1060856433       0.00      0.00      0.00         1\n",
      "         1060856867       0.00      0.00      0.00         1\n",
      "         1060858590       0.00      0.00      0.00         1\n",
      "         1060870461       0.00      0.00      0.00         1\n",
      "         1060878014       0.00      0.00      0.00         1\n",
      "         1060908262       0.00      0.00      0.00         1\n",
      "         1060916260       0.00      0.00      0.00         1\n",
      "         1060931670       0.00      0.00      0.00         1\n",
      "         1060932792       0.00      0.00      0.00         1\n",
      "         1060940300       0.00      0.00      0.00         1\n",
      "         1060961272       0.00      0.00      0.00         1\n",
      "         1060986856       0.00      0.00      0.00         1\n",
      "         1060990998       0.00      0.00      0.00         1\n",
      "         1061067424       0.00      0.00      0.00         1\n",
      "         1061074178       0.00      0.00      0.00         1\n",
      "         1061078553       0.00      0.00      0.00         1\n",
      "         1061082981       0.00      0.00      0.00         1\n",
      "         1061084990       0.00      0.00      0.00         1\n",
      "         1061106955       0.00      0.00      0.00         1\n",
      "         1061122523       0.00      0.00      0.00         1\n",
      "         1061122690       0.00      0.00      0.00         1\n",
      "         1061125120       0.00      0.00      0.00         1\n",
      "         1061155444       0.00      0.00      0.00         1\n",
      "         1061171526       0.00      0.00      0.00         1\n",
      "         1061181545       0.00      0.00      0.00         1\n",
      "         1061211278       0.00      0.00      0.00         1\n",
      "         1061213598       0.00      0.00      0.00         1\n",
      "         1061221115       0.00      0.00      0.00         1\n",
      "         1061230834       0.00      0.00      0.00         1\n",
      "         1061237527       0.00      0.00      0.00         1\n",
      "         1061239507       0.00      0.00      0.00         1\n",
      "         1061273156       0.00      0.00      0.00         1\n",
      "         1061274244       0.00      0.00      0.00         1\n",
      "         1061284976       0.00      0.00      0.00         1\n",
      "         1061285756       0.00      0.00      0.00         1\n",
      "         1061289735       0.00      0.00      0.00         1\n",
      "         1061307280       0.00      0.00      0.00         1\n",
      "         1061309303       0.00      0.00      0.00         1\n",
      "         1061311254       0.00      0.00      0.00         1\n",
      "         1061318900       0.00      0.00      0.00         1\n",
      "         1061327983       0.00      0.00      0.00         1\n",
      "         1061328569       0.00      0.00      0.00         1\n",
      "         1061332320       0.00      0.00      0.00         1\n",
      "         1061341042       0.00      0.00      0.00         1\n",
      "         1061345493       0.00      0.00      0.00         1\n",
      "         1061361026       0.00      0.00      0.00         1\n",
      "         1061378404       0.00      0.00      0.00         1\n",
      "         1061391090       0.00      0.00      0.00         1\n",
      "         1061399224       0.00      0.00      0.00         1\n",
      "         1061409047       0.00      0.00      0.00         1\n",
      "         1061431999       0.00      0.00      0.00         1\n",
      "         1061434270       0.00      0.00      0.00         1\n",
      "         1061435544       0.00      0.00      0.00         1\n",
      "         1061438374       0.00      0.00      0.00         1\n",
      "         1061443060       0.00      0.00      0.00         1\n",
      "         1061486762       0.00      0.00      0.00         1\n",
      "         1061503564       0.00      0.00      0.00         1\n",
      "         1061532592       0.00      0.00      0.00         1\n",
      "         1061537592       0.00      0.00      0.00         1\n",
      "         1061591424       0.00      0.00      0.00         1\n",
      "         1061593543       0.00      0.00      0.00         1\n",
      "         1061631290       0.00      0.00      0.00         1\n",
      "         1061635160       0.00      0.00      0.00         1\n",
      "         1061637826       0.00      0.00      0.00         1\n",
      "         1061647668       0.00      0.00      0.00         1\n",
      "         1061658857       0.00      0.00      0.00         1\n",
      "         1061672150       0.00      0.00      0.00         1\n",
      "         1061678366       0.00      0.00      0.00         1\n",
      "         1061679698       0.00      0.00      0.00         1\n",
      "         1061684726       0.00      0.00      0.00         1\n",
      "         1061717622       0.00      0.00      0.00         1\n",
      "         1061723162       0.00      0.00      0.00         1\n",
      "         1061725268       0.00      0.00      0.00         1\n",
      "         1061732757       0.00      0.00      0.00         1\n",
      "         1061755269       0.00      0.00      0.00         1\n",
      "         1061769122       0.00      0.00      0.00         1\n",
      "         1061790913       0.00      0.00      0.00         1\n",
      "         1061806843       0.00      0.00      0.00         1\n",
      "         1061807485       0.00      0.00      0.00         1\n",
      "         1061814692       0.00      0.00      0.00         1\n",
      "         1061827985       0.00      0.00      0.00         1\n",
      "         1061835522       0.00      0.00      0.00         1\n",
      "         1061836782       0.00      0.00      0.00         1\n",
      "         1061840288       0.00      0.00      0.00         1\n",
      "         1061842697       0.00      0.00      0.00         1\n",
      "         1061862836       0.00      0.00      0.00         1\n",
      "         1061895278       0.00      0.00      0.00         1\n",
      "         1061910318       0.00      0.00      0.00         1\n",
      "         1061920386       0.00      0.00      0.00         1\n",
      "         1061980335       0.00      0.00      0.00         1\n",
      "         1061992836       0.00      0.00      0.00         1\n",
      "         1062007206       0.00      0.00      0.00         1\n",
      "         1062013448       0.00      0.00      0.00         1\n",
      "         1062021660       0.00      0.00      0.00         1\n",
      "         1062063834       0.00      0.00      0.00         1\n",
      "         1062074961       0.00      0.00      0.00         1\n",
      "         1062097370       0.00      0.00      0.00         1\n",
      "         1062102558       0.00      0.00      0.00         1\n",
      "         1062107996       0.00      0.00      0.00         1\n",
      "         1062108446       0.00      0.00      0.00         1\n",
      "         1062110143       0.00      0.00      0.00         1\n",
      "         1062110256       0.00      0.00      0.00         1\n",
      "         1062113918       0.00      0.00      0.00         1\n",
      "         1062114080       0.00      0.00      0.00         1\n",
      "         1062136429       0.00      0.00      0.00         1\n",
      "         1062139239       0.00      0.00      0.00         1\n",
      "         1062143261       0.00      0.00      0.00         1\n",
      "         1062158623       0.00      0.00      0.00         1\n",
      "         1062166597       0.00      0.00      0.00         1\n",
      "         1062181756       0.00      0.00      0.00         1\n",
      "         1062205338       0.00      0.00      0.00         1\n",
      "         1062211308       0.00      0.00      0.00         1\n",
      "         1062238197       0.00      0.00      0.00         1\n",
      "         1062244950       0.00      0.00      0.00         1\n",
      "         1062264496       0.00      0.00      0.00         1\n",
      "         1062329386       0.00      0.00      0.00         1\n",
      "         1062352112       0.00      0.00      0.00         1\n",
      "         1062380320       0.00      0.00      0.00         1\n",
      "         1062382502       0.00      0.00      0.00         1\n",
      "         1062415630       0.00      0.00      0.00         1\n",
      "         1062415906       0.00      0.00      0.00         1\n",
      "         1062421256       0.00      0.00      0.00         1\n",
      "         1062438004       0.00      0.00      0.00         1\n",
      "         1062449306       0.00      0.00      0.00         1\n",
      "         1062453674       0.00      0.00      0.00         1\n",
      "         1062485747       0.00      0.00      0.00         1\n",
      "         1062522910       0.00      0.00      0.00         1\n",
      "         1062554966       0.00      0.00      0.00         1\n",
      "         1062570538       0.00      0.00      0.00         1\n",
      "         1062581306       0.00      0.00      0.00         1\n",
      "         1062581642       0.00      0.00      0.00         1\n",
      "         1062591548       0.00      0.00      0.00         1\n",
      "         1062618366       0.00      0.00      0.00         1\n",
      "         1062635526       0.00      0.00      0.00         1\n",
      "         1062643248       0.00      0.00      0.00         1\n",
      "         1062661078       0.00      0.00      0.00         1\n",
      "         1062690180       0.00      0.00      0.00         1\n",
      "         1062697332       0.00      0.00      0.00         1\n",
      "         1062700849       0.00      0.00      0.00         1\n",
      "         1062732324       0.00      0.00      0.00         1\n",
      "         1062742673       0.00      0.00      0.00         1\n",
      "         1062747605       0.00      0.00      0.00         1\n",
      "         1062772474       0.00      0.00      0.00         1\n",
      "         1062788466       0.00      0.00      0.00         1\n",
      "         1062789191       0.00      0.00      0.00         1\n",
      "         1062811399       0.00      0.00      0.00         1\n",
      "         1062814390       0.00      0.00      0.00         1\n",
      "         1062831051       0.00      0.00      0.00         1\n",
      "         1062837287       0.00      0.00      0.00         1\n",
      "         1062839984       0.00      0.00      0.00         1\n",
      "         1062849859       0.00      0.00      0.00         1\n",
      "         1062865802       0.00      0.00      0.00         1\n",
      "         1062877943       0.00      0.00      0.00         1\n",
      "         1062909478       0.00      0.00      0.00         1\n",
      "         1062944142       0.00      0.00      0.00         1\n",
      "         1062956697       0.00      0.00      0.00         1\n",
      "         1062968207       0.00      0.00      0.00         1\n",
      "         1062974967       0.00      0.00      0.00         1\n",
      "         1062993937       0.00      0.00      0.00         1\n",
      "         1062997338       0.00      0.00      0.00         1\n",
      "         1063009772       0.00      0.00      0.00         1\n",
      "         1063013284       0.00      0.00      0.00         1\n",
      "         1063018946       0.00      0.00      0.00         1\n",
      "         1063025035       0.00      0.00      0.00         1\n",
      "         1063027148       0.00      0.00      0.00         1\n",
      "         1063034091       0.00      0.00      0.00         1\n",
      "         1063050425       0.00      0.00      0.00         1\n",
      "         1063052670       0.00      0.00      0.00         1\n",
      "         1063057596       0.00      0.00      0.00         1\n",
      "         1063076951       0.00      0.00      0.00         1\n",
      "         1063123826       0.00      0.00      0.00         1\n",
      "         1063124170       0.00      0.00      0.00         1\n",
      "         1063157512       0.00      0.00      0.00         1\n",
      "         1063194972       0.00      0.00      0.00         1\n",
      "         1063216527       0.00      0.00      0.00         1\n",
      "         1063253802       0.00      0.00      0.00         1\n",
      "         1063255528       0.00      0.00      0.00         1\n",
      "         1063268329       0.00      0.00      0.00         1\n",
      "         1063279085       0.00      0.00      0.00         1\n",
      "         1063300885       0.00      0.00      0.00         1\n",
      "         1063318214       0.00      0.00      0.00         1\n",
      "         1063385083       0.00      0.00      0.00         1\n",
      "         1063386012       0.00      0.00      0.00         1\n",
      "         1063388870       0.00      0.00      0.00         1\n",
      "         1063397816       0.00      0.00      0.00         1\n",
      "         1063399289       0.00      0.00      0.00         1\n",
      "         1063400517       0.00      0.00      0.00         1\n",
      "         1063415688       0.00      0.00      0.00         1\n",
      "         1063416652       0.00      0.00      0.00         1\n",
      "         1063420012       0.00      0.00      0.00         1\n",
      "         1063434166       0.00      0.00      0.00         1\n",
      "         1063437492       0.00      0.00      0.00         1\n",
      "         1063461814       0.00      0.00      0.00         1\n",
      "         1063476926       0.00      0.00      0.00         1\n",
      "         1063527279       0.00      0.00      0.00         1\n",
      "         1063569924       0.00      0.00      0.00         1\n",
      "         1063574368       0.00      0.00      0.00         1\n",
      "         1063574474       0.00      0.00      0.00         1\n",
      "         1063589314       0.00      0.00      0.00         1\n",
      "         1063607275       0.00      0.00      0.00         1\n",
      "         1063611910       0.00      0.00      0.00         1\n",
      "         1063618378       0.00      0.00      0.00         1\n",
      "         1063624404       0.00      0.00      0.00         1\n",
      "         1063653744       0.00      0.00      0.00         1\n",
      "         1063685988       0.00      0.00      0.00         1\n",
      "         1063694311       0.00      0.00      0.00         1\n",
      "         1063704658       0.00      0.00      0.00         1\n",
      "         1063709412       0.00      0.00      0.00         1\n",
      "         1063725969       0.00      0.00      0.00         1\n",
      "         1063737849       0.00      0.00      0.00         1\n",
      "         1063771878       0.00      0.00      0.00         1\n",
      "         1063775308       0.00      0.00      0.00         1\n",
      "         1063779231       0.00      0.00      0.00         1\n",
      "         1063784665       0.00      0.00      0.00         1\n",
      "         1063817788       0.00      0.00      0.00         1\n",
      "         1063843987       0.00      0.00      0.00         1\n",
      "         1063860430       0.00      0.00      0.00         1\n",
      "         1063872634       0.00      0.00      0.00         1\n",
      "         1063893747       0.00      0.00      0.00         1\n",
      "         1063914776       0.00      0.00      0.00         1\n",
      "         1063924124       0.00      0.00      0.00         1\n",
      "         1063933210       0.00      0.00      0.00         1\n",
      "         1063937963       0.00      0.00      0.00         1\n",
      "         1063938754       0.00      0.00      0.00         1\n",
      "         1063959416       0.00      0.00      0.00         1\n",
      "         1063972906       0.00      0.00      0.00         1\n",
      "         1063991981       0.00      0.00      0.00         1\n",
      "         1064015311       0.00      0.00      0.00         1\n",
      "         1064049160       0.00      0.00      0.00         1\n",
      "         1064057189       0.00      0.00      0.00         1\n",
      "         1064059522       0.00      0.00      0.00         1\n",
      "         1064068382       0.00      0.00      0.00         1\n",
      "         1064089885       0.00      0.00      0.00         1\n",
      "         1064114747       0.00      0.00      0.00         1\n",
      "         1064125145       0.00      0.00      0.00         1\n",
      "         1064129665       0.00      0.00      0.00         1\n",
      "         1064143093       0.00      0.00      0.00         1\n",
      "         1064156322       0.00      0.00      0.00         1\n",
      "         1064162238       0.00      0.00      0.00         1\n",
      "         1064169183       0.00      0.00      0.00         1\n",
      "         1064177914       0.00      0.00      0.00         1\n",
      "         1064182916       0.00      0.00      0.00         1\n",
      "         1064188534       0.00      0.00      0.00         1\n",
      "         1064190518       0.00      0.00      0.00         1\n",
      "         1064208049       0.00      0.00      0.00         1\n",
      "         1064213497       0.00      0.00      0.00         1\n",
      "         1064232409       0.00      0.00      0.00         1\n",
      "         1064245270       0.00      0.00      0.00         1\n",
      "         1064245335       0.00      0.00      0.00         1\n",
      "         1064247968       0.00      0.00      0.00         1\n",
      "         1064253820       0.00      0.00      0.00         1\n",
      "         1064254054       0.00      0.00      0.00         1\n",
      "         1064265638       0.00      0.00      0.00         1\n",
      "         1064271696       0.00      0.00      0.00         1\n",
      "         1064320400       0.00      0.00      0.00         1\n",
      "         1064328934       0.00      0.00      0.00         1\n",
      "         1064335050       0.00      0.00      0.00         1\n",
      "         1064383330       0.00      0.00      0.00         1\n",
      "         1064396632       0.00      0.00      0.00         1\n",
      "         1064423902       0.00      0.00      0.00         1\n",
      "         1064435750       0.00      0.00      0.00         1\n",
      "         1064440076       0.00      0.00      0.00         1\n",
      "         1064448709       0.00      0.00      0.00         1\n",
      "         1064478197       0.00      0.00      0.00         1\n",
      "         1064493726       0.00      0.00      0.00         1\n",
      "         1064507608       0.00      0.00      0.00         1\n",
      "         1064512442       0.00      0.00      0.00         1\n",
      "         1064539382       0.00      0.00      0.00         1\n",
      "         1064546060       0.00      0.00      0.00         1\n",
      "         1064553730       0.00      0.00      0.00         1\n",
      "         1064572440       0.00      0.00      0.00         1\n",
      "         1064572505       0.00      0.00      0.00         1\n",
      "         1064604648       0.00      0.00      0.00         1\n",
      "         1064610696       0.00      0.00      0.00         1\n",
      "         1064613693       0.00      0.00      0.00         1\n",
      "         1064643256       0.00      0.00      0.00         1\n",
      "         1064648206       0.00      0.00      0.00         1\n",
      "         1064660996       0.00      0.00      0.00         1\n",
      "         1064692772       0.00      0.00      0.00         1\n",
      "         1064701816       0.00      0.00      0.00         1\n",
      "         1064758437       0.00      0.00      0.00         1\n",
      "         1064793356       0.00      0.00      0.00         1\n",
      "         1064793814       0.00      0.00      0.00         1\n",
      "         1064818266       0.00      0.00      0.00         1\n",
      "         1064829152       0.00      0.00      0.00         1\n",
      "         1064837932       0.00      0.00      0.00         1\n",
      "         1064844860       0.00      0.00      0.00         1\n",
      "         1064846420       0.00      0.00      0.00         1\n",
      "         1064891422       0.00      0.00      0.00         1\n",
      "         1064907232       0.00      0.00      0.00         1\n",
      "         1064910255       0.00      0.00      0.00         1\n",
      "         1064967595       0.00      0.00      0.00         1\n",
      "         1065002333       0.00      0.00      0.00         1\n",
      "         1065027336       0.00      0.00      0.00         1\n",
      "         1065027821       0.00      0.00      0.00         1\n",
      "         1065042094       0.00      0.00      0.00         1\n",
      "         1065042554       0.00      0.00      0.00         1\n",
      "         1065050582       0.00      0.00      0.00         1\n",
      "         1065052862       0.00      0.00      0.00         1\n",
      "         1065094341       0.00      0.00      0.00         1\n",
      "         1065097795       0.00      0.00      0.00         1\n",
      "         1065101260       0.00      0.00      0.00         1\n",
      "         1065118211       0.00      0.00      0.00         1\n",
      "         1065179809       0.00      0.00      0.00         1\n",
      "         1065183437       0.00      0.00      0.00         1\n",
      "         1065188756       0.00      0.00      0.00         1\n",
      "         1065201202       0.00      0.00      0.00         1\n",
      "         1065212312       0.00      0.00      0.00         1\n",
      "         1065229206       0.00      0.00      0.00         1\n",
      "         1065281064       0.00      0.00      0.00         1\n",
      "         1065297660       0.00      0.00      0.00         1\n",
      "         1065313512       0.00      0.00      0.00         1\n",
      "         1065314583       0.00      0.00      0.00         1\n",
      "         1065315955       0.00      0.00      0.00         1\n",
      "         1065346566       0.00      0.00      0.00         1\n",
      "         1065361804       0.00      0.00      0.00         1\n",
      "         1065366222       0.00      0.00      0.00         1\n",
      "         1065373362       0.00      0.00      0.00         1\n",
      "         1065375899       0.00      0.00      0.00         1\n",
      "         1065378613       0.00      0.00      0.00         1\n",
      "         1065385478       0.00      0.00      0.00         1\n",
      "         1065387275       0.00      0.00      0.00         1\n",
      "         1065387534       0.00      0.00      0.00         1\n",
      "         1065389789       0.00      0.00      0.00         1\n",
      "         1065406630       0.00      0.00      0.00         1\n",
      "         1065416837       0.00      0.00      0.00         1\n",
      "         1065426301       0.00      0.00      0.00         1\n",
      "         1065439370       0.00      0.00      0.00         1\n",
      "         1065446694       0.00      0.00      0.00         1\n",
      "         1065488372       0.00      0.00      0.00         1\n",
      "         1065490845       0.00      0.00      0.00         1\n",
      "         1065491641       0.00      0.00      0.00         1\n",
      "         1065495828       0.00      0.00      0.00         1\n",
      "         1065499065       0.00      0.00      0.00         1\n",
      "         1065505424       0.00      0.00      0.00         1\n",
      "         1065515448       0.00      0.00      0.00         1\n",
      "         1065537554       0.00      0.00      0.00         1\n",
      "         1065542552       0.00      0.00      0.00         1\n",
      "         1065544830       0.00      0.00      0.00         1\n",
      "         1065548613       0.00      0.00      0.00         1\n",
      "         1065551754       0.00      0.00      0.00         1\n",
      "         1065562490       0.00      0.00      0.00         1\n",
      "         1065567951       0.00      0.00      0.00         1\n",
      "         1065585055       0.00      0.00      0.00         1\n",
      "         1065597451       0.00      0.00      0.00         1\n",
      "         1065607358       0.00      0.00      0.00         1\n",
      "         1065613654       0.00      0.00      0.00         1\n",
      "         1065656394       0.00      0.00      0.00         1\n",
      "         1065661354       0.00      0.00      0.00         1\n",
      "         1065667981       0.00      0.00      0.00         1\n",
      "         1065679543       0.00      0.00      0.00         1\n",
      "         1065696880       0.00      0.00      0.00         1\n",
      "         1065701370       0.00      0.00      0.00         1\n",
      "         1065703310       0.00      0.00      0.00         1\n",
      "         1065713653       0.00      0.00      0.00         1\n",
      "         1065714544       0.00      0.00      0.00         1\n",
      "         1065728806       0.00      0.00      0.00         1\n",
      "         1065730995       0.00      0.00      0.00         1\n",
      "         1065750738       0.00      0.00      0.00         1\n",
      "         1065753244       0.00      0.00      0.00         1\n",
      "         1065769651       0.00      0.00      0.00         1\n",
      "         1065772497       0.00      0.00      0.00         1\n",
      "         1065777072       0.00      0.00      0.00         1\n",
      "         1065777638       0.00      0.00      0.00         1\n",
      "         1065781959       0.00      0.00      0.00         1\n",
      "         1065782705       0.00      0.00      0.00         1\n",
      "         1065789750       0.00      0.00      0.00         1\n",
      "         1065791918       0.00      0.00      0.00         1\n",
      "         1065841045       0.00      0.00      0.00         1\n",
      "         1065852007       0.00      0.00      0.00         1\n",
      "         1065870734       0.00      0.00      0.00         1\n",
      "         1065887470       0.00      0.00      0.00         1\n",
      "         1065906408       0.00      0.00      0.00         1\n",
      "         1065918503       0.00      0.00      0.00         1\n",
      "         1065921372       0.00      0.00      0.00         1\n",
      "         1065938998       0.00      0.00      0.00         1\n",
      "         1065942356       0.00      0.00      0.00         1\n",
      "         1065957761       0.00      0.00      0.00         1\n",
      "         1065963952       0.00      0.00      0.00         1\n",
      "         1065977872       0.00      0.00      0.00         1\n",
      "         1066006566       0.00      0.00      0.00         1\n",
      "         1066011349       0.00      0.00      0.00         1\n",
      "         1066018078       0.00      0.00      0.00         1\n",
      "         1066019253       0.00      0.00      0.00         1\n",
      "         1066020656       0.00      0.00      0.00         1\n",
      "         1066025701       0.00      0.00      0.00         1\n",
      "         1066028358       0.00      0.00      0.00         1\n",
      "         1066030028       0.00      0.00      0.00         1\n",
      "         1066052030       0.00      0.00      0.00         1\n",
      "         1066054298       0.00      0.00      0.00         1\n",
      "         1066063241       0.00      0.00      0.00         1\n",
      "         1066064454       0.00      0.00      0.00         1\n",
      "         1066084311       0.00      0.00      0.00         1\n",
      "         1066098923       0.00      0.00      0.00         1\n",
      "         1066101451       0.00      0.00      0.00         1\n",
      "         1066105046       0.00      0.00      0.00         1\n",
      "         1066105254       0.00      0.00      0.00         1\n",
      "         1066114312       0.00      0.00      0.00         1\n",
      "         1066116914       0.00      0.00      0.00         1\n",
      "         1066127292       0.00      0.00      0.00         1\n",
      "         1066130321       0.00      0.00      0.00         1\n",
      "         1066145530       0.00      0.00      0.00         1\n",
      "         1066163079       0.00      0.00      0.00         1\n",
      "         1066170844       0.00      0.00      0.00         1\n",
      "         1066197888       0.00      0.00      0.00         1\n",
      "         1066199516       0.00      0.00      0.00         1\n",
      "         1066202662       0.00      0.00      0.00         1\n",
      "         1066215754       0.00      0.00      0.00         1\n",
      "         1066216492       0.00      0.00      0.00         1\n",
      "         1066239806       0.00      0.00      0.00         1\n",
      "         1066254964       0.00      0.00      0.00         1\n",
      "         1066256636       0.00      0.00      0.00         1\n",
      "         1066273411       0.00      0.00      0.00         1\n",
      "         1066274178       0.00      0.00      0.00         1\n",
      "         1066291432       0.00      0.00      0.00         1\n",
      "         1066305794       0.00      0.00      0.00         1\n",
      "         1066308376       0.00      0.00      0.00         1\n",
      "         1066323068       0.00      0.00      0.00         1\n",
      "         1066323818       0.00      0.00      0.00         1\n",
      "         1066330077       0.00      0.00      0.00         1\n",
      "         1066330883       0.00      0.00      0.00         1\n",
      "         1066332575       0.00      0.00      0.00         1\n",
      "         1066345488       0.00      0.00      0.00         1\n",
      "         1066399355       0.00      0.00      0.00         1\n",
      "         1066409290       0.00      0.00      0.00         1\n",
      "         1066413056       0.00      0.00      0.00         1\n",
      "         1066421225       0.00      0.00      0.00         1\n",
      "         1066425912       0.00      0.00      0.00         1\n",
      "         1066429132       0.00      0.00      0.00         1\n",
      "         1066436030       0.00      0.00      0.00         1\n",
      "         1066439304       0.00      0.00      0.00         1\n",
      "         1066441980       0.00      0.00      0.00         1\n",
      "         1066463828       0.00      0.00      0.00         1\n",
      "         1066469222       0.00      0.00      0.00         1\n",
      "         1066471476       0.00      0.00      0.00         1\n",
      "         1066492928       0.00      0.00      0.00         1\n",
      "         1066506003       0.00      0.00      0.00         1\n",
      "         1066525118       0.00      0.00      0.00         1\n",
      "         1066533536       0.00      0.00      0.00         1\n",
      "         1066565735       0.00      0.00      0.00         1\n",
      "         1066573998       0.00      0.00      0.00         1\n",
      "         1066605877       0.00      0.00      0.00         1\n",
      "         1066609482       0.00      0.00      0.00         1\n",
      "         1066613996       0.00      0.00      0.00         1\n",
      "         1066616474       0.00      0.00      0.00         1\n",
      "         1066626692       0.00      0.00      0.00         1\n",
      "         1066632894       0.00      0.00      0.00         1\n",
      "         1066643375       0.00      0.00      0.00         1\n",
      "         1066654227       0.00      0.00      0.00         1\n",
      "         1066660742       0.00      0.00      0.00         1\n",
      "         1066661883       0.00      0.00      0.00         1\n",
      "         1066661980       0.00      0.00      0.00         1\n",
      "         1066686510       0.00      0.00      0.00         1\n",
      "         1066695011       0.00      0.00      0.00         1\n",
      "         1066707276       0.00      0.00      0.00         1\n",
      "         1066707516       0.00      0.00      0.00         1\n",
      "         1066716388       0.00      0.00      0.00         1\n",
      "         1066730581       0.00      0.00      0.00         1\n",
      "         1066731612       0.00      0.00      0.00         1\n",
      "         1066735120       0.00      0.00      0.00         1\n",
      "         1066745132       0.00      0.00      0.00         1\n",
      "         1066748208       0.00      0.00      0.00         1\n",
      "         1066751982       0.00      0.00      0.00         1\n",
      "         1066753311       0.00      0.00      0.00         1\n",
      "         1066818200       0.00      0.00      0.00         1\n",
      "         1066837656       0.00      0.00      0.00         1\n",
      "         1066838570       0.00      0.00      0.00         1\n",
      "         1066848000       0.00      0.00      0.00         1\n",
      "         1066909448       0.00      0.00      0.00         1\n",
      "         1066917956       0.00      0.00      0.00         1\n",
      "         1066921820       0.00      0.00      0.00         1\n",
      "         1066921944       0.00      0.00      0.00         1\n",
      "         1066921963       0.00      0.00      0.00         1\n",
      "         1066944715       0.00      0.00      0.00         1\n",
      "         1066963084       0.00      0.00      0.00         1\n",
      "         1066977507       0.00      0.00      0.00         1\n",
      "         1066977584       0.00      0.00      0.00         1\n",
      "         1066984476       0.00      0.00      0.00         1\n",
      "         1066988203       0.00      0.00      0.00         1\n",
      "         1067008802       0.00      0.00      0.00         1\n",
      "         1067036580       0.00      0.00      0.00         1\n",
      "         1067043635       0.00      0.00      0.00         1\n",
      "         1067046920       0.00      0.00      0.00         1\n",
      "         1067052024       0.00      0.00      0.00         1\n",
      "         1067112376       0.00      0.00      0.00         1\n",
      "         1067115541       0.00      0.00      0.00         1\n",
      "         1067123684       0.00      0.00      0.00         1\n",
      "         1067146619       0.00      0.00      0.00         1\n",
      "         1067178644       0.00      0.00      0.00         1\n",
      "         1067183594       0.00      0.00      0.00         1\n",
      "         1067196179       0.00      0.00      0.00         1\n",
      "         1067198313       0.00      0.00      0.00         1\n",
      "         1067198801       0.00      0.00      0.00         1\n",
      "         1067229665       0.00      0.00      0.00         1\n",
      "         1067232902       0.00      0.00      0.00         1\n",
      "         1067248179       0.00      0.00      0.00         1\n",
      "         1067260696       0.00      0.00      0.00         1\n",
      "         1067270766       0.00      0.00      0.00         1\n",
      "         1067309544       0.00      0.00      0.00         1\n",
      "         1067311124       0.00      0.00      0.00         1\n",
      "         1067313223       0.00      0.00      0.00         1\n",
      "         1067319705       0.00      0.00      0.00         1\n",
      "         1067330859       0.00      0.00      0.00         1\n",
      "         1067349988       0.00      0.00      0.00         1\n",
      "         1067412945       0.00      0.00      0.00         1\n",
      "         1067413980       0.00      0.00      0.00         1\n",
      "         1067423804       0.00      0.00      0.00         1\n",
      "         1067471485       0.00      0.00      0.00         1\n",
      "         1067476434       0.00      0.00      0.00         1\n",
      "         1067486379       0.00      0.00      0.00         1\n",
      "         1067494796       0.00      0.00      0.00         1\n",
      "         1067554502       0.00      0.00      0.00         1\n",
      "         1067555804       0.00      0.00      0.00         1\n",
      "         1067566127       0.00      0.00      0.00         1\n",
      "         1067576905       0.00      0.00      0.00         1\n",
      "         1067608317       0.00      0.00      0.00         1\n",
      "         1067614039       0.00      0.00      0.00         1\n",
      "         1067620547       0.00      0.00      0.00         1\n",
      "         1067628059       0.00      0.00      0.00         1\n",
      "         1067650372       0.00      0.00      0.00         1\n",
      "         1067711376       0.00      0.00      0.00         1\n",
      "         1067715916       0.00      0.00      0.00         1\n",
      "         1067717730       0.00      0.00      0.00         1\n",
      "         1067724953       0.00      0.00      0.00         1\n",
      "         1067725048       0.00      0.00      0.00         1\n",
      "         1067734864       0.00      0.00      0.00         1\n",
      "         1067741376       0.00      0.00      0.00         1\n",
      "         1067756331       0.00      0.00      0.00         1\n",
      "         1067777118       0.00      0.00      0.00         1\n",
      "         1067806536       0.00      0.00      0.00         1\n",
      "         1067859047       0.00      0.00      0.00         1\n",
      "         1067873342       0.00      0.00      0.00         1\n",
      "         1067900266       0.00      0.00      0.00         1\n",
      "         1067917937       0.00      0.00      0.00         1\n",
      "         1067919775       0.00      0.00      0.00         1\n",
      "         1067928336       0.00      0.00      0.00         1\n",
      "         1067932103       0.00      0.00      0.00         1\n",
      "         1067961848       0.00      0.00      0.00         1\n",
      "         1068020294       0.00      0.00      0.00         1\n",
      "         1068031027       0.00      0.00      0.00         1\n",
      "         1068057190       0.00      0.00      0.00         1\n",
      "         1068114470       0.00      0.00      0.00         1\n",
      "         1068125848       0.00      0.00      0.00         1\n",
      "         1068160706       0.00      0.00      0.00         1\n",
      "         1068168786       0.00      0.00      0.00         1\n",
      "         1068171131       0.00      0.00      0.00         1\n",
      "         1068185540       0.00      0.00      0.00         1\n",
      "         1068188866       0.00      0.00      0.00         1\n",
      "         1068213322       0.00      0.00      0.00         1\n",
      "         1068219152       0.00      0.00      0.00         1\n",
      "         1068259570       0.00      0.00      0.00         1\n",
      "         1068260496       0.00      0.00      0.00         1\n",
      "         1068272652       0.00      0.00      0.00         1\n",
      "         1068281976       0.00      0.00      0.00         1\n",
      "         1068289613       0.00      0.00      0.00         1\n",
      "         1068318821       0.00      0.00      0.00         1\n",
      "         1068323566       0.00      0.00      0.00         1\n",
      "         1068351911       0.00      0.00      0.00         1\n",
      "         1068418796       0.00      0.00      0.00         1\n",
      "         1068429319       0.00      0.00      0.00         1\n",
      "         1068439966       0.00      0.00      0.00         1\n",
      "         1068440700       0.00      0.00      0.00         1\n",
      "         1068441574       0.00      0.00      0.00         1\n",
      "         1068444445       0.00      0.00      0.00         1\n",
      "         1068492082       0.00      0.00      0.00         1\n",
      "         1068507599       0.00      0.00      0.00         1\n",
      "         1068538483       0.00      0.00      0.00         1\n",
      "         1068559987       0.00      0.00      0.00         1\n",
      "         1068575097       0.00      0.00      0.00         1\n",
      "         1068633826       0.00      0.00      0.00         1\n",
      "         1068647116       0.00      0.00      0.00         1\n",
      "         1068721192       0.00      0.00      0.00         1\n",
      "         1068788148       0.00      0.00      0.00         1\n",
      "         1068791494       0.00      0.00      0.00         1\n",
      "         1068797122       0.00      0.00      0.00         1\n",
      "         1068854708       0.00      0.00      0.00         1\n",
      "         1068876110       0.00      0.00      0.00         1\n",
      "         1068882869       0.00      0.00      0.00         1\n",
      "         1068967117       0.00      0.00      0.00         1\n",
      "         1068968256       0.00      0.00      0.00         1\n",
      "         1069013165       0.00      0.00      0.00         1\n",
      "         1069026070       0.00      0.00      0.00         1\n",
      "         1069041541       0.00      0.00      0.00         1\n",
      "         1069063432       0.00      0.00      0.00         1\n",
      "         1069075774       0.00      0.00      0.00         1\n",
      "         1069079362       0.00      0.00      0.00         1\n",
      "         1069098026       0.00      0.00      0.00         1\n",
      "         1069100186       0.00      0.00      0.00         1\n",
      "         1069104689       0.00      0.00      0.00         1\n",
      "         1069189765       0.00      0.00      0.00         1\n",
      "         1069213448       0.00      0.00      0.00         1\n",
      "         1069233898       0.00      0.00      0.00         1\n",
      "         1069281531       0.00      0.00      0.00         1\n",
      "         1069293907       0.00      0.00      0.00         1\n",
      "         1069316460       0.00      0.00      0.00         1\n",
      "         1069340297       0.00      0.00      0.00         1\n",
      "         1069382858       0.00      0.00      0.00         1\n",
      "         1069385790       0.00      0.00      0.00         1\n",
      "         1069442804       0.00      0.00      0.00         1\n",
      "         1069442872       0.00      0.00      0.00         1\n",
      "         1069444870       0.00      0.00      0.00         1\n",
      "         1069450110       0.00      0.00      0.00         1\n",
      "         1069453120       0.00      0.00      0.00         1\n",
      "         1069459476       0.00      0.00      0.00         1\n",
      "         1069578949       0.00      0.00      0.00         1\n",
      "         1069609619       0.00      0.00      0.00         1\n",
      "         1069622950       0.00      0.00      0.00         1\n",
      "         1069674962       0.00      0.00      0.00         1\n",
      "         1069689543       0.00      0.00      0.00         1\n",
      "         1069694657       0.00      0.00      0.00         1\n",
      "         1069760626       0.00      0.00      0.00         1\n",
      "         1069789002       0.00      0.00      0.00         1\n",
      "         1069820934       0.00      0.00      0.00         1\n",
      "         1069859760       0.00      0.00      0.00         1\n",
      "         1069929021       0.00      0.00      0.00         1\n",
      "         1069949640       0.00      0.00      0.00         1\n",
      "         1069988368       0.00      0.00      0.00         1\n",
      "         1069989592       0.00      0.00      0.00         1\n",
      "         1070050139       0.00      0.00      0.00         1\n",
      "         1070072826       0.00      0.00      0.00         1\n",
      "         1070094613       0.00      0.00      0.00         1\n",
      "         1070127849       0.00      0.00      0.00         1\n",
      "         1070153142       0.00      0.00      0.00         1\n",
      "         1070188489       0.00      0.00      0.00         1\n",
      "         1070242278       0.00      0.00      0.00         1\n",
      "         1070262721       0.00      0.00      0.00         1\n",
      "         1070289992       0.00      0.00      0.00         1\n",
      "         1070300414       0.00      0.00      0.00         1\n",
      "         1070306774       0.00      0.00      0.00         1\n",
      "         1070340980       0.00      0.00      0.00         1\n",
      "         1070351882       0.00      0.00      0.00         1\n",
      "         1070379394       0.00      0.00      0.00         1\n",
      "         1070420218       0.00      0.00      0.00         1\n",
      "         1070430897       0.00      0.00      0.00         1\n",
      "         1070453992       0.00      0.00      0.00         1\n",
      "         1070475822       0.00      0.00      0.00         1\n",
      "         1070535082       0.00      0.00      0.00         1\n",
      "         1070565364       0.00      0.00      0.00         1\n",
      "         1070581824       0.00      0.00      0.00         1\n",
      "         1070603680       0.00      0.00      0.00         1\n",
      "         1070658912       0.00      0.00      0.00         1\n",
      "         1070672133       0.00      0.00      0.00         1\n",
      "         1070786094       0.00      0.00      0.00         1\n",
      "         1070798168       0.00      0.00      0.00         1\n",
      "         1070833136       0.00      0.00      0.00         1\n",
      "         1070850616       0.00      0.00      0.00         1\n",
      "         1070862884       0.00      0.00      0.00         1\n",
      "         1070932308       0.00      0.00      0.00         1\n",
      "         1070986034       0.00      0.00      0.00         1\n",
      "         1071027038       0.00      0.00      0.00         1\n",
      "         1071053200       0.00      0.00      0.00         1\n",
      "         1071078655       0.00      0.00      0.00         1\n",
      "         1071108867       0.00      0.00      0.00         1\n",
      "         1071135301       0.00      0.00      0.00         1\n",
      "         1071250728       0.00      0.00      0.00         1\n",
      "         1071276206       0.00      0.00      0.00         1\n",
      "         1071285036       0.00      0.00      0.00         1\n",
      "         1071327042       0.00      0.00      0.00         1\n",
      "         1071335452       0.00      0.00      0.00         1\n",
      "         1071336150       0.00      0.00      0.00         1\n",
      "         1071406388       0.00      0.00      0.00         1\n",
      "         1071540258       0.00      0.00      0.00         1\n",
      "         1071586850       0.00      0.00      0.00         1\n",
      "         1071588006       0.00      0.00      0.00         1\n",
      "         1071786507       0.00      0.00      0.00         1\n",
      "         1071862661       0.00      0.00      0.00         1\n",
      "         1071905425       0.00      0.00      0.00         1\n",
      "         1071909988       0.00      0.00      0.00         1\n",
      "         1072062446       0.00      0.00      0.00         1\n",
      "         1072082487       0.00      0.00      0.00         1\n",
      "         1072101438       0.00      0.00      0.00         1\n",
      "         1072186146       0.00      0.00      0.00         1\n",
      "         1072203976       0.00      0.00      0.00         1\n",
      "         1072362363       0.00      0.00      0.00         1\n",
      "         1072441956       0.00      0.00      0.00         1\n",
      "         1072572777       0.00      0.00      0.00         1\n",
      "         1072647818       0.00      0.00      0.00         1\n",
      "         1072663934       0.00      0.00      0.00         1\n",
      "         1072736828       0.00      0.00      0.00         1\n",
      "         1072761394       0.00      0.00      0.00         1\n",
      "         1072765615       0.00      0.00      0.00         1\n",
      "         1072785480       0.00      0.00      0.00         1\n",
      "         1072804250       0.00      0.00      0.00         1\n",
      "         1073056078       0.00      0.00      0.00         1\n",
      "         1073090654       0.00      0.00      0.00         1\n",
      "         1073192608       0.00      0.00      0.00         1\n",
      "         1073267581       0.00      0.00      0.00         1\n",
      "         1073292250       0.00      0.00      0.00         1\n",
      "         1073354672       0.00      0.00      0.00         1\n",
      "         1073356822       0.00      0.00      0.00         1\n",
      "         1073403236       0.00      0.00      0.00         1\n",
      "         1073405474       0.00      0.00      0.00         1\n",
      "         1073435226       0.00      0.00      0.00         1\n",
      "         1073491694       0.00      0.00      0.00         1\n",
      "         1073548062       0.00      0.00      0.00         1\n",
      "         1073708996       0.00      0.00      0.00         1\n",
      "         1073765371       0.00      0.00      0.00         1\n",
      "         1073785672       0.00      0.00      0.00         1\n",
      "         1073934779       0.00      0.00      0.00         1\n",
      "         1074117994       0.00      0.00      0.00         1\n",
      "         1074240297       0.00      0.00      0.00         1\n",
      "         1074391020       0.00      0.00      0.00         1\n",
      "         1074632870       0.00      0.00      0.00         1\n",
      "         1074645949       0.00      0.00      0.00         1\n",
      "         1074685609       0.00      0.00      0.00         1\n",
      "         1074746369       0.00      0.00      0.00         1\n",
      "         1075254852       0.00      0.00      0.00         1\n",
      "         1075471731       0.00      0.00      0.00         1\n",
      "         1075714960       0.00      0.00      0.00         1\n",
      "         1075844832       0.00      0.00      0.00         1\n",
      "         1076011419       0.00      0.00      0.00         1\n",
      "         1076180017       0.00      0.00      0.00         1\n",
      "         1076237832       0.00      0.00      0.00         1\n",
      "         1076244149       0.00      0.00      0.00         1\n",
      "         1076518538       0.00      0.00      0.00         1\n",
      "         1076546888       0.00      0.00      0.00         1\n",
      "         1076853373       0.00      0.00      0.00         1\n",
      "         1077176400       0.00      0.00      0.00         1\n",
      "         1077974118       0.00      0.00      0.00         1\n",
      "         1078396442       0.00      0.00      0.00         1\n",
      "         1078668698       0.00      0.00      0.00         1\n",
      "         1079387370       0.00      0.00      0.00         1\n",
      "         1080689372       0.00      0.00      0.00         1\n",
      "         1081036227       0.00      0.00      0.00         1\n",
      "         1081173092       0.00      0.00      0.00         1\n",
      "         1081264598       0.00      0.00      0.00         1\n",
      "         1081462027       0.00      0.00      0.00         1\n",
      "         1082149404       0.00      0.00      0.00         1\n",
      "         1082427993       0.00      0.00      0.00         1\n",
      "         1082719687       0.00      0.00      0.00         1\n",
      "      5514453319680       0.00      0.00      0.00         1\n",
      "4166445381829263360       0.00      0.00      0.00         1\n",
      "4197339460599909148       0.00      0.00      0.00         1\n",
      "4208451125106161106       0.00      0.00      0.00         1\n",
      "4216199640195792896       0.00      0.00      0.00         1\n",
      "4218450357677719552       0.00      0.00      0.00         1\n",
      "4222533668985372672       0.00      0.00      0.00         1\n",
      "4224966889269999506       0.00      0.00      0.00         1\n",
      "4229179117263650816       0.00      0.00      0.00         1\n",
      "4241781719541219328       0.00      0.00      0.00         1\n",
      "4243205037343375360       0.00      0.00      0.00         1\n",
      "4255066019028008960       0.00      0.00      0.00         1\n",
      "4258559168537233270       0.00      0.00      0.00         1\n",
      "4261718614131867648       0.00      0.00      0.00         1\n",
      "4264127095427003401       0.00      0.00      0.00         1\n",
      "4265834635910447104       0.00      0.00      0.00         1\n",
      "4269671382747847296       0.00      0.00      0.00         1\n",
      "4277998533048532992       0.00      0.00      0.00         1\n",
      "4280795690629595136       0.00      0.00      0.00         1\n",
      "4287753606368591872       0.00      0.00      0.00         1\n",
      "4288372426309096519       0.00      0.00      0.00         1\n",
      "4288429600928145539       0.00      0.00      0.00         1\n",
      "4291383438888452843       0.00      0.00      0.00         1\n",
      "4291490640232972288       0.00      0.00      0.00         1\n",
      "4292676812460174200       0.00      0.00      0.00         1\n",
      "4294507700139589632       0.00      0.00      0.00         1\n",
      "4295970532680362392       0.00      0.00      0.00         1\n",
      "4296152844412649472       0.00      0.00      0.00         1\n",
      "4301632605265062232       0.00      0.00      0.00         1\n",
      "4305572910283620352       0.00      0.00      0.00         1\n",
      "4307231249726113360       0.00      0.00      0.00         1\n",
      "4307995409277517824       0.00      0.00      0.00         1\n",
      "4311165988495163392       0.00      0.00      0.00         1\n",
      "4314780220654616576       0.00      0.00      0.00         1\n",
      "4319572716962185216       0.00      0.00      0.00         1\n",
      "4319682668124962816       0.00      0.00      0.00         1\n",
      "4323709629461692416       0.00      0.00      0.00         1\n",
      "4327467486383294858       0.00      0.00      0.00         1\n",
      "4327526035346591092       0.00      0.00      0.00         1\n",
      "4327736315920515072       0.00      0.00      0.00         1\n",
      "4328558200862277632       0.00      0.00      0.00         1\n",
      "4329494847330189312       0.00      0.00      0.00         1\n",
      "4329595864960991232       0.00      0.00      0.00         1\n",
      "4330201696921005610       0.00      0.00      0.00         1\n",
      "4330877071918156480       0.00      0.00      0.00         1\n",
      "4332638488512954368       0.00      0.00      0.00         1\n",
      "4333484287832621056       0.00      0.00      0.00         1\n",
      "4335047793367318528       0.00      0.00      0.00         1\n",
      "4335904039094512992       0.00      0.00      0.00         1\n",
      "4337212870255752482       0.00      0.00      0.00         1\n",
      "4338156663548108306       0.00      0.00      0.00         1\n",
      "4338610212092020492       0.00      0.00      0.00         1\n",
      "4339522255936552960       0.00      0.00      0.00         1\n",
      "4340148703736198359       0.00      0.00      0.00         1\n",
      "4340669597378245384       0.00      0.00      0.00         1\n",
      "4340900768639877120       0.00      0.00      0.00         1\n",
      "4346217956384149720       0.00      0.00      0.00         1\n",
      "4346418117988057088       0.00      0.00      0.00         1\n",
      "4347419773080961024       0.00      0.00      0.00         1\n",
      "4347451384040259584       0.00      0.00      0.00         1\n",
      "4347822470271762290       0.00      0.00      0.00         1\n",
      "4349218025418734301       0.00      0.00      0.00         1\n",
      "4349623503620669440       0.00      0.00      0.00         1\n",
      "4349685609918379149       0.00      0.00      0.00         1\n",
      "4350566301542363759       0.00      0.00      0.00         1\n",
      "4350815340915274854       0.00      0.00      0.00         1\n",
      "4352060812727830368       0.00      0.00      0.00         1\n",
      "4352183133391305895       0.00      0.00      0.00         1\n",
      "4352388535900403582       0.00      0.00      0.00         1\n",
      "4352599847237320704       0.00      0.00      0.00         1\n",
      "4353089199678322480       0.00      0.00      0.00         1\n",
      "4353776050833791412       0.00      0.00      0.00         1\n",
      "4356541047726780632       0.00      0.00      0.00         1\n",
      "4356663642213580800       0.00      0.00      0.00         1\n",
      "4356934913400162725       0.00      0.00      0.00         1\n",
      "4357501195196039168       0.00      0.00      0.00         1\n",
      "4357683989004156928       0.00      0.00      0.00         1\n",
      "4361406797936852992       0.00      0.00      0.00         1\n",
      "4361483626311843840       0.00      0.00      0.00         1\n",
      "4362279673798831320       0.00      0.00      0.00         1\n",
      "4362794382655132537       0.00      0.00      0.00         1\n",
      "4363132619936462929       0.00      0.00      0.00         1\n",
      "4364783947901566976       0.00      0.00      0.00         1\n",
      "4365323945549758464       0.00      0.00      0.00         1\n",
      "4365337500466544640       0.00      0.00      0.00         1\n",
      "4366594431235653632       0.00      0.00      0.00         1\n",
      "4366819213705935619       0.00      0.00      0.00         1\n",
      "4367403827453486984       0.00      0.00      0.00         1\n",
      "4367878111061082112       0.00      0.00      0.00         1\n",
      "4367948205985210733       0.00      0.00      0.00         1\n",
      "4368388009578463232       0.00      0.00      0.00         1\n",
      "4368858256957767680       0.00      0.00      0.00         1\n",
      "4369312150144418980       0.00      0.00      0.00         1\n",
      "4369464432513333932       0.00      0.00      0.00         1\n",
      "4370213749702959132       0.00      0.00      0.00         1\n",
      "4370218743683350528       0.00      0.00      0.00         1\n",
      "4370768556391534446       0.00      0.00      0.00         1\n",
      "4371146134496739328       0.00      0.00      0.00         1\n",
      "4371260347333749627       0.00      0.00      0.00         1\n",
      "4372764891531635240       0.00      0.00      0.00         1\n",
      "4373682434012412640       0.00      0.00      0.00         1\n",
      "4373875398286480534       0.00      0.00      0.00         1\n",
      "4373999620573888512       0.00      0.00      0.00         1\n",
      "4374705804451796003       0.00      0.00      0.00         1\n",
      "4375374169022398464       0.00      0.00      0.00         1\n",
      "4375579641315366202       0.00      0.00      0.00         1\n",
      "4376164684580131269       0.00      0.00      0.00         1\n",
      "4377006807400215907       0.00      0.00      0.00         1\n",
      "4377151805500680110       0.00      0.00      0.00         1\n",
      "4377202175823052800       0.00      0.00      0.00         1\n",
      "4378641506292877676       0.00      0.00      0.00         1\n",
      "4378962974975459328       0.00      0.00      0.00         1\n",
      "4379320454766562739       0.00      0.00      0.00         1\n",
      "4379451296634555194       0.00      0.00      0.00         1\n",
      "4379563308324225024       0.00      0.00      0.00         1\n",
      "4379627629754449920       0.00      0.00      0.00         1\n",
      "4379777575652687872       0.00      0.00      0.00         1\n",
      "4379818808405747784       0.00      0.00      0.00         1\n",
      "4380015207603240960       0.00      0.00      0.00         1\n",
      "4380486074532537590       0.00      0.00      0.00         1\n",
      "4380901482694705152       0.00      0.00      0.00         1\n",
      "4381221784175771648       0.00      0.00      0.00         1\n",
      "4381391934659512932       0.00      0.00      0.00         1\n",
      "4381527586908228568       0.00      0.00      0.00         1\n",
      "4381975363003157600       0.00      0.00      0.00         1\n",
      "4382181657826820096       0.00      0.00      0.00         1\n",
      "4382582568303363975       0.00      0.00      0.00         1\n",
      "4383014331726430208       0.00      0.00      0.00         1\n",
      "4383088704437375616       0.00      0.00      0.00         1\n",
      "4384339449396330496       0.00      0.00      0.00         1\n",
      "4384372435799380862       0.00      0.00      0.00         1\n",
      "4384401228205916160       0.00      0.00      0.00         1\n",
      "4384437683888324608       0.00      0.00      0.00         1\n",
      "4384801382786332803       0.00      0.00      0.00         1\n",
      "4385080932550311936       0.00      0.00      0.00         1\n",
      "4386650142842672010       0.00      0.00      0.00         1\n",
      "4386651517247225924       0.00      0.00      0.00         1\n",
      "4386960823616860913       0.00      0.00      0.00         1\n",
      "4387737902398832640       0.00      0.00      0.00         1\n",
      "4387932653395902464       0.00      0.00      0.00         1\n",
      "4387999002050691072       0.00      0.00      0.00         1\n",
      "4388042604558680064       0.00      0.00      0.00         1\n",
      "4388380704384221184       0.00      0.00      0.00         1\n",
      "4389447575330122226       0.00      0.00      0.00         1\n",
      "4389558694704602714       0.00      0.00      0.00         1\n",
      "4389743961363709952       0.00      0.00      0.00         1\n",
      "4390034096051657173       0.00      0.00      0.00         1\n",
      "4390419268711946630       0.00      0.00      0.00         1\n",
      "4390759669589606400       0.00      0.00      0.00         1\n",
      "4391097066108491210       0.00      0.00      0.00         1\n",
      "4391243832662949888       0.00      0.00      0.00         1\n",
      "4391636083436158976       0.00      0.00      0.00         1\n",
      "4391715936521760796       0.00      0.00      0.00         1\n",
      "4391811868910431910       0.00      0.00      0.00         1\n",
      "4391884230525005311       0.00      0.00      0.00         1\n",
      "4393175675648041698       0.00      0.00      0.00         1\n",
      "4395364389926993920       0.00      0.00      0.00         1\n",
      "4395684004213293056       0.00      0.00      0.00         1\n",
      "4395801232107653523       0.00      0.00      0.00         1\n",
      "4395871575092885818       0.00      0.00      0.00         1\n",
      "4396069384099543886       0.00      0.00      0.00         1\n",
      "4396085189576439317       0.00      0.00      0.00         1\n",
      "4396175280807019047       0.00      0.00      0.00         1\n",
      "4396755067039900000       0.00      0.00      0.00         1\n",
      "4396762986951129798       0.00      0.00      0.00         1\n",
      "4396908516569251840       0.00      0.00      0.00         1\n",
      "4397073650522637448       0.00      0.00      0.00         1\n",
      "4397641890824978432       0.00      0.00      0.00         1\n",
      "4398770883108274176       0.00      0.00      0.00         1\n",
      "4398917600241683571       0.00      0.00      0.00         1\n",
      "4398943695412396032       0.00      0.00      0.00         1\n",
      "4399326206247080272       0.00      0.00      0.00         1\n",
      "4399477937804410880       0.00      0.00      0.00         1\n",
      "4399641259291289506       0.00      0.00      0.00         1\n",
      "4399880702657560576       0.00      0.00      0.00         1\n",
      "4400093527920836170       0.00      0.00      0.00         1\n",
      "4400717658639564800       0.00      0.00      0.00         1\n",
      "4401397961026667352       0.00      0.00      0.00         1\n",
      "4401444965129745314       0.00      0.00      0.00         1\n",
      "4401582919513809092       0.00      0.00      0.00         1\n",
      "4402123157665513868       0.00      0.00      0.00         1\n",
      "4402323817494478848       0.00      0.00      0.00         1\n",
      "4402731942466813952       0.00      0.00      0.00         1\n",
      "4402783894391226368       0.00      0.00      0.00         1\n",
      "4402961122974675122       0.00      0.00      0.00         1\n",
      "4403070902339886352       0.00      0.00      0.00         1\n",
      "4403276528205000601       0.00      0.00      0.00         1\n",
      "4403889385675658000       0.00      0.00      0.00         1\n",
      "4404299503508509234       0.00      0.00      0.00         1\n",
      "4404605720733941760       0.00      0.00      0.00         1\n",
      "4404691650144632832       0.00      0.00      0.00         1\n",
      "4404722952927593311       0.00      0.00      0.00         1\n",
      "4404877777900962076       0.00      0.00      0.00         1\n",
      "4405115133920018432       0.00      0.00      0.00         1\n",
      "4405488006849909654       0.00      0.00      0.00         1\n",
      "4405504396455355031       0.00      0.00      0.00         1\n",
      "4406429377802110229       0.00      0.00      0.00         1\n",
      "4406475092364623872       0.00      0.00      0.00         1\n",
      "4406680288722157568       0.00      0.00      0.00         1\n",
      "4406739663413628006       0.00      0.00      0.00         1\n",
      "4407255927082160312       0.00      0.00      0.00         1\n",
      "4407280725150138368       0.00      0.00      0.00         1\n",
      "4407349341547659264       0.00      0.00      0.00         1\n",
      "4407843023333364418       0.00      0.00      0.00         1\n",
      "4408233968428826160       0.00      0.00      0.00         1\n",
      "4408238022887453331       0.00      0.00      0.00         1\n",
      "4408934288625482112       0.00      0.00      0.00         1\n",
      "4409008710752403456       0.00      0.00      0.00         1\n",
      "4409072139895341720       0.00      0.00      0.00         1\n",
      "4409396049118096208       0.00      0.00      0.00         1\n",
      "4409469268685488128       0.00      0.00      0.00         1\n",
      "4409552592113647177       0.00      0.00      0.00         1\n",
      "4409602688605672088       0.00      0.00      0.00         1\n",
      "4409734131782116296       0.00      0.00      0.00         1\n",
      "4409743047080804352       0.00      0.00      0.00         1\n",
      "4409764659356237824       0.00      0.00      0.00         1\n",
      "4409900414682529792       0.00      0.00      0.00         1\n",
      "4410165190826393600       0.00      0.00      0.00         1\n",
      "4410780505021087744       0.00      0.00      0.00         1\n",
      "4411170832677176224       0.00      0.00      0.00         1\n",
      "4411252745265217536       0.00      0.00      0.00         1\n",
      "4411494879401164295       0.00      0.00      0.00         1\n",
      "4411507557084954624       0.00      0.00      0.00         1\n",
      "4411634208139230743       0.00      0.00      0.00         1\n",
      "4411915408234022376       0.00      0.00      0.00         1\n",
      "4411919873945370624       0.00      0.00      0.00         1\n",
      "4412499369156123904       0.00      0.00      0.00         1\n",
      "4412640810990450688       0.00      0.00      0.00         1\n",
      "4412794673928731148       0.00      0.00      0.00         1\n",
      "4413100647397538984       0.00      0.00      0.00         1\n",
      "4413313642372530176       0.00      0.00      0.00         1\n",
      "4414056396836831232       0.00      0.00      0.00         1\n",
      "4414063922681868967       0.00      0.00      0.00         1\n",
      "4414138825849176064       0.00      0.00      0.00         1\n",
      "4414500874412359680       0.00      0.00      0.00         1\n",
      "4415007710618058752       0.00      0.00      0.00         1\n",
      "4415024620953171805       0.00      0.00      0.00         1\n",
      "4415921271636754432       0.00      0.00      0.00         1\n",
      "4415994321498378666       0.00      0.00      0.00         1\n",
      "4416478174276222976       0.00      0.00      0.00         1\n",
      "4416676293562650504       0.00      0.00      0.00         1\n",
      "4416866353420435456       0.00      0.00      0.00         1\n",
      "4417229071998517248       0.00      0.00      0.00         1\n",
      "4417306106531938304       0.00      0.00      0.00         1\n",
      "4417624965961083100       0.00      0.00      0.00         1\n",
      "4417801505239728128       0.00      0.00      0.00         1\n",
      "4418288245293449216       0.00      0.00      0.00         1\n",
      "4418595697284529296       0.00      0.00      0.00         1\n",
      "4418603530252713984       0.00      0.00      0.00         1\n",
      "4418608134457655296       0.00      0.00      0.00         1\n",
      "4418973001589529080       0.00      0.00      0.00         1\n",
      "4418997397001961726       0.00      0.00      0.00         1\n",
      "4419004242111496192       0.00      0.00      0.00         1\n",
      "4419038491239391026       0.00      0.00      0.00         1\n",
      "4419268872226471936       0.00      0.00      0.00         1\n",
      "4419308386995626285       0.00      0.00      0.00         1\n",
      "4419671568360144896       0.00      0.00      0.00         1\n",
      "4420321104854253568       0.00      0.00      0.00         1\n",
      "4420397934277584456       0.00      0.00      0.00         1\n",
      "4420430301152622156       0.00      0.00      0.00         1\n",
      "4420527058194356475       0.00      0.00      0.00         1\n",
      "4420947620323655680       0.00      0.00      0.00         1\n",
      "4420964250437025792       0.00      0.00      0.00         1\n",
      "4421219965265877222       0.00      0.00      0.00         1\n",
      "4421306165258120310       0.00      0.00      0.00         1\n",
      "4421543213077849850       0.00      0.00      0.00         1\n",
      "4422051598717419520       0.00      0.00      0.00         1\n",
      "4422187355097368272       0.00      0.00      0.00         1\n",
      "4422193505504517827       0.00      0.00      0.00         1\n",
      "4422390591896158208       0.00      0.00      0.00         1\n",
      "4422473674786260748       0.00      0.00      0.00         1\n",
      "4422652500066594960       0.00      0.00      0.00         1\n",
      "4422828815999303680       0.00      0.00      0.00         1\n",
      "4422931277781314600       0.00      0.00      0.00         1\n",
      "4423209393051402240       0.00      0.00      0.00         1\n",
      "4423395838633506026       0.00      0.00      0.00         1\n",
      "4423531816246312960       0.00      0.00      0.00         1\n",
      "4423586380582728627       0.00      0.00      0.00         1\n",
      "4423694716823621865       0.00      0.00      0.00         1\n",
      "4423855142436182344       0.00      0.00      0.00         1\n",
      "4424254608761279370       0.00      0.00      0.00         1\n",
      "4424502686063967376       0.00      0.00      0.00         1\n",
      "4424565920861177552       0.00      0.00      0.00         1\n",
      "4424596973473171380       0.00      0.00      0.00         1\n",
      "4424708148716404068       0.00      0.00      0.00         1\n",
      "4424772495920827202       0.00      0.00      0.00         1\n",
      "4424930533542326536       0.00      0.00      0.00         1\n",
      "4425224927764871816       0.00      0.00      0.00         1\n",
      "4425333366048423936       0.00      0.00      0.00         1\n",
      "4426000358324630284       0.00      0.00      0.00         1\n",
      "4426132161246003200       0.00      0.00      0.00         1\n",
      "4426260324118746257       0.00      0.00      0.00         1\n",
      "4426616299605532532       0.00      0.00      0.00         1\n",
      "4426695386077331456       0.00      0.00      0.00         1\n",
      "4427479303508197376       0.00      0.00      0.00         1\n",
      "4428259923447732482       0.00      0.00      0.00         1\n",
      "4428328230617218340       0.00      0.00      0.00         1\n",
      "4428349979278442496       0.00      0.00      0.00         1\n",
      "4429260186991562448       0.00      0.00      0.00         1\n",
      "4429660082798204365       0.00      0.00      0.00         1\n",
      "4430062296840011776       0.00      0.00      0.00         1\n",
      "4430168743309475840       0.00      0.00      0.00         1\n",
      "4430678366948950016       0.00      0.00      0.00         1\n",
      "4430711078455065916       0.00      0.00      0.00         1\n",
      "4430933891733258240       0.00      0.00      0.00         1\n",
      "4431279662370390016       0.00      0.00      0.00         1\n",
      "4431667034060750848       0.00      0.00      0.00         1\n",
      "4431826232371636751       0.00      0.00      0.00         1\n",
      "4431908746230235136       0.00      0.00      0.00         1\n",
      "4431982182635942288       0.00      0.00      0.00         1\n",
      "4432048908192972800       0.00      0.00      0.00         1\n",
      "4432147417562873856       0.00      0.00      0.00         1\n",
      "4432186759463305216       0.00      0.00      0.00         1\n",
      "4432222288484014741       0.00      0.00      0.00         1\n",
      "4432282966730735616       0.00      0.00      0.00         1\n",
      "4432496375065739264       0.00      0.00      0.00         1\n",
      "4432576846634247606       0.00      0.00      0.00         1\n",
      "4432681128385880384       0.00      0.00      0.00         1\n",
      "4432707515658010624       0.00      0.00      0.00         1\n",
      "4432740689985404928       0.00      0.00      0.00         1\n",
      "4432843237687460776       0.00      0.00      0.00         1\n",
      "4433161957557665792       0.00      0.00      0.00         1\n",
      "4433300342454143584       0.00      0.00      0.00         1\n",
      "4433364216157569024       0.00      0.00      0.00         1\n",
      "4433473189133941008       0.00      0.00      0.00         1\n",
      "4433516859295268864       0.00      0.00      0.00         1\n",
      "4433864597027422208       0.00      0.00      0.00         1\n",
      "4433954589477175296       0.00      0.00      0.00         1\n",
      "4433980751181235107       0.00      0.00      0.00         1\n",
      "4434131383215980544       0.00      0.00      0.00         1\n",
      "4434423063034986496       0.00      0.00      0.00         1\n",
      "4434700793833775760       0.00      0.00      0.00         1\n",
      "4434817444111974400       0.00      0.00      0.00         1\n",
      "4434961948286648320       0.00      0.00      0.00         1\n",
      "4434978213327798272       0.00      0.00      0.00         1\n",
      "4435111786810703872       0.00      0.00      0.00         1\n",
      "4435478731636604928       0.00      0.00      0.00         1\n",
      "4435671790416560128       0.00      0.00      0.00         1\n",
      "4435818413064978040       0.00      0.00      0.00         1\n",
      "4435914757774487786       0.00      0.00      0.00         1\n",
      "4436210010991920898       0.00      0.00      0.00         1\n",
      "4436225918507155456       0.00      0.00      0.00         1\n",
      "4436261807253880832       0.00      0.00      0.00         1\n",
      "4436351434631413760       0.00      0.00      0.00         1\n",
      "4436464255860891706       0.00      0.00      0.00         1\n",
      "4436480446859051008       0.00      0.00      0.00         1\n",
      "4436487989854335840       0.00      0.00      0.00         1\n",
      "4436939776431489024       0.00      0.00      0.00         1\n",
      "4437181871895045340       0.00      0.00      0.00         1\n",
      "4437188198409515269       0.00      0.00      0.00         1\n",
      "4437422427676344320       0.00      0.00      0.00         1\n",
      "4437788291236428281       0.00      0.00      0.00         1\n",
      "4437891438105067520       0.00      0.00      0.00         1\n",
      "4438022658995357546       0.00      0.00      0.00         1\n",
      "4438053547350687744       0.00      0.00      0.00         1\n",
      "4438110413777047154       0.00      0.00      0.00         1\n",
      "4438118805083783168       0.00      0.00      0.00         1\n",
      "4438211057711666272       0.00      0.00      0.00         1\n",
      "4438268501873917952       0.00      0.00      0.00         1\n",
      "4438279033133727744       0.00      0.00      0.00         1\n",
      "4438289754431865878       0.00      0.00      0.00         1\n",
      "4438396904216199168       0.00      0.00      0.00         1\n",
      "4438567638760452288       0.00      0.00      0.00         1\n",
      "4438626084719467944       0.00      0.00      0.00         1\n",
      "4438808447982501888       0.00      0.00      0.00         1\n",
      "4439032825663979520       0.00      0.00      0.00         1\n",
      "4439083670537068156       0.00      0.00      0.00         1\n",
      "4439096057179476184       0.00      0.00      0.00         1\n",
      "4439196640011616256       0.00      0.00      0.00         1\n",
      "4439354270673292768       0.00      0.00      0.00         1\n",
      "4439464495647031296       0.00      0.00      0.00         1\n",
      "4439813522918907032       0.00      0.00      0.00         1\n",
      "4439836474174603264       0.00      0.00      0.00         1\n",
      "4439932476341127420       0.00      0.00      0.00         1\n",
      "4440072767164334652       0.00      0.00      0.00         1\n",
      "4440408460739215360       0.00      0.00      0.00         1\n",
      "4440737695752257536       0.00      0.00      0.00         1\n",
      "4440789339490601456       0.00      0.00      0.00         1\n",
      "4440908722407600544       0.00      0.00      0.00         1\n",
      "4440930729814568952       0.00      0.00      0.00         1\n",
      "4440945125492785152       0.00      0.00      0.00         1\n",
      "4440996099226506254       0.00      0.00      0.00         1\n",
      "4441149806454243328       0.00      0.00      0.00         1\n",
      "4441365956024125752       0.00      0.00      0.00         1\n",
      "4441507594409869312       0.00      0.00      0.00         1\n",
      "4441627820201173182       0.00      0.00      0.00         1\n",
      "4441644311808835584       0.00      0.00      0.00         1\n",
      "4441724571862695936       0.00      0.00      0.00         1\n",
      "4442047330065055744       0.00      0.00      0.00         1\n",
      "4442204478623449088       0.00      0.00      0.00         1\n",
      "4442204719141617664       0.00      0.00      0.00         1\n",
      "4442249215002804224       0.00      0.00      0.00         1\n",
      "4442444756273856512       0.00      0.00      0.00         1\n",
      "4442448810722983936       0.00      0.00      0.00         1\n",
      "4442544880551460864       0.00      0.00      0.00         1\n",
      "4442717645610942464       0.00      0.00      0.00         1\n",
      "4442782719708434308       0.00      0.00      0.00         1\n",
      "4443238037770158568       0.00      0.00      0.00         1\n",
      "4443466598753179465       0.00      0.00      0.00         1\n",
      "4443971806126145536       0.00      0.00      0.00         1\n",
      "4444019567226471899       0.00      0.00      0.00         1\n",
      "4444113897584851868       0.00      0.00      0.00         1\n",
      "4444114193943451243       0.00      0.00      0.00         1\n",
      "4444151611678854816       0.00      0.00      0.00         1\n",
      "4444217582398433702       0.00      0.00      0.00         1\n",
      "4444356971266622055       0.00      0.00      0.00         1\n",
      "4444515412607184864       0.00      0.00      0.00         1\n",
      "4444690509837917338       0.00      0.00      0.00         1\n",
      "4444800357915665857       0.00      0.00      0.00         1\n",
      "4444968186999209984       0.00      0.00      0.00         1\n",
      "4445037165207396538       0.00      0.00      0.00         1\n",
      "4445936419720225721       0.00      0.00      0.00         1\n",
      "4446565777394368512       0.00      0.00      0.00         1\n",
      "4446729648618207616       0.00      0.00      0.00         1\n",
      "4446924132285677568       0.00      0.00      0.00         1\n",
      "4447228112891019264       0.00      0.00      0.00         1\n",
      "4447294049228947456       0.00      0.00      0.00         1\n",
      "4447359110451403466       0.00      0.00      0.00         1\n",
      "4447413278588712704       0.00      0.00      0.00         1\n",
      "4447494675741278208       0.00      0.00      0.00         1\n",
      "4447588410169759404       0.00      0.00      0.00         1\n",
      "4447796994194276352       0.00      0.00      0.00         1\n",
      "4447868612773937152       0.00      0.00      0.00         1\n",
      "4448096623997747200       0.00      0.00      0.00         1\n",
      "4448350954781147136       0.00      0.00      0.00         1\n",
      "4448430069119142556       0.00      0.00      0.00         1\n",
      "4448455133507878912       0.00      0.00      0.00         1\n",
      "4448582694036570112       0.00      0.00      0.00         1\n",
      "4448596524855277448       0.00      0.00      0.00         1\n",
      "4448604032484349778       0.00      0.00      0.00         1\n",
      "4448628289409384448       0.00      0.00      0.00         1\n",
      "4448761314197857333       0.00      0.00      0.00         1\n",
      "4448877122634645504       0.00      0.00      0.00         1\n",
      "4448941048927879168       0.00      0.00      0.00         1\n",
      "4448975916529295036       0.00      0.00      0.00         1\n",
      "4449034989507751888       0.00      0.00      0.00         1\n",
      "4449403969092976640       0.00      0.00      0.00         1\n",
      "4449580655457599488       0.00      0.00      0.00         1\n",
      "4449613606446694400       0.00      0.00      0.00         1\n",
      "4449809336696307712       0.00      0.00      0.00         1\n",
      "4449828251732279296       0.00      0.00      0.00         1\n",
      "4450010359387527658       0.00      0.00      0.00         1\n",
      "4450155254432664876       0.00      0.00      0.00         1\n",
      "4450156800611984752       0.00      0.00      0.00         1\n",
      "4450175938977171760       0.00      0.00      0.00         1\n",
      "4450181366763487232       0.00      0.00      0.00         1\n",
      "4450293036967474718       0.00      0.00      0.00         1\n",
      "4450467687489597352       0.00      0.00      0.00         1\n",
      "4450474937425193357       0.00      0.00      0.00         1\n",
      "4450508127875366912       0.00      0.00      0.00         1\n",
      "4450573592814717600       0.00      0.00      0.00         1\n",
      "4450676405755379539       0.00      0.00      0.00         1\n",
      "4450997685427634176       0.00      0.00      0.00         1\n",
      "4451071765023555584       0.00      0.00      0.00         1\n",
      "4451098583829421088       0.00      0.00      0.00         1\n",
      "4451128975047438068       0.00      0.00      0.00         1\n",
      "4451194120051884032       0.00      0.00      0.00         1\n",
      "4451384714570401504       0.00      0.00      0.00         1\n",
      "4452262089439838208       0.00      0.00      0.00         1\n",
      "4452433888131678208       0.00      0.00      0.00         1\n",
      "4452493606417325984       0.00      0.00      0.00         1\n",
      "4452681708807869432       0.00      0.00      0.00         1\n",
      "4452694954488328916       0.00      0.00      0.00         1\n",
      "4452836310453732551       0.00      0.00      0.00         1\n",
      "4452843311241564414       0.00      0.00      0.00         1\n",
      "4452957169767153664       0.00      0.00      0.00         1\n",
      "4452979667873413529       0.00      0.00      0.00         1\n",
      "4453002885399052288       0.00      0.00      0.00         1\n",
      "4453123867061475584       0.00      0.00      0.00         1\n",
      "4453374968066380628       0.00      0.00      0.00         1\n",
      "4453832227466230079       0.00      0.00      0.00         1\n",
      "4453841470243083573       0.00      0.00      0.00         1\n",
      "4453983684130766848       0.00      0.00      0.00         1\n",
      "4454028987445805056       0.00      0.00      0.00         1\n",
      "4454280208672882688       0.00      0.00      0.00         1\n",
      "4454489323094032579       0.00      0.00      0.00         1\n",
      "4454519559653474576       0.00      0.00      0.00         1\n",
      "4454735200328351744       0.00      0.00      0.00         1\n",
      "4454815328293754579       0.00      0.00      0.00         1\n",
      "4454842711949705216       0.00      0.00      0.00         1\n",
      "4454939826521434520       0.00      0.00      0.00         1\n",
      "4455048493485056403       0.00      0.00      0.00         1\n",
      "4455083488862146820       0.00      0.00      0.00         1\n",
      "4455157199097420220       0.00      0.00      0.00         1\n",
      "4455220635775814841       0.00      0.00      0.00         1\n",
      "4455475755769397248       0.00      0.00      0.00         1\n",
      "4455491836126953472       0.00      0.00      0.00         1\n",
      "4455497011562545152       0.00      0.00      0.00         1\n",
      "4455634411861311488       0.00      0.00      0.00         1\n",
      "4455963441765777340       0.00      0.00      0.00         1\n",
      "4456153537035018682       0.00      0.00      0.00         1\n",
      "4456193256880271272       0.00      0.00      0.00         1\n",
      "4456765518328531853       0.00      0.00      0.00         1\n",
      "4457218693183802880       0.00      0.00      0.00         1\n",
      "4457296856262639616       0.00      0.00      0.00         1\n",
      "4457547674814734058       0.00      0.00      0.00         1\n",
      "4457729307929739264       0.00      0.00      0.00         1\n",
      "4457770986292379648       0.00      0.00      0.00         1\n",
      "4457820327936528033       0.00      0.00      0.00         1\n",
      "4458017792293076992       0.00      0.00      0.00         1\n",
      "4458074074602434769       0.00      0.00      0.00         1\n",
      "4458126026487972608       0.00      0.00      0.00         1\n",
      "4458361562524959328       0.00      0.00      0.00         1\n",
      "4458546966623682560       0.00      0.00      0.00         1\n",
      "4458558971057274880       0.00      0.00      0.00         1\n",
      "4458591944548869472       0.00      0.00      0.00         1\n",
      "4458647292764749824       0.00      0.00      0.00         1\n",
      "4458853112945158024       0.00      0.00      0.00         1\n",
      "4458874003662317621       0.00      0.00      0.00         1\n",
      "4459117339330921210       0.00      0.00      0.00         1\n",
      "4459160426433793492       0.00      0.00      0.00         1\n",
      "4459182845121789952       0.00      0.00      0.00         1\n",
      "4459562254992073448       0.00      0.00      0.00         1\n",
      "4459671390110908874       0.00      0.00      0.00         1\n",
      "4459704632108646400       0.00      0.00      0.00         1\n",
      "4459726434421232925       0.00      0.00      0.00         1\n",
      "4459731243726012416       0.00      0.00      0.00         1\n",
      "4459956437451276288       0.00      0.00      0.00         1\n",
      "4459990454638444144       0.00      0.00      0.00         1\n",
      "4460019866598083194       0.00      0.00      0.00         1\n",
      "4460076628879385648       0.00      0.00      0.00         1\n",
      "4460319997842948096       0.00      0.00      0.00         1\n",
      "4460597667478634496       0.00      0.00      0.00         1\n",
      "4460825489723883520       0.00      0.00      0.00         1\n",
      "4460964483455516672       0.00      0.00      0.00         1\n",
      "4461060966629194720       0.00      0.00      0.00         1\n",
      "4461148446568116898       0.00      0.00      0.00         1\n",
      "4461152568663343104       0.00      0.00      0.00         1\n",
      "4461371938412953600       0.00      0.00      0.00         1\n",
      "4461624156072443904       0.00      0.00      0.00         1\n",
      "4461639875652747264       0.00      0.00      0.00         1\n",
      "4461886836272267264       0.00      0.00      0.00         1\n",
      "4461914976897990656       0.00      0.00      0.00         1\n",
      "4461928308476477440       0.00      0.00      0.00         1\n",
      "4462060971426316288       0.00      0.00      0.00         1\n",
      "4462165029893963776       0.00      0.00      0.00         1\n",
      "4462203359226589471       0.00      0.00      0.00         1\n",
      "4462209028599331230       0.00      0.00      0.00         1\n",
      "4462344611066544128       0.00      0.00      0.00         1\n",
      "4462353613317996544       0.00      0.00      0.00         1\n",
      "4462610728291860489       0.00      0.00      0.00         1\n",
      "4462670823475921575       0.00      0.00      0.00         1\n",
      "4462807334719180666       0.00      0.00      0.00         1\n",
      "4462822487293089792       0.00      0.00      0.00         1\n",
      "4462914914003976192       0.00      0.00      0.00         1\n",
      "4463470374593563422       0.00      0.00      0.00         1\n",
      "4463525796855181604       0.00      0.00      0.00         1\n",
      "4463551582776066048       0.00      0.00      0.00         1\n",
      "4463602522150739288       0.00      0.00      0.00         1\n",
      "4463695532899958784       0.00      0.00      0.00         1\n",
      "4463737348701552640       0.00      0.00      0.00         1\n",
      "4463806893859819193       0.00      0.00      0.00         1\n",
      "4463842275810555640       0.00      0.00      0.00         1\n",
      "4464068148142687875       0.00      0.00      0.00         1\n",
      "4464268618976198656       0.00      0.00      0.00         1\n",
      "4464386783177235324       0.00      0.00      0.00         1\n",
      "4464490376727625728       0.00      0.00      0.00         1\n",
      "4464525595459452928       0.00      0.00      0.00         1\n",
      "4464797277910728704       0.00      0.00      0.00         1\n",
      "4464850656817935734       0.00      0.00      0.00         1\n",
      "4464886613230485504       0.00      0.00      0.00         1\n",
      "4465052159464148168       0.00      0.00      0.00         1\n",
      "4465168295431772448       0.00      0.00      0.00         1\n",
      "4465201950788994583       0.00      0.00      0.00         1\n",
      "4465375921674649600       0.00      0.00      0.00         1\n",
      "4465376686178828288       0.00      0.00      0.00         1\n",
      "4465889678134799366       0.00      0.00      0.00         1\n",
      "4466042063573444250       0.00      0.00      0.00         1\n",
      "4466130006262677504       0.00      0.00      0.00         1\n",
      "4466243085182548512       0.00      0.00      0.00         1\n",
      "4466440533462312740       0.00      0.00      0.00         1\n",
      "4466518357205581824       0.00      0.00      0.00         1\n",
      "4466563965463298048       0.00      0.00      0.00         1\n",
      "4466589722382172160       0.00      0.00      0.00         1\n",
      "4466599859542073368       0.00      0.00      0.00         1\n",
      "4466637689641252908       0.00      0.00      0.00         1\n",
      "4466678572370624512       0.00      0.00      0.00         1\n",
      "4467321413010718720       0.00      0.00      0.00         1\n",
      "4467402914310127616       0.00      0.00      0.00         1\n",
      "4467405319491813376       0.00      0.00      0.00         1\n",
      "4467445348587012096       0.00      0.00      0.00         1\n",
      "4467548565241069568       0.00      0.00      0.00         1\n",
      "4467660389009588224       0.00      0.00      0.00         1\n",
      "4467705365907111936       0.00      0.00      0.00         1\n",
      "4467744313740593849       0.00      0.00      0.00         1\n",
      "4467757034363682816       0.00      0.00      0.00         1\n",
      "4467764344398020608       0.00      0.00      0.00         1\n",
      "4467845523574882304       0.00      0.00      0.00         1\n",
      "4467913930586031874       0.00      0.00      0.00         1\n",
      "4467916077002653696       0.00      0.00      0.00         1\n",
      "4467923637195615836       0.00      0.00      0.00         1\n",
      "4468015694474117120       0.00      0.00      0.00         1\n",
      "4468137233458659328       0.00      0.00      0.00         1\n",
      "4468153519974645760       0.00      0.00      0.00         1\n",
      "4468163055857788727       0.00      0.00      0.00         1\n",
      "4468164773847770624       0.00      0.00      0.00         1\n",
      "4468248697511184554       0.00      0.00      0.00         1\n",
      "4468254985333095284       0.00      0.00      0.00         1\n",
      "4468311150631117468       0.00      0.00      0.00         1\n",
      "4468315629220265984       0.00      0.00      0.00         1\n",
      "4468361036668312880       0.00      0.00      0.00         1\n",
      "4468410066961170432       0.00      0.00      0.00         1\n",
      "4468467955589424943       0.00      0.00      0.00         1\n",
      "4468483924277772023       0.00      0.00      0.00         1\n",
      "4468769263666069504       0.00      0.00      0.00         1\n",
      "4468837759804506112       0.00      0.00      0.00         1\n",
      "4468839070826832002       0.00      0.00      0.00         1\n",
      "4468853513744547840       0.00      0.00      0.00         1\n",
      "4469018951589822464       0.00      0.00      0.00         1\n",
      "4469066973619159040       0.00      0.00      0.00         1\n",
      "4469093413437833216       0.00      0.00      0.00         1\n",
      "4469136106457444998       0.00      0.00      0.00         1\n",
      "4469137927537836909       0.00      0.00      0.00         1\n",
      "4469163851942392574       0.00      0.00      0.00         1\n",
      "4469220047312600827       0.00      0.00      0.00         1\n",
      "4469227158719430656       0.00      0.00      0.00         1\n",
      "4469241589809545216       0.00      0.00      0.00         1\n",
      "4469284248467761384       0.00      0.00      0.00         1\n",
      "4469320309022939933       0.00      0.00      0.00         1\n",
      "4469341152509897878       0.00      0.00      0.00         1\n",
      "4469451424731758592       0.00      0.00      0.00         1\n",
      "4469506435723096972       0.00      0.00      0.00         1\n",
      "4469592385558413312       0.00      0.00      0.00         1\n",
      "4469598364152889344       0.00      0.00      0.00         1\n",
      "4469716871940302428       0.00      0.00      0.00         1\n",
      "4469740279508076710       0.00      0.00      0.00         1\n",
      "4469811249561010796       0.00      0.00      0.00         1\n",
      "4469838607443558400       0.00      0.00      0.00         1\n",
      "4469841476481712128       0.00      0.00      0.00         1\n",
      "4470081514646799704       0.00      0.00      0.00         1\n",
      "4470188145810785876       0.00      0.00      0.00         1\n",
      "4470286109718101148       0.00      0.00      0.00         1\n",
      "4470291435477137386       0.00      0.00      0.00         1\n",
      "4470538644163133440       0.00      0.00      0.00         1\n",
      "4470564216398413824       0.00      0.00      0.00         1\n",
      "4470572394016145408       0.00      0.00      0.00         1\n",
      "4470633915127693312       0.00      0.00      0.00         1\n",
      "4470644536581816320       0.00      0.00      0.00         1\n",
      "4470644583826456576       0.00      0.00      0.00         1\n",
      "4470652324412943694       0.00      0.00      0.00         1\n",
      "4470677329715767668       0.00      0.00      0.00         1\n",
      "4470770289997064340       0.00      0.00      0.00         1\n",
      "4470778449367138304       0.00      0.00      0.00         1\n",
      "4470814063235956736       0.00      0.00      0.00         1\n",
      "4470820909413826560       0.00      0.00      0.00         1\n",
      "4470869622932897792       0.00      0.00      0.00         1\n",
      "4470887765902814576       0.00      0.00      0.00         1\n",
      "4470932313331868108       0.00      0.00      0.00         1\n",
      "4471027505930698752       0.00      0.00      0.00         1\n",
      "4471067517846028288       0.00      0.00      0.00         1\n",
      "4471114848385630208       0.00      0.00      0.00         1\n",
      "4471160478118182912       0.00      0.00      0.00         1\n",
      "4471249194962649088       0.00      0.00      0.00         1\n",
      "4471361664026068312       0.00      0.00      0.00         1\n",
      "4471392184074583609       0.00      0.00      0.00         1\n",
      "4471400103977878768       0.00      0.00      0.00         1\n",
      "4471458979400577055       0.00      0.00      0.00         1\n",
      "4471491947556818617       0.00      0.00      0.00         1\n",
      "4471594150606885462       0.00      0.00      0.00         1\n",
      "4471661186478824595       0.00      0.00      0.00         1\n",
      "4471770592158183131       0.00      0.00      0.00         1\n",
      "4471835698516721664       0.00      0.00      0.00         1\n",
      "4471843412277985280       0.00      0.00      0.00         1\n",
      "4471986521648383716       0.00      0.00      0.00         1\n",
      "4471992087890033296       0.00      0.00      0.00         1\n",
      "4472073016934662144       0.00      0.00      0.00         1\n",
      "4472084824867067080       0.00      0.00      0.00         1\n",
      "4472103897749520384       0.00      0.00      0.00         1\n",
      "4472201457931649024       0.00      0.00      0.00         1\n",
      "4472269096136500157       0.00      0.00      0.00         1\n",
      "4472305586159747547       0.00      0.00      0.00         1\n",
      "4472458949810978816       0.00      0.00      0.00         1\n",
      "4472478690537780024       0.00      0.00      0.00         1\n",
      "4472563825348969904       0.00      0.00      0.00         1\n",
      "4472591785616760932       0.00      0.00      0.00         1\n",
      "4472639262185676519       0.00      0.00      0.00         1\n",
      "4472716958133804472       0.00      0.00      0.00         1\n",
      "4472721201566834129       0.00      0.00      0.00         1\n",
      "4472735133387980800       0.00      0.00      0.00         1\n",
      "4472797325558985444       0.00      0.00      0.00         1\n",
      "4472884426451189760       0.00      0.00      0.00         1\n",
      "4472890267606712320       0.00      0.00      0.00         1\n",
      "4472905866927931392       0.00      0.00      0.00         1\n",
      "4472920917554551968       0.00      0.00      0.00         1\n",
      "4472948455823638528       0.00      0.00      0.00         1\n",
      "4473019066149144407       0.00      0.00      0.00         1\n",
      "4473065863049641984       0.00      0.00      0.00         1\n",
      "4473147329989312512       0.00      0.00      0.00         1\n",
      "4473188648640660541       0.00      0.00      0.00         1\n",
      "4473356305918066688       0.00      0.00      0.00         1\n",
      "4473377266408132780       0.00      0.00      0.00         1\n",
      "4473419527836663808       0.00      0.00      0.00         1\n",
      "4473420009910191351       0.00      0.00      0.00         1\n",
      "4473474534510969112       0.00      0.00      0.00         1\n",
      "4473562259238097248       0.00      0.00      0.00         1\n",
      "4473570866371282476       0.00      0.00      0.00         1\n",
      "4473618788549394432       0.00      0.00      0.00         1\n",
      "4473781490500501504       0.00      0.00      0.00         1\n",
      "4473787782627590144       0.00      0.00      0.00         1\n",
      "4473799306024845312       0.00      0.00      0.00         1\n",
      "4473807493281645535       0.00      0.00      0.00         1\n",
      "4474001007344739793       0.00      0.00      0.00         1\n",
      "4474021137834556429       0.00      0.00      0.00         1\n",
      "4474050871881105968       0.00      0.00      0.00         1\n",
      "4474052236648906752       0.00      0.00      0.00         1\n",
      "4474062072124014592       0.00      0.00      0.00         1\n",
      "4474110278836944896       0.00      0.00      0.00         1\n",
      "4474122872723736908       0.00      0.00      0.00         1\n",
      "4474235056226828288       0.00      0.00      0.00         1\n",
      "4474248784005439692       0.00      0.00      0.00         1\n",
      "4474280137269398016       0.00      0.00      0.00         1\n",
      "4474289895414073284       0.00      0.00      0.00         1\n",
      "4474403247146139648       0.00      0.00      0.00         1\n",
      "4474438062151041024       0.00      0.00      0.00         1\n",
      "4474479908017405952       0.00      0.00      0.00         1\n",
      "4474537921199490012       0.00      0.00      0.00         1\n",
      "4474602276996600201       0.00      0.00      0.00         1\n",
      "4474663608063623168       0.00      0.00      0.00         1\n",
      "4474694631665606331       0.00      0.00      0.00         1\n",
      "4474706627512356446       0.00      0.00      0.00         1\n",
      "4474718971242766324       0.00      0.00      0.00         1\n",
      "4474843482362094307       0.00      0.00      0.00         1\n",
      "4474895829424034524       0.00      0.00      0.00         1\n",
      "4474996383175036988       0.00      0.00      0.00         1\n",
      "4475094857139879936       0.00      0.00      0.00         1\n",
      "4475096370015167474       0.00      0.00      0.00         1\n",
      "4475104204022314768       0.00      0.00      0.00         1\n",
      "4475159049721085952       0.00      0.00      0.00         1\n",
      "4475179786886155519       0.00      0.00      0.00         1\n",
      "4475260239150579712       0.00      0.00      0.00         1\n",
      "4475269731028303872       0.00      0.00      0.00         1\n",
      "4475505606337232896       0.00      0.00      0.00         1\n",
      "4475586082188687176       0.00      0.00      0.00         1\n",
      "4475627811041705984       0.00      0.00      0.00         1\n",
      "4475634305032257536       0.00      0.00      0.00         1\n",
      "4475782890485669818       0.00      0.00      0.00         1\n",
      "4475861492676825776       0.00      0.00      0.00         1\n",
      "4476007606409756672       0.00      0.00      0.00         1\n",
      "4476019237181194240       0.00      0.00      0.00         1\n",
      "4476077254670807434       0.00      0.00      0.00         1\n",
      "4476106137254494208       0.00      0.00      0.00         1\n",
      "4476167486567350272       0.00      0.00      0.00         1\n",
      "4476232646574167590       0.00      0.00      0.00         1\n",
      "4476266284750780125       0.00      0.00      0.00         1\n",
      "4476342476419891200       0.00      0.00      0.00         1\n",
      "4476391629082908444       0.00      0.00      0.00         1\n",
      "4476394257587386804       0.00      0.00      0.00         1\n",
      "4476407065179590656       0.00      0.00      0.00         1\n",
      "4476458878623547392       0.00      0.00      0.00         1\n",
      "4476495343907113600       0.00      0.00      0.00         1\n",
      "4476844077060456448       0.00      0.00      0.00         1\n",
      "4477054504688156672       0.00      0.00      0.00         1\n",
      "4477057116028272640       0.00      0.00      0.00         1\n",
      "4477091382338950921       0.00      0.00      0.00         1\n",
      "4477122408121106432       0.00      0.00      0.00         1\n",
      "4477159792577615433       0.00      0.00      0.00         1\n",
      "4477206881537884160       0.00      0.00      0.00         1\n",
      "4477213900563573452       0.00      0.00      0.00         1\n",
      "4477304537278589742       0.00      0.00      0.00         1\n",
      "4477382103318659072       0.00      0.00      0.00         1\n",
      "4477423665717182464       0.00      0.00      0.00         1\n",
      "4477555754189914892       0.00      0.00      0.00         1\n",
      "4477594596825628672       0.00      0.00      0.00         1\n",
      "4477744285025828864       0.00      0.00      0.00         1\n",
      "4477789158844137472       0.00      0.00      0.00         1\n",
      "4477823982438973440       0.00      0.00      0.00         1\n",
      "4478030489819172089       0.00      0.00      0.00         1\n",
      "4478096971627605800       0.00      0.00      0.00         1\n",
      "4478212419281223680       0.00      0.00      0.00         1\n",
      "4478242330477134988       0.00      0.00      0.00         1\n",
      "4478322971739422720       0.00      0.00      0.00         1\n",
      "4478324689726341120       0.00      0.00      0.00         1\n",
      "4478416670745952256       0.00      0.00      0.00         1\n",
      "4478499650535186400       0.00      0.00      0.00         1\n",
      "4478578281775366144       0.00      0.00      0.00         1\n",
      "4478621068239568896       0.00      0.00      0.00         1\n",
      "4478726338946860032       0.00      0.00      0.00         1\n",
      "4478798764972209410       0.00      0.00      0.00         1\n",
      "4478832733851530824       0.00      0.00      0.00         1\n",
      "4478962124068801608       0.00      0.00      0.00         1\n",
      "4479024580417028096       0.00      0.00      0.00         1\n",
      "4479163273500950528       0.00      0.00      0.00         1\n",
      "4479169768549240862       0.00      0.00      0.00         1\n",
      "4479182945508019508       0.00      0.00      0.00         1\n",
      "4479203775042551808       0.00      0.00      0.00         1\n",
      "4479294752103900166       0.00      0.00      0.00         1\n",
      "4479296521611018396       0.00      0.00      0.00         1\n",
      "4479465571512736332       0.00      0.00      0.00         1\n",
      "4479517333424963584       0.00      0.00      0.00         1\n",
      "4479536592058318848       0.00      0.00      0.00         1\n",
      "4479580357775065088       0.00      0.00      0.00         1\n",
      "4479582393589563392       0.00      0.00      0.00         1\n",
      "4479718270220488682       0.00      0.00      0.00         1\n",
      "4479760462933655552       0.00      0.00      0.00         1\n",
      "4479831897863210090       0.00      0.00      0.00         1\n",
      "4479904053337723545       0.00      0.00      0.00         1\n",
      "4479918261082579420       0.00      0.00      0.00         1\n",
      "4479959491718152192       0.00      0.00      0.00         1\n",
      "4480013549241056028       0.00      0.00      0.00         1\n",
      "4480144640224953048       0.00      0.00      0.00         1\n",
      "4480245450640719872       0.00      0.00      0.00         1\n",
      "4480249042265918144       0.00      0.00      0.00         1\n",
      "4480333910855959144       0.00      0.00      0.00         1\n",
      "4480424749403644328       0.00      0.00      0.00         1\n",
      "4480470095669532868       0.00      0.00      0.00         1\n",
      "4480500315055124726       0.00      0.00      0.00         1\n",
      "4480502633282404352       0.00      0.00      0.00         1\n",
      "4480572715277703168       0.00      0.00      0.00         1\n",
      "4480582588393586688       0.00      0.00      0.00         1\n",
      "4480631757179191296       0.00      0.00      0.00         1\n",
      "4480673848917934712       0.00      0.00      0.00         1\n",
      "4480730714281874656       0.00      0.00      0.00         1\n",
      "4480749954679177216       0.00      0.00      0.00         1\n",
      "4480886312355053287       0.00      0.00      0.00         1\n",
      "4480906739228183846       0.00      0.00      0.00         1\n",
      "4481026035176964096       0.00      0.00      0.00         1\n",
      "4481190473342377884       0.00      0.00      0.00         1\n",
      "4481209928496709632       0.00      0.00      0.00         1\n",
      "4481236187926757376       0.00      0.00      0.00         1\n",
      "4481241523329132130       0.00      0.00      0.00         1\n",
      "4481412063594612832       0.00      0.00      0.00         1\n",
      "4481451949903839232       0.00      0.00      0.00         1\n",
      "4481467386016301056       0.00      0.00      0.00         1\n",
      "4481479189630740884       0.00      0.00      0.00         1\n",
      "4481524268563169280       0.00      0.00      0.00         1\n",
      "4481538004926509360       0.00      0.00      0.00         1\n",
      "4481582234480953312       0.00      0.00      0.00         1\n",
      "4481609885497064006       0.00      0.00      0.00         1\n",
      "4481664065453686784       0.00      0.00      0.00         1\n",
      "4481808184139177533       0.00      0.00      0.00         1\n",
      "4481811958357557248       0.00      0.00      0.00         1\n",
      "4481972277635196200       0.00      0.00      0.00         1\n",
      "4482027295132876800       0.00      0.00      0.00         1\n",
      "4482150114017673216       0.00      0.00      0.00         1\n",
      "4482174732770213888       0.00      0.00      0.00         1\n",
      "4482211120768602662       0.00      0.00      0.00         1\n",
      "4482291624600141824       0.00      0.00      0.00         1\n",
      "4482324400554257568       0.00      0.00      0.00         1\n",
      "4482341626609401856       0.00      0.00      0.00         1\n",
      "4482389877326026679       0.00      0.00      0.00         1\n",
      "4482453476147724288       0.00      0.00      0.00         1\n",
      "4482547141849263178       0.00      0.00      0.00         1\n",
      "4482575135391350784       0.00      0.00      0.00         1\n",
      "4482602735909956611       0.00      0.00      0.00         1\n",
      "4482620052159332352       0.00      0.00      0.00         1\n",
      "4482653399351385886       0.00      0.00      0.00         1\n",
      "4482675371338104832       0.00      0.00      0.00         1\n",
      "4482703804021604352       0.00      0.00      0.00         1\n",
      "4482835333100077056       0.00      0.00      0.00         1\n",
      "4482869074363154432       0.00      0.00      0.00         1\n",
      "4482933619131678720       0.00      0.00      0.00         1\n",
      "4482936818882314240       0.00      0.00      0.00         1\n",
      "4482942363685093376       0.00      0.00      0.00         1\n",
      "4483038864070745852       0.00      0.00      0.00         1\n",
      "4483154964566245376       0.00      0.00      0.00         1\n",
      "4483252571993014272       0.00      0.00      0.00         1\n",
      "4483295349867282432       0.00      0.00      0.00         1\n",
      "4483332588294702182       0.00      0.00      0.00         1\n",
      "4483367927283653152       0.00      0.00      0.00         1\n",
      "4483374524351451787       0.00      0.00      0.00         1\n",
      "4483446748515151576       0.00      0.00      0.00         1\n",
      "4483605369196642304       0.00      0.00      0.00         1\n",
      "4483668420382295676       0.00      0.00      0.00         1\n",
      "4483682627068362752       0.00      0.00      0.00         1\n",
      "4483860318455332864       0.00      0.00      0.00         1\n",
      "4483907064879382528       0.00      0.00      0.00         1\n",
      "4483933196526698056       0.00      0.00      0.00         1\n",
      "4483937748125745152       0.00      0.00      0.00         1\n",
      "4484115904432022020       0.00      0.00      0.00         1\n",
      "4484145641722740736       0.00      0.00      0.00         1\n",
      "4484219309001801728       0.00      0.00      0.00         1\n",
      "4484330819237707776       0.00      0.00      0.00         1\n",
      "4484337919875792301       0.00      0.00      0.00         1\n",
      "4484340840464655649       0.00      0.00      0.00         1\n",
      "4484361559363799520       0.00      0.00      0.00         1\n",
      "4484451774106697728       0.00      0.00      0.00         1\n",
      "4484644704037634048       0.00      0.00      0.00         1\n",
      "4484834215174602752       0.00      0.00      0.00         1\n",
      "4485001800503525376       0.00      0.00      0.00         1\n",
      "4485140314249553768       0.00      0.00      0.00         1\n",
      "4485176172893533696       0.00      0.00      0.00         1\n",
      "4485177843681173554       0.00      0.00      0.00         1\n",
      "4485269797872861184       0.00      0.00      0.00         1\n",
      "4485452562666375580       0.00      0.00      0.00         1\n",
      "4485525614714945536       0.00      0.00      0.00         1\n",
      "4485537980965979054       0.00      0.00      0.00         1\n",
      "4485642674048598016       0.00      0.00      0.00         1\n",
      "4485668370837929984       0.00      0.00      0.00         1\n",
      "4485833357711638528       0.00      0.00      0.00         1\n",
      "4485893332634959872       0.00      0.00      0.00         1\n",
      "4485949279930417828       0.00      0.00      0.00         1\n",
      "4485959956167655424       0.00      0.00      0.00         1\n",
      "4485960454383861760       0.00      0.00      0.00         1\n",
      "4485974525762474386       0.00      0.00      0.00         1\n",
      "4485975259136131072       0.00      0.00      0.00         1\n",
      "4486182638386258566       0.00      0.00      0.00         1\n",
      "4486259075933301150       0.00      0.00      0.00         1\n",
      "4486435443407060992       0.00      0.00      0.00         1\n",
      "4486541151142150144       0.00      0.00      0.00         1\n",
      "4486584892157761440       0.00      0.00      0.00         1\n",
      "4486724976794244875       0.00      0.00      0.00         1\n",
      "4486921650884837376       0.00      0.00      0.00         1\n",
      "4487011348034160082       0.00      0.00      0.00         1\n",
      "4487045449022177280       0.00      0.00      0.00         1\n",
      "4487179099814494208       0.00      0.00      0.00         1\n",
      "4487236649138398936       0.00      0.00      0.00         1\n",
      "4487267842428764160       0.00      0.00      0.00         1\n",
      "4487348089597722624       0.00      0.00      0.00         1\n",
      "4487419063932289024       0.00      0.00      0.00         1\n",
      "4487655712335331328       0.00      0.00      0.00         1\n",
      "4487706882575695872       0.00      0.00      0.00         1\n",
      "4487815382039527424       0.00      0.00      0.00         1\n",
      "4487862490299119408       0.00      0.00      0.00         1\n",
      "4487918658823127040       0.00      0.00      0.00         1\n",
      "4487944884951583342       0.00      0.00      0.00         1\n",
      "4488168033214267392       0.00      0.00      0.00         1\n",
      "4488202307053289472       0.00      0.00      0.00         1\n",
      "4488282098955714560       0.00      0.00      0.00         1\n",
      "4488326930882018532       0.00      0.00      0.00         1\n",
      "4488407736388311348       0.00      0.00      0.00         1\n",
      "4488421969904715843       0.00      0.00      0.00         1\n",
      "4488442121911551954       0.00      0.00      0.00         1\n",
      "4488442138027098112       0.00      0.00      0.00         1\n",
      "4488520753108484096       0.00      0.00      0.00         1\n",
      "4488540853555429376       0.00      0.00      0.00         1\n",
      "4488684985130529308       0.00      0.00      0.00         1\n",
      "4488816495966552064       0.00      0.00      0.00         1\n",
      "4488860072704737280       0.00      0.00      0.00         1\n",
      "4488913862875152384       0.00      0.00      0.00         1\n",
      "4488915882570180849       0.00      0.00      0.00         1\n",
      "4488944383981849846       0.00      0.00      0.00         1\n",
      "4488982359013588992       0.00      0.00      0.00         1\n",
      "4489069230088763098       0.00      0.00      0.00         1\n",
      "4489135362928541696       0.00      0.00      0.00         1\n",
      "4489253887909561606       0.00      0.00      0.00         1\n",
      "4489367756073292100       0.00      0.00      0.00         1\n",
      "4489406977692981284       0.00      0.00      0.00         1\n",
      "4489597090159084202       0.00      0.00      0.00         1\n",
      "4489651077883295114       0.00      0.00      0.00         1\n",
      "4489784641724612608       0.00      0.00      0.00         1\n",
      "4489790328261312512       0.00      0.00      0.00         1\n",
      "4489803540625408256       0.00      0.00      0.00         1\n",
      "4489818040440674734       0.00      0.00      0.00         1\n",
      "4489831186285199360       0.00      0.00      0.00         1\n",
      "4489861470099603456       0.00      0.00      0.00         1\n",
      "4489862347326171462       0.00      0.00      0.00         1\n",
      "4489930826285026612       0.00      0.00      0.00         1\n",
      "4490025159893188608       0.00      0.00      0.00         1\n",
      "4490178800518655900       0.00      0.00      0.00         1\n",
      "4490221061941493760       0.00      0.00      0.00         1\n",
      "4490242124461113344       0.00      0.00      0.00         1\n",
      "4490295387409577462       0.00      0.00      0.00         1\n",
      "4490333908974860355       0.00      0.00      0.00         1\n",
      "4490561275958165849       0.00      0.00      0.00         1\n",
      "4490805297752834048       0.00      0.00      0.00         1\n",
      "4490808777732281194       0.00      0.00      0.00         1\n",
      "4490814421332764182       0.00      0.00      0.00         1\n",
      "4490864190344396800       0.00      0.00      0.00         1\n",
      "4490901629574316032       0.00      0.00      0.00         1\n",
      "4490906462468743289       0.00      0.00      0.00         1\n",
      "4490922816647987200       0.00      0.00      0.00         1\n",
      "4491004860179564783       0.00      0.00      0.00         1\n",
      "4491085226541318144       0.00      0.00      0.00         1\n",
      "4491149007874880984       0.00      0.00      0.00         1\n",
      "4491149358992982016       0.00      0.00      0.00         1\n",
      "4491334266992435983       0.00      0.00      0.00         1\n",
      "4491522514341593088       0.00      0.00      0.00         1\n",
      "4491597247822384407       0.00      0.00      0.00         1\n",
      "4491606360693145600       0.00      0.00      0.00         1\n",
      "4491619030846668800       0.00      0.00      0.00         1\n",
      "4491875680912408576       0.00      0.00      0.00         1\n",
      "4492137725457072128       0.00      0.00      0.00         1\n",
      "4492167266242134016       0.00      0.00      0.00         1\n",
      "4492173760232685568       0.00      0.00      0.00         1\n",
      "4492301114602946560       0.00      0.00      0.00         1\n",
      "4492337433905916095       0.00      0.00      0.00         1\n",
      "4492389762727936000       0.00      0.00      0.00         1\n",
      "4492437625843482624       0.00      0.00      0.00         1\n",
      "4492465766469206016       0.00      0.00      0.00         1\n",
      "4492486349002324923       0.00      0.00      0.00         1\n",
      "4492556751056404480       0.00      0.00      0.00         1\n",
      "4492567592610970650       0.00      0.00      0.00         1\n",
      "4492593928293318656       0.00      0.00      0.00         1\n",
      "4492662288055028410       0.00      0.00      0.00         1\n",
      "4492697566897671680       0.00      0.00      0.00         1\n",
      "4492709549866714140       0.00      0.00      0.00         1\n",
      "4492829808915218992       0.00      0.00      0.00         1\n",
      "4492947061562899094       0.00      0.00      0.00         1\n",
      "4492951819328159744       0.00      0.00      0.00         1\n",
      "4492963793696980992       0.00      0.00      0.00         1\n",
      "4492968673841724730       0.00      0.00      0.00         1\n",
      "4492975059396198400       0.00      0.00      0.00         1\n",
      "4493246591523618816       0.00      0.00      0.00         1\n",
      "4493292935279064916       0.00      0.00      0.00         1\n",
      "4493367692421496832       0.00      0.00      0.00         1\n",
      "4493421675865440256       0.00      0.00      0.00         1\n",
      "4493450696959459328       0.00      0.00      0.00         1\n",
      "4493458865987256320       0.00      0.00      0.00         1\n",
      "4493472851434945168       0.00      0.00      0.00         1\n",
      "4493481063437291018       0.00      0.00      0.00         1\n",
      "4493595656400666624       0.00      0.00      0.00         1\n",
      "4493683420811872602       0.00      0.00      0.00         1\n",
      "4493701408142829630       0.00      0.00      0.00         1\n",
      "4493717956643027604       0.00      0.00      0.00         1\n",
      "4493954437549718540       0.00      0.00      0.00         1\n",
      "4493963491318993040       0.00      0.00      0.00         1\n",
      "4493969624564178384       0.00      0.00      0.00         1\n",
      "4493969835016970093       0.00      0.00      0.00         1\n",
      "4493970886218481664       0.00      0.00      0.00         1\n",
      "4493972080219389952       0.00      0.00      0.00         1\n",
      "4493975551594156864       0.00      0.00      0.00         1\n",
      "4494084453743722496       0.00      0.00      0.00         1\n",
      "4494280605955766824       0.00      0.00      0.00         1\n",
      "4494498419871580160       0.00      0.00      0.00         1\n",
      "4494505669776375808       0.00      0.00      0.00         1\n",
      "4494618697201854283       0.00      0.00      0.00         1\n",
      "4494623489319239680       0.00      0.00      0.00         1\n",
      "4494740966330557540       0.00      0.00      0.00         1\n",
      "4494760700639444992       0.00      0.00      0.00         1\n",
      "4494859679213347721       0.00      0.00      0.00         1\n",
      "4494880398151428388       0.00      0.00      0.00         1\n",
      "4494890813434772514       0.00      0.00      0.00         1\n",
      "4494945595760663087       0.00      0.00      0.00         1\n",
      "4494956745484582026       0.00      0.00      0.00         1\n",
      "4495069763240243364       0.00      0.00      0.00         1\n",
      "4495086057297018880       0.00      0.00      0.00         1\n",
      "4495117341838802944       0.00      0.00      0.00         1\n",
      "4495157792892460966       0.00      0.00      0.00         1\n",
      "4495194032774840320       0.00      0.00      0.00         1\n",
      "4495199083656380416       0.00      0.00      0.00         1\n",
      "4495206105927909376       0.00      0.00      0.00         1\n",
      "4495347517726130176       0.00      0.00      0.00         1\n",
      "4495429938148540416       0.00      0.00      0.00         1\n",
      "4495494100664975360       0.00      0.00      0.00         1\n",
      "4495638613429583872       0.00      0.00      0.00         1\n",
      "4495669631683395584       0.00      0.00      0.00         1\n",
      "4495796102355752559       0.00      0.00      0.00         1\n",
      "4495800809633885896       0.00      0.00      0.00         1\n",
      "4495803299655581696       0.00      0.00      0.00         1\n",
      "4496019526554016574       0.00      0.00      0.00         1\n",
      "4496055123238382910       0.00      0.00      0.00         1\n",
      "4496150504811790336       0.00      0.00      0.00         1\n",
      "4496286354627362816       0.00      0.00      0.00         1\n",
      "4496403960476245247       0.00      0.00      0.00         1\n",
      "4496508430206369792       0.00      0.00      0.00         1\n",
      "4496510269518725801       0.00      0.00      0.00         1\n",
      "4496519683020685312       0.00      0.00      0.00         1\n",
      "4496592080002183424       0.00      0.00      0.00         1\n",
      "4496652072145845520       0.00      0.00      0.00         1\n",
      "4496668185809911808       0.00      0.00      0.00         1\n",
      "4496752083701071872       0.00      0.00      0.00         1\n",
      "4496791589867804075       0.00      0.00      0.00         1\n",
      "4496805995184476178       0.00      0.00      0.00         1\n",
      "4496821603103871320       0.00      0.00      0.00         1\n",
      "4496827511916724224       0.00      0.00      0.00         1\n",
      "4496882213669752995       0.00      0.00      0.00         1\n",
      "4496949249513654296       0.00      0.00      0.00         1\n",
      "4497093593730646016       0.00      0.00      0.00         1\n",
      "4497127387563775296       0.00      0.00      0.00         1\n",
      "4497137652555429711       0.00      0.00      0.00         1\n",
      "4497157365405057024       0.00      0.00      0.00         1\n",
      "4497188444854761647       0.00      0.00      0.00         1\n",
      "4497193006084155616       0.00      0.00      0.00         1\n",
      "4497229366236807168       0.00      0.00      0.00         1\n",
      "4497331501613947480       0.00      0.00      0.00         1\n",
      "4497411850807279616       0.00      0.00      0.00         1\n",
      "4497436986014843592       0.00      0.00      0.00         1\n",
      "4497505215863711302       0.00      0.00      0.00         1\n",
      "4497536199750298952       0.00      0.00      0.00         1\n",
      "4497773392564322304       0.00      0.00      0.00         1\n",
      "4497819650341730304       0.00      0.00      0.00         1\n",
      "4497967490726363136       0.00      0.00      0.00         1\n",
      "4498030035101637783       0.00      0.00      0.00         1\n",
      "4498105376356433920       0.00      0.00      0.00         1\n",
      "4498144907235426304       0.00      0.00      0.00         1\n",
      "4498185272387790508       0.00      0.00      0.00         1\n",
      "4498544356373823488       0.00      0.00      0.00         1\n",
      "4498585215438527716       0.00      0.00      0.00         1\n",
      "4498612371475922944       0.00      0.00      0.00         1\n",
      "4498639349226821865       0.00      0.00      0.00         1\n",
      "4498643575464689895       0.00      0.00      0.00         1\n",
      "4498670534979295868       0.00      0.00      0.00         1\n",
      "4498701905413666706       0.00      0.00      0.00         1\n",
      "4498735028201598206       0.00      0.00      0.00         1\n",
      "4498762172409264698       0.00      0.00      0.00         1\n",
      "4498909249262556299       0.00      0.00      0.00         1\n",
      "4499217456099481886       0.00      0.00      0.00         1\n",
      "4499248946819618738       0.00      0.00      0.00         1\n",
      "4499266315663015016       0.00      0.00      0.00         1\n",
      "4499477129839646344       0.00      0.00      0.00         1\n",
      "4499525181913385632       0.00      0.00      0.00         1\n",
      "4499628380349530112       0.00      0.00      0.00         1\n",
      "4499700674299746067       0.00      0.00      0.00         1\n",
      "4499777561743589376       0.00      0.00      0.00         1\n",
      "4499837064220508160       0.00      0.00      0.00         1\n",
      "4499842481224345587       0.00      0.00      0.00         1\n",
      "4499898324398335070       0.00      0.00      0.00         1\n",
      "4499912569745571840       0.00      0.00      0.00         1\n",
      "4499933602200420352       0.00      0.00      0.00         1\n",
      "4499934406420612004       0.00      0.00      0.00         1\n",
      "4500121562854195200       0.00      0.00      0.00         1\n",
      "4500134517517464344       0.00      0.00      0.00         1\n",
      "4500193340347645952       0.00      0.00      0.00         1\n",
      "4500244136925855744       0.00      0.00      0.00         1\n",
      "4500350467431202816       0.00      0.00      0.00         1\n",
      "4500415841128415232       0.00      0.00      0.00         1\n",
      "4500538656754314000       0.00      0.00      0.00         1\n",
      "4500583599305819163       0.00      0.00      0.00         1\n",
      "4500679037735294624       0.00      0.00      0.00         1\n",
      "4500763712004554752       0.00      0.00      0.00         1\n",
      "4500826014800150528       0.00      0.00      0.00         1\n",
      "4500891586065858560       0.00      0.00      0.00         1\n",
      "4501182128763117936       0.00      0.00      0.00         1\n",
      "4501196266750869504       0.00      0.00      0.00         1\n",
      "4501230377381134336       0.00      0.00      0.00         1\n",
      "4501296984796361590       0.00      0.00      0.00         1\n",
      "4501394505261383680       0.00      0.00      0.00         1\n",
      "4501409980028551168       0.00      0.00      0.00         1\n",
      "4501456491274765608       0.00      0.00      0.00         1\n",
      "4501473601379106816       0.00      0.00      0.00         1\n",
      "4501507326534193553       0.00      0.00      0.00         1\n",
      "4501549881051980379       0.00      0.00      0.00         1\n",
      "4501573984406602855       0.00      0.00      0.00         1\n",
      "4501777050475954779       0.00      0.00      0.00         1\n",
      "4501827377835278336       0.00      0.00      0.00         1\n",
      "4501836424098194785       0.00      0.00      0.00         1\n",
      "4501877130736435200       0.00      0.00      0.00         1\n",
      "4501959456669564928       0.00      0.00      0.00         1\n",
      "4501979231729168256       0.00      0.00      0.00         1\n",
      "4501987807748685824       0.00      0.00      0.00         1\n",
      "4502015591892123648       0.00      0.00      0.00         1\n",
      "4502026948827458596       0.00      0.00      0.00         1\n",
      "4502182064824516608       0.00      0.00      0.00         1\n",
      "4502259631933882368       0.00      0.00      0.00         1\n",
      "4502324898256912384       0.00      0.00      0.00         1\n",
      "4502351193109405080       0.00      0.00      0.00         1\n",
      "4502487686107365376       0.00      0.00      0.00         1\n",
      "4502553519366078464       0.00      0.00      0.00         1\n",
      "4502636318781938400       0.00      0.00      0.00         1\n",
      "4502662791924023296       0.00      0.00      0.00         1\n",
      "4502670128792617073       0.00      0.00      0.00         1\n",
      "4502689270397403136       0.00      0.00      0.00         1\n",
      "4502720036308069834       0.00      0.00      0.00         1\n",
      "4502745650433097728       0.00      0.00      0.00         1\n",
      "4502760596919287808       0.00      0.00      0.00         1\n",
      "4502790421172191232       0.00      0.00      0.00         1\n",
      "4502814594322359311       0.00      0.00      0.00         1\n",
      "4502922774884384768       0.00      0.00      0.00         1\n",
      "4502967610047987712       0.00      0.00      0.00         1\n",
      "4502974899634549808       0.00      0.00      0.00         1\n",
      "4503005600093569667       0.00      0.00      0.00         1\n",
      "4503022470731167314       0.00      0.00      0.00         1\n",
      "4503207599935586304       0.00      0.00      0.00         1\n",
      "4503210194095833088       0.00      0.00      0.00         1\n",
      "4503259754785262107       0.00      0.00      0.00         1\n",
      "4503402850213853006       0.00      0.00      0.00         1\n",
      "4503606474532078080       0.00      0.00      0.00         1\n",
      "4503641512966228469       0.00      0.00      0.00         1\n",
      "4503646081736769536       0.00      0.00      0.00         1\n",
      "4503814684972941312       0.00      0.00      0.00         1\n",
      "4503824590212736104       0.00      0.00      0.00         1\n",
      "4503841983785074688       0.00      0.00      0.00         1\n",
      "4503870743926721024       0.00      0.00      0.00         1\n",
      "4503875399681617687       0.00      0.00      0.00         1\n",
      "4503888001110722663       0.00      0.00      0.00         1\n",
      "4503890294600556384       0.00      0.00      0.00         1\n",
      "4503908426929143808       0.00      0.00      0.00         1\n",
      "4503913026839117824       0.00      0.00      0.00         1\n",
      "4503942186432293738       0.00      0.00      0.00         1\n",
      "4503964648051048448       0.00      0.00      0.00         1\n",
      "4504016230608273408       0.00      0.00      0.00         1\n",
      "4504083893523054592       0.00      0.00      0.00         1\n",
      "4504132453491531342       0.00      0.00      0.00         1\n",
      "4504134814655315968       0.00      0.00      0.00         1\n",
      "4504134849015054336       0.00      0.00      0.00         1\n",
      "4504136894470177087       0.00      0.00      0.00         1\n",
      "4504156793054882897       0.00      0.00      0.00         1\n",
      "4504157101240614912       0.00      0.00      0.00         1\n",
      "4504164047263777012       0.00      0.00      0.00         1\n",
      "4504171270337724416       0.00      0.00      0.00         1\n",
      "4504175702743973888       0.00      0.00      0.00         1\n",
      "4504213111909122048       0.00      0.00      0.00         1\n",
      "4504305805893304320       0.00      0.00      0.00         1\n",
      "4504320873698212350       0.00      0.00      0.00         1\n",
      "4504436776626028544       0.00      0.00      0.00         1\n",
      "4504465007446065152       0.00      0.00      0.00         1\n",
      "4504532588756467712       0.00      0.00      0.00         1\n",
      "4504568899468890428       0.00      0.00      0.00         1\n",
      "4504580185584041984       0.00      0.00      0.00         1\n",
      "4504591318139273216       0.00      0.00      0.00         1\n",
      "4504627284195409920       0.00      0.00      0.00         1\n",
      "4504638270721753088       0.00      0.00      0.00         1\n",
      "4504707115758778624       0.00      0.00      0.00         1\n",
      "4504719308164694016       0.00      0.00      0.00         1\n",
      "4504723521527611392       0.00      0.00      0.00         1\n",
      "4504768769008074752       0.00      0.00      0.00         1\n",
      "4504792481522515968       0.00      0.00      0.00         1\n",
      "4504812762358087680       0.00      0.00      0.00         1\n",
      "4504838138082010097       0.00      0.00      0.00         1\n",
      "4504858636903776256       0.00      0.00      0.00         1\n",
      "4504907977488072704       0.00      0.00      0.00         1\n",
      "4504989479851657974       0.00      0.00      0.00         1\n",
      "4505018564306010112       0.00      0.00      0.00         1\n",
      "4505081744340798624       0.00      0.00      0.00         1\n",
      "4505102947528474624       0.00      0.00      0.00         1\n",
      "4505113028844915232       0.00      0.00      0.00         1\n",
      "4505149394362421217       0.00      0.00      0.00         1\n",
      "4505177091548905472       0.00      0.00      0.00         1\n",
      "4505213701850136576       0.00      0.00      0.00         1\n",
      "4505246455270735872       0.00      0.00      0.00         1\n",
      "4505247573022328853       0.00      0.00      0.00         1\n",
      "4505265318767099904       0.00      0.00      0.00         1\n",
      "4505312803925524480       0.00      0.00      0.00         1\n",
      "4505373200800784338       0.00      0.00      0.00         1\n",
      "4505387390327586816       0.00      0.00      0.00         1\n",
      "4505440927094931456       0.00      0.00      0.00         1\n",
      "4505481986982281216       0.00      0.00      0.00         1\n",
      "4505538329423760390       0.00      0.00      0.00         1\n",
      "4505601508378388524       0.00      0.00      0.00         1\n",
      "4505623309656162045       0.00      0.00      0.00         1\n",
      "4505633072103979362       0.00      0.00      0.00         1\n",
      "4505640055701478720       0.00      0.00      0.00         1\n",
      "4505652115997884314       0.00      0.00      0.00         1\n",
      "4505674027854987264       0.00      0.00      0.00         1\n",
      "4505707838890923818       0.00      0.00      0.00         1\n",
      "4505707983866429440       0.00      0.00      0.00         1\n",
      "4505747755263590400       0.00      0.00      0.00         1\n",
      "4505803787406934016       0.00      0.00      0.00         1\n",
      "4505889318435664309       0.00      0.00      0.00         1\n",
      "4505914889620946944       0.00      0.00      0.00         1\n",
      "4505941459347251200       0.00      0.00      0.00         1\n",
      "4505948063948341248       0.00      0.00      0.00         1\n",
      "4505983180648422908       0.00      0.00      0.00         1\n",
      "4505992572694429696       0.00      0.00      0.00         1\n",
      "4506011474845499392       0.00      0.00      0.00         1\n",
      "4506018840714412032       0.00      0.00      0.00         1\n",
      "4506033890279817216       0.00      0.00      0.00         1\n",
      "4506041214238736957       0.00      0.00      0.00         1\n",
      "4506125214169432064       0.00      0.00      0.00         1\n",
      "4506191228869770242       0.00      0.00      0.00         1\n",
      "4506247531608385481       0.00      0.00      0.00         1\n",
      "4506265016417322085       0.00      0.00      0.00         1\n",
      "4506279009408987332       0.00      0.00      0.00         1\n",
      "4506311041295331775       0.00      0.00      0.00         1\n",
      "4506329578368559139       0.00      0.00      0.00         1\n",
      "4506394277734537848       0.00      0.00      0.00         1\n",
      "4506403429265965056       0.00      0.00      0.00         1\n",
      "4506488066934247700       0.00      0.00      0.00         1\n",
      "4506542835314458624       0.00      0.00      0.00         1\n",
      "4506545725827448832       0.00      0.00      0.00         1\n",
      "4506564537784205312       0.00      0.00      0.00         1\n",
      "4506576311355192557       0.00      0.00      0.00         1\n",
      "4506609687534031437       0.00      0.00      0.00         1\n",
      "4506669490664794028       0.00      0.00      0.00         1\n",
      "4506694659172475138       0.00      0.00      0.00         1\n",
      "4506707312142009724       0.00      0.00      0.00         1\n",
      "4506715471524921344       0.00      0.00      0.00         1\n",
      "4506756429382758664       0.00      0.00      0.00         1\n",
      "4506781845949513728       0.00      0.00      0.00         1\n",
      "4506843634373833984       0.00      0.00      0.00         1\n",
      "4506849491684425728       0.00      0.00      0.00         1\n",
      "4506856523615302209       0.00      0.00      0.00         1\n",
      "4506984053009809408       0.00      0.00      0.00         1\n",
      "4507053714135637720       0.00      0.00      0.00         1\n",
      "4507067007072342254       0.00      0.00      0.00         1\n",
      "4507131096564115646       0.00      0.00      0.00         1\n",
      "4507187815900486508       0.00      0.00      0.00         1\n",
      "4507236589568776518       0.00      0.00      0.00         1\n",
      "4507272374164389888       0.00      0.00      0.00         1\n",
      "4507295533686420358       0.00      0.00      0.00         1\n",
      "4507297045516397516       0.00      0.00      0.00         1\n",
      "4507301064545927168       0.00      0.00      0.00         1\n",
      "4507328158257489947       0.00      0.00      0.00         1\n",
      "4507361388411797960       0.00      0.00      0.00         1\n",
      "4507380561145556775       0.00      0.00      0.00         1\n",
      "4507394086008957488       0.00      0.00      0.00         1\n",
      "4507398569951117526       0.00      0.00      0.00         1\n",
      "4507412957033922560       0.00      0.00      0.00         1\n",
      "4507474749794948905       0.00      0.00      0.00         1\n",
      "4507482828598110384       0.00      0.00      0.00         1\n",
      "4507485301463056384       0.00      0.00      0.00         1\n",
      "4507497447630569472       0.00      0.00      0.00         1\n",
      "4507503151347138560       0.00      0.00      0.00         1\n",
      "4507525761091539672       0.00      0.00      0.00         1\n",
      "4507554068184432640       0.00      0.00      0.00         1\n",
      "4507561361038901248       0.00      0.00      0.00         1\n",
      "4507567176424620032       0.00      0.00      0.00         1\n",
      "4507577197640953159       0.00      0.00      0.00         1\n",
      "4507587565693096128       0.00      0.00      0.00         1\n",
      "4507589342750834688       0.00      0.00      0.00         1\n",
      "4507597451649089536       0.00      0.00      0.00         1\n",
      "4507705256396928052       0.00      0.00      0.00         1\n",
      "4507720742980288512       0.00      0.00      0.00         1\n",
      "4507755889745622020       0.00      0.00      0.00         1\n",
      "4507786736202930630       0.00      0.00      0.00         1\n",
      "4507816267347918848       0.00      0.00      0.00         1\n",
      "4507824261348017760       0.00      0.00      0.00         1\n",
      "4507847221177221120       0.00      0.00      0.00         1\n",
      "4507869997388791808       0.00      0.00      0.00         1\n",
      "4507912230862860350       0.00      0.00      0.00         1\n",
      "4507966470944194560       0.00      0.00      0.00         1\n",
      "4507991928271719276       0.00      0.00      0.00         1\n",
      "4508009205868789760       0.00      0.00      0.00         1\n",
      "4508009656840355840       0.00      0.00      0.00         1\n",
      "4508010524423749632       0.00      0.00      0.00         1\n",
      "4508066059391376784       0.00      0.00      0.00         1\n",
      "4508109189465626321       0.00      0.00      0.00         1\n",
      "4508120261872981952       0.00      0.00      0.00         1\n",
      "4508155110202802176       0.00      0.00      0.00         1\n",
      "4508155393670643712       0.00      0.00      0.00         1\n",
      "4508221219386029476       0.00      0.00      0.00         1\n",
      "4508224791752212480       0.00      0.00      0.00         1\n",
      "4508264365580877824       0.00      0.00      0.00         1\n",
      "4508299627262377984       0.00      0.00      0.00         1\n",
      "4508306413310705664       0.00      0.00      0.00         1\n",
      "4508416348357723636       0.00      0.00      0.00         1\n",
      "4508453783295800960       0.00      0.00      0.00         1\n",
      "4508496200391067596       0.00      0.00      0.00         1\n",
      "4508503449230376960       0.00      0.00      0.00         1\n",
      "4508599056262510256       0.00      0.00      0.00         1\n",
      "4508607818003265405       0.00      0.00      0.00         1\n",
      "4508634007646240768       0.00      0.00      0.00         1\n",
      "4508643774401871872       0.00      0.00      0.00         1\n",
      "4508675428310843392       0.00      0.00      0.00         1\n",
      "4508699163364815284       0.00      0.00      0.00         1\n",
      "4508754567378239488       0.00      0.00      0.00         1\n",
      "4508792990155669504       0.00      0.00      0.00         1\n",
      "4508815603158482944       0.00      0.00      0.00         1\n",
      "4508819202341076992       0.00      0.00      0.00         1\n",
      "4508849805033760194       0.00      0.00      0.00         1\n",
      "4508861602258223104       0.00      0.00      0.00         1\n",
      "4508868986341913824       0.00      0.00      0.00         1\n",
      "4508873624929808556       0.00      0.00      0.00         1\n",
      "4508890753264739648       0.00      0.00      0.00         1\n",
      "4508911558069619504       0.00      0.00      0.00         1\n",
      "4508929028949803008       0.00      0.00      0.00         1\n",
      "4508938959962797973       0.00      0.00      0.00         1\n",
      "4508946897080797716       0.00      0.00      0.00         1\n",
      "4509048254002129482       0.00      0.00      0.00         1\n",
      "4509067275357126656       0.00      0.00      0.00         1\n",
      "4509126150768820224       0.00      0.00      0.00         1\n",
      "4509163603938509892       0.00      0.00      0.00         1\n",
      "4509179701480269787       0.00      0.00      0.00         1\n",
      "4509202072905711616       0.00      0.00      0.00         1\n",
      "4509219471818227712       0.00      0.00      0.00         1\n",
      "4509361902572970388       0.00      0.00      0.00         1\n",
      "4509376676211195904       0.00      0.00      0.00         1\n",
      "4509476930387739212       0.00      0.00      0.00         1\n",
      "4509486764812926976       0.00      0.00      0.00         1\n",
      "4509539017385050112       0.00      0.00      0.00         1\n",
      "4509584111311995721       0.00      0.00      0.00         1\n",
      "4509637802698738628       0.00      0.00      0.00         1\n",
      "4509722585352837894       0.00      0.00      0.00         1\n",
      "4509731922611344971       0.00      0.00      0.00         1\n",
      "4509779957508180682       0.00      0.00      0.00         1\n",
      "4509847460461412352       0.00      0.00      0.00         1\n",
      "4509859818116707136       0.00      0.00      0.00         1\n",
      "4509863494623309243       0.00      0.00      0.00         1\n",
      "4509882670603304960       0.00      0.00      0.00         1\n",
      "4509904128259915776       0.00      0.00      0.00         1\n",
      "4509922317446414336       0.00      0.00      0.00         1\n",
      "4509966882027077632       0.00      0.00      0.00         1\n",
      "4509982637009692357       0.00      0.00      0.00         1\n",
      "4509984989609197568       0.00      0.00      0.00         1\n",
      "4510000572812378039       0.00      0.00      0.00         1\n",
      "4510008260781718064       0.00      0.00      0.00         1\n",
      "4510027150071367564       0.00      0.00      0.00         1\n",
      "4510085835441307648       0.00      0.00      0.00         1\n",
      "4510109235478994910       0.00      0.00      0.00         1\n",
      "4510144342542845358       0.00      0.00      0.00         1\n",
      "4510161062838231984       0.00      0.00      0.00         1\n",
      "4510173555853361152       0.00      0.00      0.00         1\n",
      "4510205013249199964       0.00      0.00      0.00         1\n",
      "4510260865013370334       0.00      0.00      0.00         1\n",
      "4510270351531311104       0.00      0.00      0.00         1\n",
      "4510311369534928796       0.00      0.00      0.00         1\n",
      "4510323438382035100       0.00      0.00      0.00         1\n",
      "4510328063006867456       0.00      0.00      0.00         1\n",
      "4510375678080614254       0.00      0.00      0.00         1\n",
      "4510400209867505664       0.00      0.00      0.00         1\n",
      "4510408026707984384       0.00      0.00      0.00         1\n",
      "4510413472726515712       0.00      0.00      0.00         1\n",
      "4510417880426562384       0.00      0.00      0.00         1\n",
      "4510457517616136192       0.00      0.00      0.00         1\n",
      "4510457630354408190       0.00      0.00      0.00         1\n",
      "4510460919230234624       0.00      0.00      0.00         1\n",
      "4510492067397109828       0.00      0.00      0.00         1\n",
      "4510688402173067264       0.00      0.00      0.00         1\n",
      "4510691546089127936       0.00      0.00      0.00         1\n",
      "4510696460595677348       0.00      0.00      0.00         1\n",
      "4510707661856691733       0.00      0.00      0.00         1\n",
      "4510737905966120960       0.00      0.00      0.00         1\n",
      "4510740097444289408       0.00      0.00      0.00         1\n",
      "4510760523263901696       0.00      0.00      0.00         1\n",
      "4510830841531730052       0.00      0.00      0.00         1\n",
      "4510885714022235908       0.00      0.00      0.00         1\n",
      "4510913708599454916       0.00      0.00      0.00         1\n",
      "4510929655833202868       0.00      0.00      0.00         1\n",
      "4510943660669403136       0.00      0.00      0.00         1\n",
      "4510953848331829248       0.00      0.00      0.00         1\n",
      "4510958645810298880       0.00      0.00      0.00         1\n",
      "4510996094699363432       0.00      0.00      0.00         1\n",
      "4511026562128150528       0.00      0.00      0.00         1\n",
      "4511031355311652864       0.00      0.00      0.00         1\n",
      "4511038154244882432       0.00      0.00      0.00         1\n",
      "4511071620630052864       0.00      0.00      0.00         1\n",
      "4511111825818910720       0.00      0.00      0.00         1\n",
      "4511142230959654445       0.00      0.00      0.00         1\n",
      "4511189642036379648       0.00      0.00      0.00         1\n",
      "4511206242084978688       0.00      0.00      0.00         1\n",
      "4511265002581462336       0.00      0.00      0.00         1\n",
      "4511277491297452032       0.00      0.00      0.00         1\n",
      "4511335586078383172       0.00      0.00      0.00         1\n",
      "4511360033022488736       0.00      0.00      0.00         1\n",
      "4511365031320879104       0.00      0.00      0.00         1\n",
      "4511402878572691456       0.00      0.00      0.00         1\n",
      "4511413754489690784       0.00      0.00      0.00         1\n",
      "4511428991973851136       0.00      0.00      0.00         1\n",
      "4511437865376284672       0.00      0.00      0.00         1\n",
      "4511443307099848704       0.00      0.00      0.00         1\n",
      "4511447821110476800       0.00      0.00      0.00         1\n",
      "4511536117048147968       0.00      0.00      0.00         1\n",
      "4511565193976741888       0.00      0.00      0.00         1\n",
      "4511586601151808046       0.00      0.00      0.00         1\n",
      "4511683846743261184       0.00      0.00      0.00         1\n",
      "4511704728874254336       0.00      0.00      0.00         1\n",
      "4511810884339960980       0.00      0.00      0.00         1\n",
      "4511818889104982016       0.00      0.00      0.00         1\n",
      "4512017858815398636       0.00      0.00      0.00         1\n",
      "4512033586986870884       0.00      0.00      0.00         1\n",
      "4512035329686896640       0.00      0.00      0.00         1\n",
      "4512055650232347030       0.00      0.00      0.00         1\n",
      "4512068663987538825       0.00      0.00      0.00         1\n",
      "4512081260067160064       0.00      0.00      0.00         1\n",
      "4512092419449969974       0.00      0.00      0.00         1\n",
      "4512099200145555456       0.00      0.00      0.00         1\n",
      "4512175835247017984       0.00      0.00      0.00         1\n",
      "4512190198656116640       0.00      0.00      0.00         1\n",
      "4512226997886351872       0.00      0.00      0.00         1\n",
      "4512252478938415104       0.00      0.00      0.00         1\n",
      "4512349945696487928       0.00      0.00      0.00         1\n",
      "4512412613561963349       0.00      0.00      0.00         1\n",
      "4512439374440300544       0.00      0.00      0.00         1\n",
      "4512450516644179103       0.00      0.00      0.00         1\n",
      "4512460764426239006       0.00      0.00      0.00         1\n",
      "4512464050063351712       0.00      0.00      0.00         1\n",
      "4512467807123800064       0.00      0.00      0.00         1\n",
      "4512483664143056896       0.00      0.00      0.00         1\n",
      "4512487263325650944       0.00      0.00      0.00         1\n",
      "4512546447974989824       0.00      0.00      0.00         1\n",
      "4512552255833807992       0.00      0.00      0.00         1\n",
      "4512583552197459968       0.00      0.00      0.00         1\n",
      "4512612239336145276       0.00      0.00      0.00         1\n",
      "4512617710072365056       0.00      0.00      0.00         1\n",
      "4512636703484260891       0.00      0.00      0.00         1\n",
      "4512796691009122178       0.00      0.00      0.00         1\n",
      "4512798426165545210       0.00      0.00      0.00         1\n",
      "4512799305584607232       0.00      0.00      0.00         1\n",
      "4512811929544996712       0.00      0.00      0.00         1\n",
      "4512835031122575360       0.00      0.00      0.00         1\n",
      "4512845919914267423       0.00      0.00      0.00         1\n",
      "4512934189032538112       0.00      0.00      0.00         1\n",
      "4512936140014454606       0.00      0.00      0.00         1\n",
      "4512936929221672960       0.00      0.00      0.00         1\n",
      "4512977620790813227       0.00      0.00      0.00         1\n",
      "4512983340638273536       0.00      0.00      0.00         1\n",
      "4513030594918840850       0.00      0.00      0.00         1\n",
      "4513064257822130176       0.00      0.00      0.00         1\n",
      "4513070654094007086       0.00      0.00      0.00         1\n",
      "4513078028553111501       0.00      0.00      0.00         1\n",
      "4513126259970015232       0.00      0.00      0.00         1\n",
      "4513175128107909120       0.00      0.00      0.00         1\n",
      "4513190616801955876       0.00      0.00      0.00         1\n",
      "4513195993059033088       0.00      0.00      0.00         1\n",
      "4513219405930571648       0.00      0.00      0.00         1\n",
      "4513267097301101644       0.00      0.00      0.00         1\n",
      "4513272530433384134       0.00      0.00      0.00         1\n",
      "4513288412165308416       0.00      0.00      0.00         1\n",
      "4513324456590639698       0.00      0.00      0.00         1\n",
      "4513370166867787776       0.00      0.00      0.00         1\n",
      "4513437035213619200       0.00      0.00      0.00         1\n",
      "4513492530486050816       0.00      0.00      0.00         1\n",
      "4513524550523801368       0.00      0.00      0.00         1\n",
      "4513534603985682432       0.00      0.00      0.00         1\n",
      "4513564475483226112       0.00      0.00      0.00         1\n",
      "4513572824899649536       0.00      0.00      0.00         1\n",
      "4513595408890150270       0.00      0.00      0.00         1\n",
      "4513621513699221082       0.00      0.00      0.00         1\n",
      "4513691478732998446       0.00      0.00      0.00         1\n",
      "4513727204253501090       0.00      0.00      0.00         1\n",
      "4513733868993380352       0.00      0.00      0.00         1\n",
      "4513736532933002388       0.00      0.00      0.00         1\n",
      "4513745694079544412       0.00      0.00      0.00         1\n",
      "4513757827366086425       0.00      0.00      0.00         1\n",
      "4513785555674015049       0.00      0.00      0.00         1\n",
      "4513808781812957184       0.00      0.00      0.00         1\n",
      "4513818575377583157       0.00      0.00      0.00         1\n",
      "4513949786652218519       0.00      0.00      0.00         1\n",
      "4513950516786561538       0.00      0.00      0.00         1\n",
      "4513952439879073792       0.00      0.00      0.00         1\n",
      "4513965179801505536       0.00      0.00      0.00         1\n",
      "4513974650217474086       0.00      0.00      0.00         1\n",
      "4513975831323477287       0.00      0.00      0.00         1\n",
      "4513975864630706176       0.00      0.00      0.00         1\n",
      "4514024724178665472       0.00      0.00      0.00         1\n",
      "4514074872216813568       0.00      0.00      0.00         1\n",
      "4514085348661653888       0.00      0.00      0.00         1\n",
      "4514117298956034247       0.00      0.00      0.00         1\n",
      "4514145773536935936       0.00      0.00      0.00         1\n",
      "4514224441204643370       0.00      0.00      0.00         1\n",
      "4514240700904112128       0.00      0.00      0.00         1\n",
      "4514243004054010852       0.00      0.00      0.00         1\n",
      "4514251061426718734       0.00      0.00      0.00         1\n",
      "4514293340023291904       0.00      0.00      0.00         1\n",
      "4514324530075795456       0.00      0.00      0.00         1\n",
      "4514332111759790329       0.00      0.00      0.00         1\n",
      "4514363065579984448       0.00      0.00      0.00         1\n",
      "4514465865614576132       0.00      0.00      0.00         1\n",
      "4514517833668886528       0.00      0.00      0.00         1\n",
      "4514575820022349824       0.00      0.00      0.00         1\n",
      "4514578972528345088       0.00      0.00      0.00         1\n",
      "4514581201616371712       0.00      0.00      0.00         1\n",
      "4514588292607377408       0.00      0.00      0.00         1\n",
      "4514617584284336128       0.00      0.00      0.00         1\n",
      "4514635653211750400       0.00      0.00      0.00         1\n",
      "4514636079446369326       0.00      0.00      0.00         1\n",
      "4514637469982916608       0.00      0.00      0.00         1\n",
      "4514654091506352128       0.00      0.00      0.00         1\n",
      "4514659787671245696       0.00      0.00      0.00         1\n",
      "4514791325340181900       0.00      0.00      0.00         1\n",
      "4514800757108002400       0.00      0.00      0.00         1\n",
      "4514836816594993152       0.00      0.00      0.00         1\n",
      "4514890254578089984       0.00      0.00      0.00         1\n",
      "4514900512018044653       0.00      0.00      0.00         1\n",
      "4514913371104787136       0.00      0.00      0.00         1\n",
      "4514963270064775990       0.00      0.00      0.00         1\n",
      "4515013907748182707       0.00      0.00      0.00         1\n",
      "4515024695644389376       0.00      0.00      0.00         1\n",
      "4515053369903588349       0.00      0.00      0.00         1\n",
      "4515106463231770624       0.00      0.00      0.00         1\n",
      "4515122346020831232       0.00      0.00      0.00         1\n",
      "4515152419381837824       0.00      0.00      0.00         1\n",
      "4515159103400373049       0.00      0.00      0.00         1\n",
      "4515176214568249782       0.00      0.00      0.00         1\n",
      "4515201803977418329       0.00      0.00      0.00         1\n",
      "4515204156557885440       0.00      0.00      0.00         1\n",
      "4515254708322959360       0.00      0.00      0.00         1\n",
      "4515405032178319360       0.00      0.00      0.00         1\n",
      "4515410078764892160       0.00      0.00      0.00         1\n",
      "4515447286066577408       0.00      0.00      0.00         1\n",
      "4515454729244901376       0.00      0.00      0.00         1\n",
      "4515548879222996992       0.00      0.00      0.00         1\n",
      "4515584278343450624       0.00      0.00      0.00         1\n",
      "4515662108511716670       0.00      0.00      0.00         1\n",
      "4515671307265769472       0.00      0.00      0.00         1\n",
      "4515759281080893440       0.00      0.00      0.00         1\n",
      "4515774807387668480       0.00      0.00      0.00         1\n",
      "4515902373245823136       0.00      0.00      0.00         1\n",
      "4515909656475860992       0.00      0.00      0.00         1\n",
      "4515964189675618304       0.00      0.00      0.00         1\n",
      "4515997438078453220       0.00      0.00      0.00         1\n",
      "4516024994586955931       0.00      0.00      0.00         1\n",
      "4516123094875635712       0.00      0.00      0.00         1\n",
      "4516136808706211840       0.00      0.00      0.00         1\n",
      "4516183340381896704       0.00      0.00      0.00         1\n",
      "4516230842720190464       0.00      0.00      0.00         1\n",
      "4516242241563394048       0.00      0.00      0.00         1\n",
      "4516247688642555786       0.00      0.00      0.00         1\n",
      "4516308749131972608       0.00      0.00      0.00         1\n",
      "4516349110007825031       0.00      0.00      0.00         1\n",
      "4516438024400965197       0.00      0.00      0.00         1\n",
      "4516440119296655360       0.00      0.00      0.00         1\n",
      "4516449646581031680       0.00      0.00      0.00         1\n",
      "4516474105372868608       0.00      0.00      0.00         1\n",
      "4516481196363874304       0.00      0.00      0.00         1\n",
      "4516508247108666807       0.00      0.00      0.00         1\n",
      "4516553979940998455       0.00      0.00      0.00         1\n",
      "4516595554163097600       0.00      0.00      0.00         1\n",
      "4516635110811893760       0.00      0.00      0.00         1\n",
      "4516658630052806656       0.00      0.00      0.00         1\n",
      "4516799012114498421       0.00      0.00      0.00         1\n",
      "4516931669713747968       0.00      0.00      0.00         1\n",
      "4516961304988090368       0.00      0.00      0.00         1\n",
      "4516974520602460160       0.00      0.00      0.00         1\n",
      "4517013136653418496       0.00      0.00      0.00         1\n",
      "4517039981232464944       0.00      0.00      0.00         1\n",
      "4517049988518659160       0.00      0.00      0.00         1\n",
      "4517091249223630848       0.00      0.00      0.00         1\n",
      "4517127631891595264       0.00      0.00      0.00         1\n",
      "4517152207694462976       0.00      0.00      0.00         1\n",
      "4517188028773409596       0.00      0.00      0.00         1\n",
      "4517203773071818752       0.00      0.00      0.00         1\n",
      "4517217413887950848       0.00      0.00      0.00         1\n",
      "4517264495319449600       0.00      0.00      0.00         1\n",
      "4517270491093794816       0.00      0.00      0.00         1\n",
      "4517282947553353860       0.00      0.00      0.00         1\n",
      "4517300242332254208       0.00      0.00      0.00         1\n",
      "4517350069285549984       0.00      0.00      0.00         1\n",
      "4517357044321643184       0.00      0.00      0.00         1\n",
      "4517367050548543488       0.00      0.00      0.00         1\n",
      "4517392825714291179       0.00      0.00      0.00         1\n",
      "4517420372567523328       0.00      0.00      0.00         1\n",
      "4517425742327946365       0.00      0.00      0.00         1\n",
      "4517453994618980560       0.00      0.00      0.00         1\n",
      "4517698541476570497       0.00      0.00      0.00         1\n",
      "4517706568762991504       0.00      0.00      0.00         1\n",
      "4517737843665141760       0.00      0.00      0.00         1\n",
      "4517741743495446528       0.00      0.00      0.00         1\n",
      "4517852227234168832       0.00      0.00      0.00         1\n",
      "4517853210781679616       0.00      0.00      0.00         1\n",
      "4517881871098445824       0.00      0.00      0.00         1\n",
      "4517884422309019648       0.00      0.00      0.00         1\n",
      "4517886489229898788       0.00      0.00      0.00         1\n",
      "4517915096965447680       0.00      0.00      0.00         1\n",
      "4517950406939830784       0.00      0.00      0.00         1\n",
      "4517957823300108288       0.00      0.00      0.00         1\n",
      "4517988708409933824       0.00      0.00      0.00         1\n",
      "4518040717234297004       0.00      0.00      0.00         1\n",
      "4518107177560962321       0.00      0.00      0.00         1\n",
      "4518138260209533944       0.00      0.00      0.00         1\n",
      "4518164557255933952       0.00      0.00      0.00         1\n",
      "4518224971325923961       0.00      0.00      0.00         1\n",
      "4518227608428866691       0.00      0.00      0.00         1\n",
      "4518232685065792936       0.00      0.00      0.00         1\n",
      "4518240037011193856       0.00      0.00      0.00         1\n",
      "4518246308714480228       0.00      0.00      0.00         1\n",
      "4518258462420893696       0.00      0.00      0.00         1\n",
      "4518279954437242880       0.00      0.00      0.00         1\n",
      "4518347140610654208       0.00      0.00      0.00         1\n",
      "4518377683152741088       0.00      0.00      0.00         1\n",
      "4518393865559867392       0.00      0.00      0.00         1\n",
      "4518405925828034560       0.00      0.00      0.00         1\n",
      "4518438052183408640       0.00      0.00      0.00         1\n",
      "4518497597610000384       0.00      0.00      0.00         1\n",
      "4518533546486267904       0.00      0.00      0.00         1\n",
      "4518550013390880768       0.00      0.00      0.00         1\n",
      "4518554743158091136       0.00      0.00      0.00         1\n",
      "4518640903488798720       0.00      0.00      0.00         1\n",
      "4518650671297749936       0.00      0.00      0.00         1\n",
      "4518693551197913088       0.00      0.00      0.00         1\n",
      "4518702855163943765       0.00      0.00      0.00         1\n",
      "4518707578561101824       0.00      0.00      0.00         1\n",
      "4518726090920304623       0.00      0.00      0.00         1\n",
      "4518734620738887596       0.00      0.00      0.00         1\n",
      "4518764779978960602       0.00      0.00      0.00         1\n",
      "4518769298272293552       0.00      0.00      0.00         1\n",
      "4518802867772052447       0.00      0.00      0.00         1\n",
      "4518802883885400064       0.00      0.00      0.00         1\n",
      "4518810863934636032       0.00      0.00      0.00         1\n",
      "4518853719118315520       0.00      0.00      0.00         1\n",
      "4518854398779600149       0.00      0.00      0.00         1\n",
      "4518875229358184792       0.00      0.00      0.00         1\n",
      "4518882496440452390       0.00      0.00      0.00         1\n",
      "4518885039080807915       0.00      0.00      0.00         1\n",
      "4518926133324143462       0.00      0.00      0.00         1\n",
      "4518931854202158920       0.00      0.00      0.00         1\n",
      "4518984492282544128       0.00      0.00      0.00         1\n",
      "4519006998958796156       0.00      0.00      0.00         1\n",
      "4519018492313250305       0.00      0.00      0.00         1\n",
      "4519059199973229824       0.00      0.00      0.00         1\n",
      "4519096049763090432       0.00      0.00      0.00         1\n",
      "4519098304620920832       0.00      0.00      0.00         1\n",
      "4519102823980009234       0.00      0.00      0.00         1\n",
      "4519116326303694848       0.00      0.00      0.00         1\n",
      "4519117121939226135       0.00      0.00      0.00         1\n",
      "4519118727190413312       0.00      0.00      0.00         1\n",
      "4519121730433156587       0.00      0.00      0.00         1\n",
      "4519157472090390528       0.00      0.00      0.00         1\n",
      "4519269000556224760       0.00      0.00      0.00         1\n",
      "4519277418700572185       0.00      0.00      0.00         1\n",
      "4519280784896425984       0.00      0.00      0.00         1\n",
      "4519306425851183104       0.00      0.00      0.00         1\n",
      "4519363227846448890       0.00      0.00      0.00         1\n",
      "4519386381997555827       0.00      0.00      0.00         1\n",
      "4519454954410213376       0.00      0.00      0.00         1\n",
      "4519489103695183872       0.00      0.00      0.00         1\n",
      "4519578735367684096       0.00      0.00      0.00         1\n",
      "4519586535028293632       0.00      0.00      0.00         1\n",
      "4519706314144170311       0.00      0.00      0.00         1\n",
      "4519807787028238720       0.00      0.00      0.00         1\n",
      "4519849949667524608       0.00      0.00      0.00         1\n",
      "4519853317979878714       0.00      0.00      0.00         1\n",
      "4519860211382318172       0.00      0.00      0.00         1\n",
      "4519930987110465536       0.00      0.00      0.00         1\n",
      "4519985048930012678       0.00      0.00      0.00         1\n",
      "4520012648365113308       0.00      0.00      0.00         1\n",
      "4520013473023137535       0.00      0.00      0.00         1\n",
      "4520016754372524381       0.00      0.00      0.00         1\n",
      "4520028780265135230       0.00      0.00      0.00         1\n",
      "4520049104037788008       0.00      0.00      0.00         1\n",
      "4520068208086129388       0.00      0.00      0.00         1\n",
      "4520116311694674422       0.00      0.00      0.00         1\n",
      "4520207042902490790       0.00      0.00      0.00         1\n",
      "4520207909421842432       0.00      0.00      0.00         1\n",
      "4520328809516254860       0.00      0.00      0.00         1\n",
      "4520361162444898304       0.00      0.00      0.00         1\n",
      "4520372927414442053       0.00      0.00      0.00         1\n",
      "4520390339211367061       0.00      0.00      0.00         1\n",
      "4520410524504031232       0.00      0.00      0.00         1\n",
      "4520479707837235200       0.00      0.00      0.00         1\n",
      "4520542809496748032       0.00      0.00      0.00         1\n",
      "4520546881125744640       0.00      0.00      0.00         1\n",
      "4520548479915294257       0.00      0.00      0.00         1\n",
      "4520610353218343793       0.00      0.00      0.00         1\n",
      "4520723520245727232       0.00      0.00      0.00         1\n",
      "4520730852808139956       0.00      0.00      0.00         1\n",
      "4520744556995543040       0.00      0.00      0.00         1\n",
      "4520778286422916218       0.00      0.00      0.00         1\n",
      "4520938470473990144       0.00      0.00      0.00         1\n",
      "4520939140488888320       0.00      0.00      0.00         1\n",
      "4520958708359888896       0.00      0.00      0.00         1\n",
      "4520977365697822720       0.00      0.00      0.00         1\n",
      "4521049899105517568       0.00      0.00      0.00         1\n",
      "4521056900944935984       0.00      0.00      0.00         1\n",
      "4521163947667095552       0.00      0.00      0.00         1\n",
      "4521218219916758808       0.00      0.00      0.00         1\n",
      "4521247703824334848       0.00      0.00      0.00         1\n",
      "4521253360296263680       0.00      0.00      0.00         1\n",
      "4521346063925210316       0.00      0.00      0.00         1\n",
      "4521377192793341952       0.00      0.00      0.00         1\n",
      "4521445268024983552       0.00      0.00      0.00         1\n",
      "4521447957726661678       0.00      0.00      0.00         1\n",
      "4521472137340387328       0.00      0.00      0.00         1\n",
      "4521500600088657920       0.00      0.00      0.00         1\n",
      "4521502519939039232       0.00      0.00      0.00         1\n",
      "4521508714332000493       0.00      0.00      0.00         1\n",
      "4521528165188763648       0.00      0.00      0.00         1\n",
      "4521557581419773952       0.00      0.00      0.00         1\n",
      "4521561094703022080       0.00      0.00      0.00         1\n",
      "4521614662584775304       0.00      0.00      0.00         1\n",
      "4521615950025326592       0.00      0.00      0.00         1\n",
      "4521650620062503886       0.00      0.00      0.00         1\n",
      "4521662425866436608       0.00      0.00      0.00         1\n",
      "4521682955810111488       0.00      0.00      0.00         1\n",
      "4521740844448050101       0.00      0.00      0.00         1\n",
      "4521754730067918256       0.00      0.00      0.00         1\n",
      "4521769143967096148       0.00      0.00      0.00         1\n",
      "4521783935792715264       0.00      0.00      0.00         1\n",
      "4521808009135975934       0.00      0.00      0.00         1\n",
      "4521814145586167808       0.00      0.00      0.00         1\n",
      "4521860896305184768       0.00      0.00      0.00         1\n",
      "4521868193454620672       0.00      0.00      0.00         1\n",
      "4521876517101240320       0.00      0.00      0.00         1\n",
      "4521894272496041984       0.00      0.00      0.00         1\n",
      "4521933554266931200       0.00      0.00      0.00         1\n",
      "4521938390400106496       0.00      0.00      0.00         1\n",
      "4521938815601868800       0.00      0.00      0.00         1\n",
      "4522045404866153394       0.00      0.00      0.00         1\n",
      "4522089411099464112       0.00      0.00      0.00         1\n",
      "4522119922551082836       0.00      0.00      0.00         1\n",
      "4522125582254735360       0.00      0.00      0.00         1\n",
      "4522138545521808662       0.00      0.00      0.00         1\n",
      "4522222171774255104       0.00      0.00      0.00         1\n",
      "4522239248564224000       0.00      0.00      0.00         1\n",
      "4522265641138257920       0.00      0.00      0.00         1\n",
      "4522276687794143232       0.00      0.00      0.00         1\n",
      "4522288043687673856       0.00      0.00      0.00         1\n",
      "4522293722695006504       0.00      0.00      0.00         1\n",
      "4522305533824964448       0.00      0.00      0.00         1\n",
      "4522374614104453592       0.00      0.00      0.00         1\n",
      "4522388253864624128       0.00      0.00      0.00         1\n",
      "4522404915106204067       0.00      0.00      0.00         1\n",
      "4522465339937652736       0.00      0.00      0.00         1\n",
      "4522509470726619136       0.00      0.00      0.00         1\n",
      "4522520697771130880       0.00      0.00      0.00         1\n",
      "4522562677832518233       0.00      0.00      0.00         1\n",
      "4522585019201355776       0.00      0.00      0.00         1\n",
      "4522590967731060736       0.00      0.00      0.00         1\n",
      "4522616441182093312       0.00      0.00      0.00         1\n",
      "4522670158338064384       0.00      0.00      0.00         1\n",
      "4522680175258707926       0.00      0.00      0.00         1\n",
      "4522681058965061632       0.00      0.00      0.00         1\n",
      "4522758179397828608       0.00      0.00      0.00         1\n",
      "4522758724858675200       0.00      0.00      0.00         1\n",
      "4522759219821974856       0.00      0.00      0.00         1\n",
      "4522812597698882394       0.00      0.00      0.00         1\n",
      "4522813911934461600       0.00      0.00      0.00         1\n",
      "4522826185909993472       0.00      0.00      0.00         1\n",
      "4522842824613298176       0.00      0.00      0.00         1\n",
      "4522862217448176805       0.00      0.00      0.00         1\n",
      "4522877099520033330       0.00      0.00      0.00         1\n",
      "4522898693547884544       0.00      0.00      0.00         1\n",
      "4522925886048176698       0.00      0.00      0.00         1\n",
      "4522947279289184668       0.00      0.00      0.00         1\n",
      "4522960850314592256       0.00      0.00      0.00         1\n",
      "4522977695176327168       0.00      0.00      0.00         1\n",
      "4522995029664333824       0.00      0.00      0.00         1\n",
      "4523011487979012096       0.00      0.00      0.00         1\n",
      "4523073023033204498       0.00      0.00      0.00         1\n",
      "4523128136054868413       0.00      0.00      0.00         1\n",
      "4523198436079126585       0.00      0.00      0.00         1\n",
      "4523213540420485120       0.00      0.00      0.00         1\n",
      "4523243760855324252       0.00      0.00      0.00         1\n",
      "4523297709959238126       0.00      0.00      0.00         1\n",
      "4523325605765737040       0.00      0.00      0.00         1\n",
      "4523334190346797056       0.00      0.00      0.00         1\n",
      "4523392245419737088       0.00      0.00      0.00         1\n",
      "4523479636179400025       0.00      0.00      0.00         1\n",
      "4523493401560916862       0.00      0.00      0.00         1\n",
      "4523493606647922688       0.00      0.00      0.00         1\n",
      "4523514583268196352       0.00      0.00      0.00         1\n",
      "4523558994266300736       0.00      0.00      0.00         1\n",
      "4523562606356181207       0.00      0.00      0.00         1\n",
      "4523585545763163017       0.00      0.00      0.00         1\n",
      "4523673402568867840       0.00      0.00      0.00         1\n",
      "4523703021696207632       0.00      0.00      0.00         1\n",
      "4523729993057959936       0.00      0.00      0.00         1\n",
      "4523836474937622030       0.00      0.00      0.00         1\n",
      "4523879784337375232       0.00      0.00      0.00         1\n",
      "4523910249605564190       0.00      0.00      0.00         1\n",
      "4523946529205022170       0.00      0.00      0.00         1\n",
      "4524005997308312832       0.00      0.00      0.00         1\n",
      "4524019808861159424       0.00      0.00      0.00         1\n",
      "4524127269998465982       0.00      0.00      0.00         1\n",
      "4524153074167563925       0.00      0.00      0.00         1\n",
      "4524231002052093772       0.00      0.00      0.00         1\n",
      "4524252360928273708       0.00      0.00      0.00         1\n",
      "4524298888287119592       0.00      0.00      0.00         1\n",
      "4524435236331188492       0.00      0.00      0.00         1\n",
      "4524445784778493714       0.00      0.00      0.00         1\n",
      "4524452930543157248       0.00      0.00      0.00         1\n",
      "4524466107502821376       0.00      0.00      0.00         1\n",
      "4524474007012911487       0.00      0.00      0.00         1\n",
      "4524554957491273728       0.00      0.00      0.00         1\n",
      "4524589957179768832       0.00      0.00      0.00         1\n",
      "4524615529415049216       0.00      0.00      0.00         1\n",
      "4524619747072933888       0.00      0.00      0.00         1\n",
      "4524656727789879536       0.00      0.00      0.00         1\n",
      "4524668487411403620       0.00      0.00      0.00         1\n",
      "4524698990285352196       0.00      0.00      0.00         1\n",
      "4524828689684560901       0.00      0.00      0.00         1\n",
      "4524845955466873152       0.00      0.00      0.00         1\n",
      "4524847089336029620       0.00      0.00      0.00         1\n",
      "4524858366865965056       0.00      0.00      0.00         1\n",
      "4524926194047722040       0.00      0.00      0.00         1\n",
      "4524940327726874624       0.00      0.00      0.00         1\n",
      "4524972193145365210       0.00      0.00      0.00         1\n",
      "4524975431547744482       0.00      0.00      0.00         1\n",
      "4525045701502241284       0.00      0.00      0.00         1\n",
      "4525051292501934080       0.00      0.00      0.00         1\n",
      "4525056383088764334       0.00      0.00      0.00         1\n",
      "4525082178658115916       0.00      0.00      0.00         1\n",
      "4525133064384282624       0.00      0.00      0.00         1\n",
      "4525203085236109312       0.00      0.00      0.00         1\n",
      "4525217649470210048       0.00      0.00      0.00         1\n",
      "4525222057147567608       0.00      0.00      0.00         1\n",
      "4525243805821042688       0.00      0.00      0.00         1\n",
      "4525254285541244928       0.00      0.00      0.00         1\n",
      "4525258348580306944       0.00      0.00      0.00         1\n",
      "4525275146197401600       0.00      0.00      0.00         1\n",
      "4525280901453578240       0.00      0.00      0.00         1\n",
      "4525319740842835968       0.00      0.00      0.00         1\n",
      "4525335998316000523       0.00      0.00      0.00         1\n",
      "4525378149155431416       0.00      0.00      0.00         1\n",
      "4525400594655869292       0.00      0.00      0.00         1\n",
      "4525403956561575936       0.00      0.00      0.00         1\n",
      "4525516640340252512       0.00      0.00      0.00         1\n",
      "4525522308680384512       0.00      0.00      0.00         1\n",
      "4525557270763306090       0.00      0.00      0.00         1\n",
      "4525608904868994888       0.00      0.00      0.00         1\n",
      "4525652966929494792       0.00      0.00      0.00         1\n",
      "4525706116100784128       0.00      0.00      0.00         1\n",
      "4525734876245427520       0.00      0.00      0.00         1\n",
      "4525818227632111616       0.00      0.00      0.00         1\n",
      "4525826289285726208       0.00      0.00      0.00         1\n",
      "4525846622719361096       0.00      0.00      0.00         1\n",
      "4525852403756518680       0.00      0.00      0.00         1\n",
      "4525891797188328564       0.00      0.00      0.00         1\n",
      "4525935872139595534       0.00      0.00      0.00         1\n",
      "4525970876136475584       0.00      0.00      0.00         1\n",
      "4526031104449592302       0.00      0.00      0.00         1\n",
      "4526138671912492933       0.00      0.00      0.00         1\n",
      "4526151449433485142       0.00      0.00      0.00         1\n",
      "4526212630183936000       0.00      0.00      0.00         1\n",
      "4526222929515511808       0.00      0.00      0.00         1\n",
      "4526269362406948864       0.00      0.00      0.00         1\n",
      "4526269728533998376       0.00      0.00      0.00         1\n",
      "4526301909669117952       0.00      0.00      0.00         1\n",
      "4526307115169480704       0.00      0.00      0.00         1\n",
      "4526316916284850176       0.00      0.00      0.00         1\n",
      "4526320503638916032       0.00      0.00      0.00         1\n",
      "4526325206624795332       0.00      0.00      0.00         1\n",
      "4526329333035302912       0.00      0.00      0.00         1\n",
      "4526334788684920980       0.00      0.00      0.00         1\n",
      "4526386245646942208       0.00      0.00      0.00         1\n",
      "4526395909323358208       0.00      0.00      0.00         1\n",
      "4526404255511609809       0.00      0.00      0.00         1\n",
      "4526458479464121927       0.00      0.00      0.00         1\n",
      "4526492216412211248       0.00      0.00      0.00         1\n",
      "4526492285156712505       0.00      0.00      0.00         1\n",
      "4526498499970931128       0.00      0.00      0.00         1\n",
      "4526514721003667456       0.00      0.00      0.00         1\n",
      "4526519694575796224       0.00      0.00      0.00         1\n",
      "4526537725910365678       0.00      0.00      0.00         1\n",
      "4526578090001815748       0.00      0.00      0.00         1\n",
      "4526579162692976640       0.00      0.00      0.00         1\n",
      "4526603446438068224       0.00      0.00      0.00         1\n",
      "4526614802331598848       0.00      0.00      0.00         1\n",
      "4526706160580952064       0.00      0.00      0.00         1\n",
      "4526711148086260156       0.00      0.00      0.00         1\n",
      "4526726119293976576       0.00      0.00      0.00         1\n",
      "4526788895568856224       0.00      0.00      0.00         1\n",
      "4526790406364463104       0.00      0.00      0.00         1\n",
      "4526855749996904448       0.00      0.00      0.00         1\n",
      "4526875602392854004       0.00      0.00      0.00         1\n",
      "4526965350024550922       0.00      0.00      0.00         1\n",
      "4526975030886398800       0.00      0.00      0.00         1\n",
      "4526985428996080930       0.00      0.00      0.00         1\n",
      "4527007620040491008       0.00      0.00      0.00         1\n",
      "4527033081664561954       0.00      0.00      0.00         1\n",
      "4527148818132412296       0.00      0.00      0.00         1\n",
      "4527214126363049984       0.00      0.00      0.00         1\n",
      "4527273770573889536       0.00      0.00      0.00         1\n",
      "4527284818274478539       0.00      0.00      0.00         1\n",
      "4527320937904734208       0.00      0.00      0.00         1\n",
      "4527425083333689210       0.00      0.00      0.00         1\n",
      "4527469321476435320       0.00      0.00      0.00         1\n",
      "4527512656654893056       0.00      0.00      0.00         1\n",
      "4527514633396203342       0.00      0.00      0.00         1\n",
      "4527564849097474048       0.00      0.00      0.00         1\n",
      "4527596830460350032       0.00      0.00      0.00         1\n",
      "4527617351833815312       0.00      0.00      0.00         1\n",
      "4527632921084758909       0.00      0.00      0.00         1\n",
      "4527633904631452610       0.00      0.00      0.00         1\n",
      "4527717819712658050       0.00      0.00      0.00         1\n",
      "4527732481671036928       0.00      0.00      0.00         1\n",
      "4527786320140962599       0.00      0.00      0.00         1\n",
      "4527847393521041408       0.00      0.00      0.00         1\n",
      "4527862116668932096       0.00      0.00      0.00         1\n",
      "4527880152276846120       0.00      0.00      0.00         1\n",
      "4527975963367047168       0.00      0.00      0.00         1\n",
      "4527980826327760338       0.00      0.00      0.00         1\n",
      "4527995174755762176       0.00      0.00      0.00         1\n",
      "4527998994040821499       0.00      0.00      0.00         1\n",
      "4528027113092702208       0.00      0.00      0.00         1\n",
      "4528046410466835681       0.00      0.00      0.00         1\n",
      "4528117667223044096       0.00      0.00      0.00         1\n",
      "4528133112980544114       0.00      0.00      0.00         1\n",
      "4528150563418878794       0.00      0.00      0.00         1\n",
      "4528151086363574272       0.00      0.00      0.00         1\n",
      "4528174499282408716       0.00      0.00      0.00         1\n",
      "4528175138180431872       0.00      0.00      0.00         1\n",
      "4528263392175291008       0.00      0.00      0.00         1\n",
      "4528282994457642407       0.00      0.00      0.00         1\n",
      "4528304021559050240       0.00      0.00      0.00         1\n",
      "4528325033577122984       0.00      0.00      0.00         1\n",
      "4528326554032857784       0.00      0.00      0.00         1\n",
      "4528329994286650470       0.00      0.00      0.00         1\n",
      "4528339016952578048       0.00      0.00      0.00         1\n",
      "4528449029282273664       0.00      0.00      0.00         1\n",
      "4528459775297437252       0.00      0.00      0.00         1\n",
      "4528461235609701005       0.00      0.00      0.00         1\n",
      "4528470547091072351       0.00      0.00      0.00         1\n",
      "4528547262736891904       0.00      0.00      0.00         1\n",
      "4528578024350820402       0.00      0.00      0.00         1\n",
      "4528579574817705984       0.00      0.00      0.00         1\n",
      "4528587589242974566       0.00      0.00      0.00         1\n",
      "4528748938265745272       0.00      0.00      0.00         1\n",
      "4528759400826571142       0.00      0.00      0.00         1\n",
      "4528775687326304223       0.00      0.00      0.00         1\n",
      "4528822150294397087       0.00      0.00      0.00         1\n",
      "4528842508391680704       0.00      0.00      0.00         1\n",
      "4528857334665132702       0.00      0.00      0.00         1\n",
      "4528871584307347456       0.00      0.00      0.00         1\n",
      "4528885190763741184       0.00      0.00      0.00         1\n",
      "4528936060356395008       0.00      0.00      0.00         1\n",
      "4528984139273637584       0.00      0.00      0.00         1\n",
      "4529020938567425492       0.00      0.00      0.00         1\n",
      "4529028910000569580       0.00      0.00      0.00         1\n",
      "4529051620746461184       0.00      0.00      0.00         1\n",
      "4529072769165426688       0.00      0.00      0.00         1\n",
      "4529134214026429159       0.00      0.00      0.00         1\n",
      "4529150423230295960       0.00      0.00      0.00         1\n",
      "4529197920217464832       0.00      0.00      0.00         1\n",
      "4529259459541544176       0.00      0.00      0.00         1\n",
      "4529282058626793472       0.00      0.00      0.00         1\n",
      "4529333284701732864       0.00      0.00      0.00         1\n",
      "4529340208189014016       0.00      0.00      0.00         1\n",
      "4529376552202272768       0.00      0.00      0.00         1\n",
      "4529426804383621422       0.00      0.00      0.00         1\n",
      "4529435883948088666       0.00      0.00      0.00         1\n",
      "4529504881530109952       0.00      0.00      0.00         1\n",
      "4529520073866842556       0.00      0.00      0.00         1\n",
      "4529569168600596480       0.00      0.00      0.00         1\n",
      "4529627078693304050       0.00      0.00      0.00         1\n",
      "4529646529551532032       0.00      0.00      0.00         1\n",
      "4529659702216228864       0.00      0.00      0.00         1\n",
      "4529664426680254464       0.00      0.00      0.00         1\n",
      "4529746221085779832       0.00      0.00      0.00         1\n",
      "4529752010713142771       0.00      0.00      0.00         1\n",
      "4529760514751964619       0.00      0.00      0.00         1\n",
      "4529766922812433416       0.00      0.00      0.00         1\n",
      "4529809003869372416       0.00      0.00      0.00         1\n",
      "4529812315289157632       0.00      0.00      0.00         1\n",
      "4529901149096264704       0.00      0.00      0.00         1\n",
      "4529902595501719552       0.00      0.00      0.00         1\n",
      "4529931874293776384       0.00      0.00      0.00         1\n",
      "4529957696687635955       0.00      0.00      0.00         1\n",
      "4529964963770772300       0.00      0.00      0.00         1\n",
      "4530140645122553712       0.00      0.00      0.00         1\n",
      "4530179882885316608       0.00      0.00      0.00         1\n",
      "4530192909521125376       0.00      0.00      0.00         1\n",
      "4530209626563013328       0.00      0.00      0.00         1\n",
      "4530230503369867264       0.00      0.00      0.00         1\n",
      "4530232222425836880       0.00      0.00      0.00         1\n",
      "4530293459000492032       0.00      0.00      0.00         1\n",
      "4530299017753990378       0.00      0.00      0.00         1\n",
      "4530343186131845120       0.00      0.00      0.00         1\n",
      "4530351777111380099       0.00      0.00      0.00         1\n",
      "4530371838905981736       0.00      0.00      0.00         1\n",
      "4530386496582057984       0.00      0.00      0.00         1\n",
      "4530400546490325552       0.00      0.00      0.00         1\n",
      "4530416539878293504       0.00      0.00      0.00         1\n",
      "4530426645936340992       0.00      0.00      0.00         1\n",
      "4530454375298758688       0.00      0.00      0.00         1\n",
      "4530467457771375234       0.00      0.00      0.00         1\n",
      "4530583310205901284       0.00      0.00      0.00         1\n",
      "4530585689619201098       0.00      0.00      0.00         1\n",
      "4530718910938222480       0.00      0.00      0.00         1\n",
      "4530741694672404480       0.00      0.00      0.00         1\n",
      "4530751896278325578       0.00      0.00      0.00         1\n",
      "4530855240722808832       0.00      0.00      0.00         1\n",
      "4530869225136324608       0.00      0.00      0.00         1\n",
      "4530921221071985092       0.00      0.00      0.00         1\n",
      "4531010126856464160       0.00      0.00      0.00         1\n",
      "4531050100108979968       0.00      0.00      0.00         1\n",
      "4531051662462156800       0.00      0.00      0.00         1\n",
      "4531055429148475392       0.00      0.00      0.00         1\n",
      "4531110684968199351       0.00      0.00      0.00         1\n",
      "4531149077657940338       0.00      0.00      0.00         1\n",
      "4531169769767829504       0.00      0.00      0.00         1\n",
      "4531184571288889778       0.00      0.00      0.00         1\n",
      "4531211396590862336       0.00      0.00      0.00         1\n",
      "4531215047313063936       0.00      0.00      0.00         1\n",
      "4531234297356484608       0.00      0.00      0.00         1\n",
      "4531283758199865344       0.00      0.00      0.00         1\n",
      "4531344849814683648       0.00      0.00      0.00         1\n",
      "4531361625956941824       0.00      0.00      0.00         1\n",
      "4531388040005812224       0.00      0.00      0.00         1\n",
      "4531475057096108192       0.00      0.00      0.00         1\n",
      "4531475306192312256       0.00      0.00      0.00         1\n",
      "4531606490632421376       0.00      0.00      0.00         1\n",
      "4531608685360709632       0.00      0.00      0.00         1\n",
      "4531665314504507392       0.00      0.00      0.00         1\n",
      "4531670262306832384       0.00      0.00      0.00         1\n",
      "4531680892350889984       0.00      0.00      0.00         1\n",
      "4531768085550518245       0.00      0.00      0.00         1\n",
      "4531812554573348864       0.00      0.00      0.00         1\n",
      "4531842087829103418       0.00      0.00      0.00         1\n",
      "4531906537047719936       0.00      0.00      0.00         1\n",
      "4531923666412453968       0.00      0.00      0.00         1\n",
      "4531937185934344192       0.00      0.00      0.00         1\n",
      "4531967483700514169       0.00      0.00      0.00         1\n",
      "4531975462682886144       0.00      0.00      0.00         1\n",
      "4532036459808423936       0.00      0.00      0.00         1\n",
      "4532054005816065721       0.00      0.00      0.00         1\n",
      "4532133802018234126       0.00      0.00      0.00         1\n",
      "4532157414677413888       0.00      0.00      0.00         1\n",
      "4532157853816286909       0.00      0.00      0.00         1\n",
      "4532188329852010496       0.00      0.00      0.00         1\n",
      "4532222681000443904       0.00      0.00      0.00         1\n",
      "4532328029018967444       0.00      0.00      0.00         1\n",
      "4532402495165006151       0.00      0.00      0.00         1\n",
      "4532416586944399360       0.00      0.00      0.00         1\n",
      "4532422174684850208       0.00      0.00      0.00         1\n",
      "4532479863642128384       0.00      0.00      0.00         1\n",
      "4532499037434603184       0.00      0.00      0.00         1\n",
      "4532525888511672320       0.00      0.00      0.00         1\n",
      "4532540028605235453       0.00      0.00      0.00         1\n",
      "4532541771300732928       0.00      0.00      0.00         1\n",
      "4532565789790708464       0.00      0.00      0.00         1\n",
      "4532616950408282112       0.00      0.00      0.00         1\n",
      "4532758766987848993       0.00      0.00      0.00         1\n",
      "4532775139408647719       0.00      0.00      0.00         1\n",
      "4532814665998748794       0.00      0.00      0.00         1\n",
      "4532816434459312128       0.00      0.00      0.00         1\n",
      "4532850416240558080       0.00      0.00      0.00         1\n",
      "4532854609183730926       0.00      0.00      0.00         1\n",
      "4532905586163426941       0.00      0.00      0.00         1\n",
      "4532939425142800384       0.00      0.00      0.00         1\n",
      "4532997365302992387       0.00      0.00      0.00         1\n",
      "4533010094534688768       0.00      0.00      0.00         1\n",
      "4533028804482460406       0.00      0.00      0.00         1\n",
      "4533030448384704512       0.00      0.00      0.00         1\n",
      "4533066964196655104       0.00      0.00      0.00         1\n",
      "4533069787037185280       0.00      0.00      0.00         1\n",
      "4533131289921847296       0.00      0.00      0.00         1\n",
      "4533204666179762808       0.00      0.00      0.00         1\n",
      "4533287149990051840       0.00      0.00      0.00         1\n",
      "4533358424972328960       0.00      0.00      0.00         1\n",
      "4533434936566807993       0.00      0.00      0.00         1\n",
      "4533435795555001459       0.00      0.00      0.00         1\n",
      "4533467690005077484       0.00      0.00      0.00         1\n",
      "4533471434151821312       0.00      0.00      0.00         1\n",
      "4533478698001780375       0.00      0.00      0.00         1\n",
      "4533480738109140523       0.00      0.00      0.00         1\n",
      "4533646857796059136       0.00      0.00      0.00         1\n",
      "4533665970400526336       0.00      0.00      0.00         1\n",
      "4533698856965111808       0.00      0.00      0.00         1\n",
      "4533704866675494060       0.00      0.00      0.00         1\n",
      "4533708002005961126       0.00      0.00      0.00         1\n",
      "4533754738784600064       0.00      0.00      0.00         1\n",
      "4533758729861287670       0.00      0.00      0.00         1\n",
      "4533844490716184576       0.00      0.00      0.00         1\n",
      "4533849985033397575       0.00      0.00      0.00         1\n",
      "4533864051040241488       0.00      0.00      0.00         1\n",
      "4533870990664400896       0.00      0.00      0.00         1\n",
      "4533876794224219530       0.00      0.00      0.00         1\n",
      "4533986075377770827       0.00      0.00      0.00         1\n",
      "4534022320600354632       0.00      0.00      0.00         1\n",
      "4534049305883399019       0.00      0.00      0.00         1\n",
      "4534098817204617216       0.00      0.00      0.00         1\n",
      "4534142042825571645       0.00      0.00      0.00         1\n",
      "4534154797808353280       0.00      0.00      0.00         1\n",
      "4534328044976179060       0.00      0.00      0.00         1\n",
      "4534491885074752700       0.00      0.00      0.00         1\n",
      "4534536174765775422       0.00      0.00      0.00         1\n",
      "4534555423767789568       0.00      0.00      0.00         1\n",
      "4534574734998971276       0.00      0.00      0.00         1\n",
      "4534585326379826384       0.00      0.00      0.00         1\n",
      "4534661552409673728       0.00      0.00      0.00         1\n",
      "4534682306749541733       0.00      0.00      0.00         1\n",
      "4534732205680431102       0.00      0.00      0.00         1\n",
      "4534740382239424512       0.00      0.00      0.00         1\n",
      "4534766917611204154       0.00      0.00      0.00         1\n",
      "4534770382585987072       0.00      0.00      0.00         1\n",
      "4534787842684181716       0.00      0.00      0.00         1\n",
      "4534892176036653614       0.00      0.00      0.00         1\n",
      "4534895431614530690       0.00      0.00      0.00         1\n",
      "4534920849237456284       0.00      0.00      0.00         1\n",
      "4534973217772940412       0.00      0.00      0.00         1\n",
      "4535007968346591684       0.00      0.00      0.00         1\n",
      "4535037629370963216       0.00      0.00      0.00         1\n",
      "4535075871786294538       0.00      0.00      0.00         1\n",
      "4535190761100017664       0.00      0.00      0.00         1\n",
      "4535190783631832990       0.00      0.00      0.00         1\n",
      "4535246621444669440       0.00      0.00      0.00         1\n",
      "4535303655379540860       0.00      0.00      0.00         1\n",
      "4535335914860032724       0.00      0.00      0.00         1\n",
      "4535479804874961185       0.00      0.00      0.00         1\n",
      "4535520060537569280       0.00      0.00      0.00         1\n",
      "4535535346326175744       0.00      0.00      0.00         1\n",
      "4535568086861873152       0.00      0.00      0.00         1\n",
      "4535592529520754688       0.00      0.00      0.00         1\n",
      "4535631906838738678       0.00      0.00      0.00         1\n",
      "4535638262332522496       0.00      0.00      0.00         1\n",
      "4535644783135665562       0.00      0.00      0.00         1\n",
      "4535653414977142784       0.00      0.00      0.00         1\n",
      "4535690282976411648       0.00      0.00      0.00         1\n",
      "4535703014327233930       0.00      0.00      0.00         1\n",
      "4535727529986371899       0.00      0.00      0.00         1\n",
      "4535784524208514866       0.00      0.00      0.00         1\n",
      "4535875335937327104       0.00      0.00      0.00         1\n",
      "4535906998436233216       0.00      0.00      0.00         1\n",
      "4536015791000002940       0.00      0.00      0.00         1\n",
      "4536118143323471872       0.00      0.00      0.00         1\n",
      "4536122094693384192       0.00      0.00      0.00         1\n",
      "4536145077063385088       0.00      0.00      0.00         1\n",
      "4536212761453002752       0.00      0.00      0.00         1\n",
      "4536218379270225920       0.00      0.00      0.00         1\n",
      "4536244166253871104       0.00      0.00      0.00         1\n",
      "4536265594917624677       0.00      0.00      0.00         1\n",
      "4536282336670417844       0.00      0.00      0.00         1\n",
      "4536295324675123562       0.00      0.00      0.00         1\n",
      "4536300998304494676       0.00      0.00      0.00         1\n",
      "4536310734994230502       0.00      0.00      0.00         1\n",
      "4536311258968528736       0.00      0.00      0.00         1\n",
      "4536319797396664866       0.00      0.00      0.00         1\n",
      "4536338879938947500       0.00      0.00      0.00         1\n",
      "4536359951034978987       0.00      0.00      0.00         1\n",
      "4536401628344877056       0.00      0.00      0.00         1\n",
      "4536438097939773232       0.00      0.00      0.00         1\n",
      "4536508204718407264       0.00      0.00      0.00         1\n",
      "4536547648643006464       0.00      0.00      0.00         1\n",
      "4536574677931681391       0.00      0.00      0.00         1\n",
      "4536583409594262008       0.00      0.00      0.00         1\n",
      "4536598763048796160       0.00      0.00      0.00         1\n",
      "4536649461901877803       0.00      0.00      0.00         1\n",
      "4536837310885252633       0.00      0.00      0.00         1\n",
      "4536868697448382464       0.00      0.00      0.00         1\n",
      "4536869758305304576       0.00      0.00      0.00         1\n",
      "4536945909137283192       0.00      0.00      0.00         1\n",
      "4536963715009871872       0.00      0.00      0.00         1\n",
      "4536983068132507648       0.00      0.00      0.00         1\n",
      "4536989649072842462       0.00      0.00      0.00         1\n",
      "4537034033279194024       0.00      0.00      0.00         1\n",
      "4537046445722049312       0.00      0.00      0.00         1\n",
      "4537081741766531334       0.00      0.00      0.00         1\n",
      "4537125773782644810       0.00      0.00      0.00         1\n",
      "4537154008889992228       0.00      0.00      0.00         1\n",
      "4537224154270449648       0.00      0.00      0.00         1\n",
      "4537276431578693632       0.00      0.00      0.00         1\n",
      "4537281105567556136       0.00      0.00      0.00         1\n",
      "4537377278472174725       0.00      0.00      0.00         1\n",
      "4537386064913891328       0.00      0.00      0.00         1\n",
      "4537393131174412600       0.00      0.00      0.00         1\n",
      "4537495672479285248       0.00      0.00      0.00         1\n",
      "4537527214719107072       0.00      0.00      0.00         1\n",
      "4537563207590846238       0.00      0.00      0.00         1\n",
      "4537674266848734944       0.00      0.00      0.00         1\n",
      "4537753053743688723       0.00      0.00      0.00         1\n",
      "4537810931668746240       0.00      0.00      0.00         1\n",
      "4537850339068702557       0.00      0.00      0.00         1\n",
      "4537860432208893184       0.00      0.00      0.00         1\n",
      "4537873832525373632       0.00      0.00      0.00         1\n",
      "4537877838669283328       0.00      0.00      0.00         1\n",
      "4537973616439984128       0.00      0.00      0.00         1\n",
      "4537985136586220886       0.00      0.00      0.00         1\n",
      "4537991122726682624       0.00      0.00      0.00         1\n",
      "4537992960972685312       0.00      0.00      0.00         1\n",
      "4537993511769462272       0.00      0.00      0.00         1\n",
      "4538015940111196482       0.00      0.00      0.00         1\n",
      "4538044974073053555       0.00      0.00      0.00         1\n",
      "4538057936301608720       0.00      0.00      0.00         1\n",
      "4538060934185546614       0.00      0.00      0.00         1\n",
      "4538075188121567232       0.00      0.00      0.00         1\n",
      "4538081429775201301       0.00      0.00      0.00         1\n",
      "4538097303971876362       0.00      0.00      0.00         1\n",
      "4538117490317145325       0.00      0.00      0.00         1\n",
      "4538153103123283968       0.00      0.00      0.00         1\n",
      "4538161908830808096       0.00      0.00      0.00         1\n",
      "4538203186736922624       0.00      0.00      0.00         1\n",
      "4538203246866464768       0.00      0.00      0.00         1\n",
      "4538232121931595776       0.00      0.00      0.00         1\n",
      "4538248409472369792       0.00      0.00      0.00         1\n",
      "4538277356527157248       0.00      0.00      0.00         1\n",
      "4538334850006537126       0.00      0.00      0.00         1\n",
      "4538358866416500736       0.00      0.00      0.00         1\n",
      "4538376906295961024       0.00      0.00      0.00         1\n",
      "4538398630271061494       0.00      0.00      0.00         1\n",
      "4538415427881649368       0.00      0.00      0.00         1\n",
      "4538458648162527580       0.00      0.00      0.00         1\n",
      "4538472073164488704       0.00      0.00      0.00         1\n",
      "4538578314535709149       0.00      0.00      0.00         1\n",
      "4538618187951898624       0.00      0.00      0.00         1\n",
      "4538658792572715008       0.00      0.00      0.00         1\n",
      "4538663586806683099       0.00      0.00      0.00         1\n",
      "4538669857467731546       0.00      0.00      0.00         1\n",
      "4538674061181452288       0.00      0.00      0.00         1\n",
      "4538676900154834944       0.00      0.00      0.00         1\n",
      "4538711312494818226       0.00      0.00      0.00         1\n",
      "4538753334449849176       0.00      0.00      0.00         1\n",
      "4538769379390652416       0.00      0.00      0.00         1\n",
      "4538804556227577388       0.00      0.00      0.00         1\n",
      "4538829517522731008       0.00      0.00      0.00         1\n",
      "4538843026242359420       0.00      0.00      0.00         1\n",
      "4538967739212281272       0.00      0.00      0.00         1\n",
      "4539066416092288244       0.00      0.00      0.00         1\n",
      "4539238717268755152       0.00      0.00      0.00         1\n",
      "4539250429652577216       0.00      0.00      0.00         1\n",
      "4539255098291020687       0.00      0.00      0.00         1\n",
      "4539334993257912304       0.00      0.00      0.00         1\n",
      "4539341860946048660       0.00      0.00      0.00         1\n",
      "4539376757549428572       0.00      0.00      0.00         1\n",
      "4539436182699642920       0.00      0.00      0.00         1\n",
      "4539474051431616112       0.00      0.00      0.00         1\n",
      "4539476606936499980       0.00      0.00      0.00         1\n",
      "4539541541496029184       0.00      0.00      0.00         1\n",
      "4539580518324240384       0.00      0.00      0.00         1\n",
      "4539638697951232000       0.00      0.00      0.00         1\n",
      "4539643538379374592       0.00      0.00      0.00         1\n",
      "4539661857468356422       0.00      0.00      0.00         1\n",
      "4539678546657804288       0.00      0.00      0.00         1\n",
      "4539704754548244480       0.00      0.00      0.00         1\n",
      "4539727261238415488       0.00      0.00      0.00         1\n",
      "4539730137804963840       0.00      0.00      0.00         1\n",
      "4539736945328128000       0.00      0.00      0.00         1\n",
      "4539758763761991680       0.00      0.00      0.00         1\n",
      "4539787064347838343       0.00      0.00      0.00         1\n",
      "4539825040468131890       0.00      0.00      0.00         1\n",
      "4539831864105369600       0.00      0.00      0.00         1\n",
      "4539832693034057728       0.00      0.00      0.00         1\n",
      "4539875342059307008       0.00      0.00      0.00         1\n",
      "4539875630897205171       0.00      0.00      0.00         1\n",
      "4539903608288583022       0.00      0.00      0.00         1\n",
      "4539914297412681728       0.00      0.00      0.00         1\n",
      "4539916299890876256       0.00      0.00      0.00         1\n",
      "4539923553067204608       0.00      0.00      0.00         1\n",
      "4539944496387291791       0.00      0.00      0.00         1\n",
      "4539947120613411393       0.00      0.00      0.00         1\n",
      "4539950728385917730       0.00      0.00      0.00         1\n",
      "4539961925353589212       0.00      0.00      0.00         1\n",
      "4539999947650498560       0.00      0.00      0.00         1\n",
      "4540006115223535616       0.00      0.00      0.00         1\n",
      "4540021504091357184       0.00      0.00      0.00         1\n",
      "4540023154381621664       0.00      0.00      0.00         1\n",
      "4540030571826653108       0.00      0.00      0.00         1\n",
      "4540108580258316288       0.00      0.00      0.00         1\n",
      "4540114052046651392       0.00      0.00      0.00         1\n",
      "4540115074248867840       0.00      0.00      0.00         1\n",
      "4540118266470413869       0.00      0.00      0.00         1\n",
      "4540169925276205056       0.00      0.00      0.00         1\n",
      "4540237751399743488       0.00      0.00      0.00         1\n",
      "4540257848578685154       0.00      0.00      0.00         1\n",
      "4540280159906824192       0.00      0.00      0.00         1\n",
      "4540285714329521152       0.00      0.00      0.00         1\n",
      "4540310698174305252       0.00      0.00      0.00         1\n",
      "4540362538406853168       0.00      0.00      0.00         1\n",
      "4540372845301071872       0.00      0.00      0.00         1\n",
      "4540383273481666560       0.00      0.00      0.00         1\n",
      "4540422070958064397       0.00      0.00      0.00         1\n",
      "4540427001602948510       0.00      0.00      0.00         1\n",
      "4540431411475120128       0.00      0.00      0.00         1\n",
      "4540454140442050560       0.00      0.00      0.00         1\n",
      "4540478317850938104       0.00      0.00      0.00         1\n",
      "4540488956502441222       0.00      0.00      0.00         1\n",
      "4540517559929143296       0.00      0.00      0.00         1\n",
      "4540562662395889232       0.00      0.00      0.00         1\n",
      "4540568141758988288       0.00      0.00      0.00         1\n",
      "4540596535787782144       0.00      0.00      0.00         1\n",
      "4540609481882570987       0.00      0.00      0.00         1\n",
      "4540614888183037952       0.00      0.00      0.00         1\n",
      "4540650978793226240       0.00      0.00      0.00         1\n",
      "4540699176916221952       0.00      0.00      0.00         1\n",
      "4540716605893509120       0.00      0.00      0.00         1\n",
      "4540722533997571942       0.00      0.00      0.00         1\n",
      "4540734641513028924       0.00      0.00      0.00         1\n",
      "4540744883958185984       0.00      0.00      0.00         1\n",
      "4540745469120422656       0.00      0.00      0.00         1\n",
      "4540751987834093568       0.00      0.00      0.00         1\n",
      "4540781034697916416       0.00      0.00      0.00         1\n",
      "4540785687210316022       0.00      0.00      0.00         1\n",
      "4540794130053201920       0.00      0.00      0.00         1\n",
      "4540798267154735632       0.00      0.00      0.00         1\n",
      "4540870206808915968       0.00      0.00      0.00         1\n",
      "4540874824949545960       0.00      0.00      0.00         1\n",
      "4540890836582620194       0.00      0.00      0.00         1\n",
      "4540895088612346885       0.00      0.00      0.00         1\n",
      "4540898885359134540       0.00      0.00      0.00         1\n",
      "4540899528550645760       0.00      0.00      0.00         1\n",
      "4540916012635127808       0.00      0.00      0.00         1\n",
      "4540922077128949760       0.00      0.00      0.00         1\n",
      "4540956618304002088       0.00      0.00      0.00         1\n",
      "4540959529243770880       0.00      0.00      0.00         1\n",
      "4540978539826132912       0.00      0.00      0.00         1\n",
      "4540979870208884736       0.00      0.00      0.00         1\n",
      "4540990020247624856       0.00      0.00      0.00         1\n",
      "4540995290171824160       0.00      0.00      0.00         1\n",
      "4541019135849118543       0.00      0.00      0.00         1\n",
      "4541026934460514304       0.00      0.00      0.00         1\n",
      "4541038483627573248       0.00      0.00      0.00         1\n",
      "4541042203069251584       0.00      0.00      0.00         1\n",
      "4541088173146473426       0.00      0.00      0.00         1\n",
      "4541099231645007872       0.00      0.00      0.00         1\n",
      "4541107473687248896       0.00      0.00      0.00         1\n",
      "4541134726306902965       0.00      0.00      0.00         1\n",
      "4541148869634865865       0.00      0.00      0.00         1\n",
      "4541168978676813140       0.00      0.00      0.00         1\n",
      "4541169007683698688       0.00      0.00      0.00         1\n",
      "4541226521590759424       0.00      0.00      0.00         1\n",
      "4541227776780359492       0.00      0.00      0.00         1\n",
      "4541266603267873259       0.00      0.00      0.00         1\n",
      "4541296410357761680       0.00      0.00      0.00         1\n",
      "4541299475905249280       0.00      0.00      0.00         1\n",
      "4541305901176324096       0.00      0.00      0.00         1\n",
      "4541310977827667968       0.00      0.00      0.00         1\n",
      "4541327067831480463       0.00      0.00      0.00         1\n",
      "4541359321979551744       0.00      0.00      0.00         1\n",
      "4541364769020267776       0.00      0.00      0.00         1\n",
      "4541395959091871335       0.00      0.00      0.00         1\n",
      "4541454190272147892       0.00      0.00      0.00         1\n",
      "4541475759594131032       0.00      0.00      0.00         1\n",
      "4541477426052399288       0.00      0.00      0.00         1\n",
      "4541479977235788488       0.00      0.00      0.00         1\n",
      "4541529373670068364       0.00      0.00      0.00         1\n",
      "4541551388622061568       0.00      0.00      0.00         1\n",
      "4541552208960815104       0.00      0.00      0.00         1\n",
      "4541554463818645504       0.00      0.00      0.00         1\n",
      "4541560533672679097       0.00      0.00      0.00         1\n",
      "4541561103838085120       0.00      0.00      0.00         1\n",
      "4541568572786212864       0.00      0.00      0.00         1\n",
      "4541575901062617211       0.00      0.00      0.00         1\n",
      "4541590081982431232       0.00      0.00      0.00         1\n",
      "4541600261054922752       0.00      0.00      0.00         1\n",
      "4541623051221505960       0.00      0.00      0.00         1\n",
      "4541647833179425151       0.00      0.00      0.00         1\n",
      "4541656019377563036       0.00      0.00      0.00         1\n",
      "4541661435329193140       0.00      0.00      0.00         1\n",
      "4541710452735868928       0.00      0.00      0.00         1\n",
      "4541725467941535744       0.00      0.00      0.00         1\n",
      "4541730944024838144       0.00      0.00      0.00         1\n",
      "4541733311603985756       0.00      0.00      0.00         1\n",
      "4541764372805573011       0.00      0.00      0.00         1\n",
      "4541776122785824768       0.00      0.00      0.00         1\n",
      "4541794427936440320       0.00      0.00      0.00         1\n",
      "4541811221258567680       0.00      0.00      0.00         1\n",
      "4541837653543348440       0.00      0.00      0.00         1\n",
      "4541840822173171712       0.00      0.00      0.00         1\n",
      "4541844409499509520       0.00      0.00      0.00         1\n",
      "4541858063157199104       0.00      0.00      0.00         1\n",
      "4541864406858926944       0.00      0.00      0.00         1\n",
      "4541914403552886784       0.00      0.00      0.00         1\n",
      "4541917179182026138       0.00      0.00      0.00         1\n",
      "4541936747038526924       0.00      0.00      0.00         1\n",
      "4541947480165133040       0.00      0.00      0.00         1\n",
      "4541950246112649666       0.00      0.00      0.00         1\n",
      "4541978742162980864       0.00      0.00      0.00         1\n",
      "4541993499670609920       0.00      0.00      0.00         1\n",
      "4542030802510120191       0.00      0.00      0.00         1\n",
      "4542050147055239926       0.00      0.00      0.00         1\n",
      "4542101350594379776       0.00      0.00      0.00         1\n",
      "4542121575595376640       0.00      0.00      0.00         1\n",
      "4542128603215766500       0.00      0.00      0.00         1\n",
      "4542143098727194778       0.00      0.00      0.00         1\n",
      "4542157365557854208       0.00      0.00      0.00         1\n",
      "4542173854993230233       0.00      0.00      0.00         1\n",
      "4542189050590019332       0.00      0.00      0.00         1\n",
      "4542230229678030848       0.00      0.00      0.00         1\n",
      "4542245696913623956       0.00      0.00      0.00         1\n",
      "4542247834748977152       0.00      0.00      0.00         1\n",
      "4542254608978391560       0.00      0.00      0.00         1\n",
      "4542262296953719396       0.00      0.00      0.00         1\n",
      "4542301286681955094       0.00      0.00      0.00         1\n",
      "4542318894982889472       0.00      0.00      0.00         1\n",
      "4542330277714038342       0.00      0.00      0.00         1\n",
      "4542330809222168576       0.00      0.00      0.00         1\n",
      "4542350554244110311       0.00      0.00      0.00         1\n",
      "4542366070903668736       0.00      0.00      0.00         1\n",
      "4542392344275058578       0.00      0.00      0.00         1\n",
      "4542414775832805376       0.00      0.00      0.00         1\n",
      "4542424589833076736       0.00      0.00      0.00         1\n",
      "4542472913569695981       0.00      0.00      0.00         1\n",
      "4542476139082955994       0.00      0.00      0.00         1\n",
      "4542479887537012736       0.00      0.00      0.00         1\n",
      "4542551574836150272       0.00      0.00      0.00         1\n",
      "4542634919740515701       0.00      0.00      0.00         1\n",
      "4542639295248203776       0.00      0.00      0.00         1\n",
      "4542665800561167974       0.00      0.00      0.00         1\n",
      "4542700159229755392       0.00      0.00      0.00         1\n",
      "4542729159914720705       0.00      0.00      0.00         1\n",
      "4542743024060734550       0.00      0.00      0.00         1\n",
      "4542758570784980992       0.00      0.00      0.00         1\n",
      "4542785174854041582       0.00      0.00      0.00         1\n",
      "4542786707115737088       0.00      0.00      0.00         1\n",
      "4542795035057324032       0.00      0.00      0.00         1\n",
      "4542864249505077728       0.00      0.00      0.00         1\n",
      "4542883584398065664       0.00      0.00      0.00         1\n",
      "4542927703357185451       0.00      0.00      0.00         1\n",
      "4542927905225905875       0.00      0.00      0.00         1\n",
      "4542941949771270597       0.00      0.00      0.00         1\n",
      "4542945771229544448       0.00      0.00      0.00         1\n",
      "4542963578163953664       0.00      0.00      0.00         1\n",
      "4542972714594235264       0.00      0.00      0.00         1\n",
      "4542998733532103817       0.00      0.00      0.00         1\n",
      "4542999784738258944       0.00      0.00      0.00         1\n",
      "4543023072050937856       0.00      0.00      0.00         1\n",
      "4543030102912401408       0.00      0.00      0.00         1\n",
      "4543105075861520384       0.00      0.00      0.00         1\n",
      "4543107760216080384       0.00      0.00      0.00         1\n",
      "4543109735901036544       0.00      0.00      0.00         1\n",
      "4543117596758510318       0.00      0.00      0.00         1\n",
      "4543154619356703280       0.00      0.00      0.00         1\n",
      "4543166953455353856       0.00      0.00      0.00         1\n",
      "4543212060259224254       0.00      0.00      0.00         1\n",
      "4543257286230624960       0.00      0.00      0.00         1\n",
      "4543310913231636368       0.00      0.00      0.00         1\n",
      "4543340813731495936       0.00      0.00      0.00         1\n",
      "4543341115424484089       0.00      0.00      0.00         1\n",
      "4543345907562708992       0.00      0.00      0.00         1\n",
      "4543365526973317120       0.00      0.00      0.00         1\n",
      "4543366210894527872       0.00      0.00      0.00         1\n",
      "4543373653051441152       0.00      0.00      0.00         1\n",
      "4543377363903184896       0.00      0.00      0.00         1\n",
      "4543398314753654784       0.00      0.00      0.00         1\n",
      "4543410470567125051       0.00      0.00      0.00         1\n",
      "4543415070478350814       0.00      0.00      0.00         1\n",
      "4543418398020730880       0.00      0.00      0.00         1\n",
      "4543420657173528576       0.00      0.00      0.00         1\n",
      "4543433632269729792       0.00      0.00      0.00         1\n",
      "4543446041496234199       0.00      0.00      0.00         1\n",
      "4543482557251307456       0.00      0.00      0.00         1\n",
      "4543500158072002424       0.00      0.00      0.00         1\n",
      "4543508327115671955       0.00      0.00      0.00         1\n",
      "4543534096916234984       0.00      0.00      0.00         1\n",
      "4543567120909192518       0.00      0.00      0.00         1\n",
      "4543576419513580306       0.00      0.00      0.00         1\n",
      "4543585993007307321       0.00      0.00      0.00         1\n",
      "4543626705000384836       0.00      0.00      0.00         1\n",
      "4543681645156237312       0.00      0.00      0.00         1\n",
      "4543684900741447680       0.00      0.00      0.00         1\n",
      "4543695729380677248       0.00      0.00      0.00         1\n",
      "4543720896862355456       0.00      0.00      0.00         1\n",
      "4543742238554849280       0.00      0.00      0.00         1\n",
      "4543757408379338752       0.00      0.00      0.00         1\n",
      "4543848770923659264       0.00      0.00      0.00         1\n",
      "4543870297299746816       0.00      0.00      0.00         1\n",
      "4543877754414232640       0.00      0.00      0.00         1\n",
      "4543896711348617216       0.00      0.00      0.00         1\n",
      "4543908858543790982       0.00      0.00      0.00         1\n",
      "4543920394856967838       0.00      0.00      0.00         1\n",
      "4543930453668946450       0.00      0.00      0.00         1\n",
      "4543931647657334248       0.00      0.00      0.00         1\n",
      "4543935860996867296       0.00      0.00      0.00         1\n",
      "4543965612264208902       0.00      0.00      0.00         1\n",
      "4543975893365686272       0.00      0.00      0.00         1\n",
      "4544024804453253120       0.00      0.00      0.00         1\n",
      "4544064877556963507       0.00      0.00      0.00         1\n",
      "4544072385161186584       0.00      0.00      0.00         1\n",
      "4544107658667360256       0.00      0.00      0.00         1\n",
      "4544109698776825856       0.00      0.00      0.00         1\n",
      "4544149488410890143       0.00      0.00      0.00         1\n",
      "4544158284503545548       0.00      0.00      0.00         1\n",
      "4544181378537841030       0.00      0.00      0.00         1\n",
      "4544204110747926528       0.00      0.00      0.00         1\n",
      "4544241107596214272       0.00      0.00      0.00         1\n",
      "4544267534529986560       0.00      0.00      0.00         1\n",
      "4544291321100598784       0.00      0.00      0.00         1\n",
      "4544299644738494092       0.00      0.00      0.00         1\n",
      "4544320663275438080       0.00      0.00      0.00         1\n",
      "4544334849552416768       0.00      0.00      0.00         1\n",
      "4544335795503403677       0.00      0.00      0.00         1\n",
      "4544335992013717504       0.00      0.00      0.00         1\n",
      "4544339019965661184       0.00      0.00      0.00         1\n",
      "4544348000742277120       0.00      0.00      0.00         1\n",
      "4544358995858554880       0.00      0.00      0.00         1\n",
      "4544366633359903988       0.00      0.00      0.00         1\n",
      "4544408095924682752       0.00      0.00      0.00         1\n",
      "4544410436681859072       0.00      0.00      0.00         1\n",
      "4544415375894249472       0.00      0.00      0.00         1\n",
      "4544459842722946144       0.00      0.00      0.00         1\n",
      "4544488335558805574       0.00      0.00      0.00         1\n",
      "4544498798111648795       0.00      0.00      0.00         1\n",
      "4544515328373161984       0.00      0.00      0.00         1\n",
      "4544525117621505472       0.00      0.00      0.00         1\n",
      "4544537683677937664       0.00      0.00      0.00         1\n",
      "4544562637437927424       0.00      0.00      0.00         1\n",
      "4544568020086987588       0.00      0.00      0.00         1\n",
      "4544591608018619728       0.00      0.00      0.00         1\n",
      "4544633238110339072       0.00      0.00      0.00         1\n",
      "4544633281060012032       0.00      0.00      0.00         1\n",
      "4544666189099433984       0.00      0.00      0.00         1\n",
      "4544716143864053760       0.00      0.00      0.00         1\n",
      "4544803207146110976       0.00      0.00      0.00         1\n",
      "4544814516861655706       0.00      0.00      0.00         1\n",
      "4544821560575752048       0.00      0.00      0.00         1\n",
      "4544823888467751183       0.00      0.00      0.00         1\n",
      "4544867279468232704       0.00      0.00      0.00         1\n",
      "4544894011344683008       0.00      0.00      0.00         1\n",
      "4544917952552362801       0.00      0.00      0.00         1\n",
      "4544925527814701056       0.00      0.00      0.00         1\n",
      "4544933645302890496       0.00      0.00      0.00         1\n",
      "4544935952751734999       0.00      0.00      0.00         1\n",
      "4544945018376290304       0.00      0.00      0.00         1\n",
      "4544945788234063198       0.00      0.00      0.00         1\n",
      "4544966790625928944       0.00      0.00      0.00         1\n",
      "4544975595298783245       0.00      0.00      0.00         1\n",
      "4544989686036168704       0.00      0.00      0.00         1\n",
      "4545001481065780218       0.00      0.00      0.00         1\n",
      "4545005965027770988       0.00      0.00      0.00         1\n",
      "4545036261722826032       0.00      0.00      0.00         1\n",
      "4545046646951616228       0.00      0.00      0.00         1\n",
      "4545073399243735040       0.00      0.00      0.00         1\n",
      "4545079322003636224       0.00      0.00      0.00         1\n",
      "4545082600125729754       0.00      0.00      0.00         1\n",
      "4545129312173199416       0.00      0.00      0.00         1\n",
      "4545144353146877518       0.00      0.00      0.00         1\n",
      "4545177234373083136       0.00      0.00      0.00         1\n",
      "4545210988521062400       0.00      0.00      0.00         1\n",
      "4545233701356194968       0.00      0.00      0.00         1\n",
      "4545238570801037312       0.00      0.00      0.00         1\n",
      "4545263455841550336       0.00      0.00      0.00         1\n",
      "4545269959441736608       0.00      0.00      0.00         1\n",
      "4545310160346499028       0.00      0.00      0.00         1\n",
      "4545313286052118528       0.00      0.00      0.00         1\n",
      "4545388092558896496       0.00      0.00      0.00         1\n",
      "4545408295023673344       0.00      0.00      0.00         1\n",
      "4545417799786299392       0.00      0.00      0.00         1\n",
      "4545421304479612928       0.00      0.00      0.00         1\n",
      "4545446207753681782       0.00      0.00      0.00         1\n",
      "4545450003451084800       0.00      0.00      0.00         1\n",
      "4545476511989235712       0.00      0.00      0.00         1\n",
      "4545522867571261440       0.00      0.00      0.00         1\n",
      "4545524478183997440       0.00      0.00      0.00         1\n",
      "4545528718363699736       0.00      0.00      0.00         1\n",
      "4545543411455550367       0.00      0.00      0.00         1\n",
      "4545556819287736320       0.00      0.00      0.00         1\n",
      "4545568210588464292       0.00      0.00      0.00         1\n",
      "4545589809999547703       0.00      0.00      0.00         1\n",
      "4545592021864705672       0.00      0.00      0.00         1\n",
      "4545631071743414346       0.00      0.00      0.00         1\n",
      "4545653799649280000       0.00      0.00      0.00         1\n",
      "4545655990082600960       0.00      0.00      0.00         1\n",
      "4545660002649352256       0.00      0.00      0.00         1\n",
      "4545691604987722408       0.00      0.00      0.00         1\n",
      "4545712671814814848       0.00      0.00      0.00         1\n",
      "4545730199587156991       0.00      0.00      0.00         1\n",
      "4545763093682061312       0.00      0.00      0.00         1\n",
      "4545775185056912454       0.00      0.00      0.00         1\n",
      "4545786824434450554       0.00      0.00      0.00         1\n",
      "4545803779907256320       0.00      0.00      0.00         1\n",
      "4545808908098207744       0.00      0.00      0.00         1\n",
      "4545809105666703360       0.00      0.00      0.00         1\n",
      "4545820694518123960       0.00      0.00      0.00         1\n",
      "4545821940077406532       0.00      0.00      0.00         1\n",
      "4545836705126547456       0.00      0.00      0.00         1\n",
      "4545845781455657204       0.00      0.00      0.00         1\n",
      "4545860878262948179       0.00      0.00      0.00         1\n",
      "4545861350704941056       0.00      0.00      0.00         1\n",
      "4545916136251719680       0.00      0.00      0.00         1\n",
      "4545927965646198800       0.00      0.00      0.00         1\n",
      "4545936610360819712       0.00      0.00      0.00         1\n",
      "4545979024194592320       0.00      0.00      0.00         1\n",
      "4545981050387431424       0.00      0.00      0.00         1\n",
      "4546030434976239084       0.00      0.00      0.00         1\n",
      "4546048644582735872       0.00      0.00      0.00         1\n",
      "4546096465781231504       0.00      0.00      0.00         1\n",
      "4546115272410398720       0.00      0.00      0.00         1\n",
      "4546132491985357432       0.00      0.00      0.00         1\n",
      "4546139097643706421       0.00      0.00      0.00         1\n",
      "4546142347884232704       0.00      0.00      0.00         1\n",
      "4546146098455123962       0.00      0.00      0.00         1\n",
      "4546146235890407802       0.00      0.00      0.00         1\n",
      "4546168671738789888       0.00      0.00      0.00         1\n",
      "4546190877759909236       0.00      0.00      0.00         1\n",
      "4546197431894860827       0.00      0.00      0.00         1\n",
      "4546201980266356646       0.00      0.00      0.00         1\n",
      "4546211080245870592       0.00      0.00      0.00         1\n",
      "4546231684230837360       0.00      0.00      0.00         1\n",
      "4546353432641929216       0.00      0.00      0.00         1\n",
      "4546386203242397696       0.00      0.00      0.00         1\n",
      "4546440165211504640       0.00      0.00      0.00         1\n",
      "4546442999889920000       0.00      0.00      0.00         1\n",
      "4546452577666990080       0.00      0.00      0.00         1\n",
      "4546458252370072653       0.00      0.00      0.00         1\n",
      "4546459008253285376       0.00      0.00      0.00         1\n",
      "4546467465081029152       0.00      0.00      0.00         1\n",
      "4546548743022889386       0.00      0.00      0.00         1\n",
      "4546567606536069673       0.00      0.00      0.00         1\n",
      "4546593423589534568       0.00      0.00      0.00         1\n",
      "4546599727541518336       0.00      0.00      0.00         1\n",
      "4546610057997104046       0.00      0.00      0.00         1\n",
      "4546674121729686242       0.00      0.00      0.00         1\n",
      "4546674375132367114       0.00      0.00      0.00         1\n",
      "4546718050663675750       0.00      0.00      0.00         1\n",
      "4546738142512675984       0.00      0.00      0.00         1\n",
      "4546763074787074826       0.00      0.00      0.00         1\n",
      "4546810495506090836       0.00      0.00      0.00         1\n",
      "4546829598486167552       0.00      0.00      0.00         1\n",
      "4546874292958694464       0.00      0.00      0.00         1\n",
      "4546901097859977374       0.00      0.00      0.00         1\n",
      "4546906306602074112       0.00      0.00      0.00         1\n",
      "4546913891514318848       0.00      0.00      0.00         1\n",
      "4546943737242058752       0.00      0.00      0.00         1\n",
      "4546946255158236158       0.00      0.00      0.00         1\n",
      "4546948710814187520       0.00      0.00      0.00         1\n",
      "4546952981072238318       0.00      0.00      0.00         1\n",
      "4546977604113862172       0.00      0.00      0.00         1\n",
      "4546993516958376571       0.00      0.00      0.00         1\n",
      "4547009464184214192       0.00      0.00      0.00         1\n",
      "4547016880535109632       0.00      0.00      0.00         1\n",
      "4547032131963977728       0.00      0.00      0.00         1\n",
      "4547033360324624384       0.00      0.00      0.00         1\n",
      "4547033773678205768       0.00      0.00      0.00         1\n",
      "4547043806726394344       0.00      0.00      0.00         1\n",
      "4547066178169733120       0.00      0.00      0.00         1\n",
      "4547116356272652288       0.00      0.00      0.00         1\n",
      "4547128034288730112       0.00      0.00      0.00         1\n",
      "4547146851597915463       0.00      0.00      0.00         1\n",
      "4547165191107976977       0.00      0.00      0.00         1\n",
      "4547169068406276096       0.00      0.00      0.00         1\n",
      "4547192515686520414       0.00      0.00      0.00         1\n",
      "4547203974648607887       0.00      0.00      0.00         1\n",
      "4547204879843590144       0.00      0.00      0.00         1\n",
      "4547234433513553920       0.00      0.00      0.00         1\n",
      "4547250493446826734       0.00      0.00      0.00         1\n",
      "4547299374459115224       0.00      0.00      0.00         1\n",
      "4547303961505561922       0.00      0.00      0.00         1\n",
      "4547324374423699456       0.00      0.00      0.00         1\n",
      "4547329258850882960       0.00      0.00      0.00         1\n",
      "4547347597311868928       0.00      0.00      0.00         1\n",
      "4547348448733707136       0.00      0.00      0.00         1\n",
      "4547365856259759472       0.00      0.00      0.00         1\n",
      "4547375442642385318       0.00      0.00      0.00         1\n",
      "4547380815656673189       0.00      0.00      0.00         1\n",
      "4547503797749299190       0.00      0.00      0.00         1\n",
      "4547519062049164547       0.00      0.00      0.00         1\n",
      "4547519657996713984       0.00      0.00      0.00         1\n",
      "4547584053491453394       0.00      0.00      0.00         1\n",
      "4547597737241050560       0.00      0.00      0.00         1\n",
      "4547611733505605632       0.00      0.00      0.00         1\n",
      "4547615375637872640       0.00      0.00      0.00         1\n",
      "4547677244641771520       0.00      0.00      0.00         1\n",
      "4547691547941184556       0.00      0.00      0.00         1\n",
      "4547723820322755186       0.00      0.00      0.00         1\n",
      "4547757720508751477       0.00      0.00      0.00         1\n",
      "4547875535718055712       0.00      0.00      0.00         1\n",
      "4547895885237179392       0.00      0.00      0.00         1\n",
      "4547898088616133270       0.00      0.00      0.00         1\n",
      "4547907954155875418       0.00      0.00      0.00         1\n",
      "4547948631806722808       0.00      0.00      0.00         1\n",
      "4547967180704055296       0.00      0.00      0.00         1\n",
      "4548008214821601280       0.00      0.00      0.00         1\n",
      "4548084447253182025       0.00      0.00      0.00         1\n",
      "4548105638624448535       0.00      0.00      0.00         1\n",
      "4548121718959577364       0.00      0.00      0.00         1\n",
      "4548123388664610816       0.00      0.00      0.00         1\n",
      "4548145129789063168       0.00      0.00      0.00         1\n",
      "4548150099066224640       0.00      0.00      0.00         1\n",
      "4548166312567767040       0.00      0.00      0.00         1\n",
      "4548229302558130176       0.00      0.00      0.00         1\n",
      "4548242015661326336       0.00      0.00      0.00         1\n",
      "4548250541171408896       0.00      0.00      0.00         1\n",
      "4548260974699254645       0.00      0.00      0.00         1\n",
      "4548268963327194120       0.00      0.00      0.00         1\n",
      "4548270397859389406       0.00      0.00      0.00         1\n",
      "4548393294062734526       0.00      0.00      0.00         1\n",
      "4548419337681108992       0.00      0.00      0.00         1\n",
      "4548421082503357184       0.00      0.00      0.00         1\n",
      "4548486287631319040       0.00      0.00      0.00         1\n",
      "4548503862637494272       0.00      0.00      0.00         1\n",
      "4548517125496504320       0.00      0.00      0.00         1\n",
      "4548552242215371799       0.00      0.00      0.00         1\n",
      "4548588653881851904       0.00      0.00      0.00         1\n",
      "4548610352056631296       0.00      0.00      0.00         1\n",
      "4548625613123923744       0.00      0.00      0.00         1\n",
      "4548631029077322028       0.00      0.00      0.00         1\n",
      "4548635109314878372       0.00      0.00      0.00         1\n",
      "4548639411805356032       0.00      0.00      0.00         1\n",
      "4548641396080246784       0.00      0.00      0.00         1\n",
      "4548647719333859892       0.00      0.00      0.00         1\n",
      "4548679415130750976       0.00      0.00      0.00         1\n",
      "4548694298230349888       0.00      0.00      0.00         1\n",
      "4548743380078690304       0.00      0.00      0.00         1\n",
      "4548745780965408768       0.00      0.00      0.00         1\n",
      "4548754177626472448       0.00      0.00      0.00         1\n",
      "4548755272843132928       0.00      0.00      0.00         1\n",
      "4548758061328624275       0.00      0.00      0.00         1\n",
      "4548777117046800384       0.00      0.00      0.00         1\n",
      "4548782177563581648       0.00      0.00      0.00         1\n",
      "4548822312987656192       0.00      0.00      0.00         1\n",
      "4548887712454672384       0.00      0.00      0.00         1\n",
      "4548947047427866624       0.00      0.00      0.00         1\n",
      "4548961710446215168       0.00      0.00      0.00         1\n",
      "4548962131353010176       0.00      0.00      0.00         1\n",
      "4548991015008075776       0.00      0.00      0.00         1\n",
      "4548997900897459180       0.00      0.00      0.00         1\n",
      "4549011016670773248       0.00      0.00      0.00         1\n",
      "4549015968768065536       0.00      0.00      0.00         1\n",
      "4549018710009861704       0.00      0.00      0.00         1\n",
      "4549022608787505152       0.00      0.00      0.00         1\n",
      "4549102689506050728       0.00      0.00      0.00         1\n",
      "4549111588667888044       0.00      0.00      0.00         1\n",
      "4549112176035495936       0.00      0.00      0.00         1\n",
      "4549136362037272616       0.00      0.00      0.00         1\n",
      "4549136843092385389       0.00      0.00      0.00         1\n",
      "4549159489395228672       0.00      0.00      0.00         1\n",
      "4549159696596229342       0.00      0.00      0.00         1\n",
      "4549184795342536704       0.00      0.00      0.00         1\n",
      "4549199021316091748       0.00      0.00      0.00         1\n",
      "4549228427915296768       0.00      0.00      0.00         1\n",
      "4549237508527517969       0.00      0.00      0.00         1\n",
      "4549247898061461704       0.00      0.00      0.00         1\n",
      "4549258244629980920       0.00      0.00      0.00         1\n",
      "4549321659819820362       0.00      0.00      0.00         1\n",
      "4549349826217445097       0.00      0.00      0.00         1\n",
      "4549355481637847040       0.00      0.00      0.00         1\n",
      "4549356638044625012       0.00      0.00      0.00         1\n",
      "4549358775877763072       0.00      0.00      0.00         1\n",
      "4549375956800131416       0.00      0.00      0.00         1\n",
      "4549376704124265698       0.00      0.00      0.00         1\n",
      "4549405578136387584       0.00      0.00      0.00         1\n",
      "4549454546098770930       0.00      0.00      0.00         1\n",
      "4549636983424055632       0.00      0.00      0.00         1\n",
      "4549664677390273610       0.00      0.00      0.00         1\n",
      "4549709915224014848       0.00      0.00      0.00         1\n",
      "4549716989035151360       0.00      0.00      0.00         1\n",
      "4549770775959241173       0.00      0.00      0.00         1\n",
      "4549771711213469696       0.00      0.00      0.00         1\n",
      "4549775696943120384       0.00      0.00      0.00         1\n",
      "4549776346541678210       0.00      0.00      0.00         1\n",
      "4549792520330018816       0.00      0.00      0.00         1\n",
      "4549800418774876160       0.00      0.00      0.00         1\n",
      "4549822934042717162       0.00      0.00      0.00         1\n",
      "4549852199965091574       0.00      0.00      0.00         1\n",
      "4549863031870305692       0.00      0.00      0.00         1\n",
      "4549889746560961108       0.00      0.00      0.00         1\n",
      "4549896309257949006       0.00      0.00      0.00         1\n",
      "4549910395707457536       0.00      0.00      0.00         1\n",
      "4549931402392502272       0.00      0.00      0.00         1\n",
      "4550001432885392158       0.00      0.00      0.00         1\n",
      "4550028559914313082       0.00      0.00      0.00         1\n",
      "4550061986578169856       0.00      0.00      0.00         1\n",
      "4550064251095507189       0.00      0.00      0.00         1\n",
      "4550067867438799661       0.00      0.00      0.00         1\n",
      "4550071561110942710       0.00      0.00      0.00         1\n",
      "4550131955890388992       0.00      0.00      0.00         1\n",
      "4550161634114404352       0.00      0.00      0.00         1\n",
      "4550200155676082176       0.00      0.00      0.00         1\n",
      "4550219319820156928       0.00      0.00      0.00         1\n",
      "4550231866473544937       0.00      0.00      0.00         1\n",
      "4550232140297535488       0.00      0.00      0.00         1\n",
      "4550232910117266280       0.00      0.00      0.00         1\n",
      "4550249436130836480       0.00      0.00      0.00         1\n",
      "4550280527399092224       0.00      0.00      0.00         1\n",
      "4550293923402088448       0.00      0.00      0.00         1\n",
      "4550327777383440980       0.00      0.00      0.00         1\n",
      "4550331658984751104       0.00      0.00      0.00         1\n",
      "4550361109575499776       0.00      0.00      0.00         1\n",
      "4550363510462218240       0.00      0.00      0.00         1\n",
      "4550364447797702184       0.00      0.00      0.00         1\n",
      "4550368918863987616       0.00      0.00      0.00         1\n",
      "4550380021368919709       0.00      0.00      0.00         1\n",
      "4550392179368919040       0.00      0.00      0.00         1\n",
      "4550418851115827200       0.00      0.00      0.00         1\n",
      "4550428184079761408       0.00      0.00      0.00         1\n",
      "4550478787384442880       0.00      0.00      0.00         1\n",
      "4550562578948607104       0.00      0.00      0.00         1\n",
      "4550580625353998336       0.00      0.00      0.00         1\n",
      "4550592775816478720       0.00      0.00      0.00         1\n",
      "4550609427404685312       0.00      0.00      0.00         1\n",
      "4550616798619073094       0.00      0.00      0.00         1\n",
      "4550620423549106816       0.00      0.00      0.00         1\n",
      "4550632071531975416       0.00      0.00      0.00         1\n",
      "4550635962769716422       0.00      0.00      0.00         1\n",
      "4550644963964092416       0.00      0.00      0.00         1\n",
      "4550673822906415672       0.00      0.00      0.00         1\n",
      "4550676060596012162       0.00      0.00      0.00         1\n",
      "4550684135073553652       0.00      0.00      0.00         1\n",
      "4550685707066366674       0.00      0.00      0.00         1\n",
      "4550702702277466196       0.00      0.00      0.00         1\n",
      "4550742632581647312       0.00      0.00      0.00         1\n",
      "4550745376004505600       0.00      0.00      0.00         1\n",
      "4550769982943467286       0.00      0.00      0.00         1\n",
      "4550774776119450991       0.00      0.00      0.00         1\n",
      "4550777833072361472       0.00      0.00      0.00         1\n",
      "4550790933774950373       0.00      0.00      0.00         1\n",
      "4550826327548100608       0.00      0.00      0.00         1\n",
      "4550852372229783552       0.00      0.00      0.00         1\n",
      "4550859979684606674       0.00      0.00      0.00         1\n",
      "4550864085666899390       0.00      0.00      0.00         1\n",
      "4550897818344827017       0.00      0.00      0.00         1\n",
      "4550900768971265739       0.00      0.00      0.00         1\n",
      "4550919714070960324       0.00      0.00      0.00         1\n",
      "4550927556689239957       0.00      0.00      0.00         1\n",
      "4550944494983315456       0.00      0.00      0.00         1\n",
      "4550970360330845286       0.00      0.00      0.00         1\n",
      "4550983330077605888       0.00      0.00      0.00         1\n",
      "4550996203155398101       0.00      0.00      0.00         1\n",
      "4551046229873655808       0.00      0.00      0.00         1\n",
      "4551050696639643648       0.00      0.00      0.00         1\n",
      "4551072236944376464       0.00      0.00      0.00         1\n",
      "4551080908474513512       0.00      0.00      0.00         1\n",
      "4551082897067187015       0.00      0.00      0.00         1\n",
      "4551110516944142336       0.00      0.00      0.00         1\n",
      "4551121185642905600       0.00      0.00      0.00         1\n",
      "4551122960519692467       0.00      0.00      0.00         1\n",
      "4551137283180331008       0.00      0.00      0.00         1\n",
      "4551213548914606080       0.00      0.00      0.00         1\n",
      "4551233736312977988       0.00      0.00      0.00         1\n",
      "4551261442094923776       0.00      0.00      0.00         1\n",
      "4551286528998899712       0.00      0.00      0.00         1\n",
      "4551287284913143808       0.00      0.00      0.00         1\n",
      "4551296374104735700       0.00      0.00      0.00         1\n",
      "4551319467103092736       0.00      0.00      0.00         1\n",
      "4551319754865901568       0.00      0.00      0.00         1\n",
      "4551336493418911838       0.00      0.00      0.00         1\n",
      "4551372639835753704       0.00      0.00      0.00         1\n",
      "4551374697144646218       0.00      0.00      0.00         1\n",
      "4551384699066384384       0.00      0.00      0.00         1\n",
      "4551387156828626688       0.00      0.00      0.00         1\n",
      "4551392349456974866       0.00      0.00      0.00         1\n",
      "4551397132996706304       0.00      0.00      0.00         1\n",
      "4551399715330534270       0.00      0.00      0.00         1\n",
      "4551459707432822872       0.00      0.00      0.00         1\n",
      "4551462510988886016       0.00      0.00      0.00         1\n",
      "4551487044876144800       0.00      0.00      0.00         1\n",
      "4551506564468441088       0.00      0.00      0.00         1\n",
      "4551531360340160120       0.00      0.00      0.00         1\n",
      "4551560556502319104       0.00      0.00      0.00         1\n",
      "4551571745950042762       0.00      0.00      0.00         1\n",
      "4551573389864599552       0.00      0.00      0.00         1\n",
      "4551647344906469376       0.00      0.00      0.00         1\n",
      "4551705365619671040       0.00      0.00      0.00         1\n",
      "4551713716095762824       0.00      0.00      0.00         1\n",
      "4551762746382745600       0.00      0.00      0.00         1\n",
      "4551767832699121600       0.00      0.00      0.00         1\n",
      "4551785811408087864       0.00      0.00      0.00         1\n",
      "4551817765946684328       0.00      0.00      0.00         1\n",
      "4551839910824829296       0.00      0.00      0.00         1\n",
      "4551841498903085056       0.00      0.00      0.00         1\n",
      "4551846666810589696       0.00      0.00      0.00         1\n",
      "4551893785834946560       0.00      0.00      0.00         1\n",
      "4551946768551510016       0.00      0.00      0.00         1\n",
      "4551962758714753024       0.00      0.00      0.00         1\n",
      "4551997999992024268       0.00      0.00      0.00         1\n",
      "4552001951316723232       0.00      0.00      0.00         1\n",
      "4552024507459567616       0.00      0.00      0.00         1\n",
      "4552030589133258752       0.00      0.00      0.00         1\n",
      "4552031818543609104       0.00      0.00      0.00         1\n",
      "4552031977474078515       0.00      0.00      0.00         1\n",
      "4552136485846908928       0.00      0.00      0.00         1\n",
      "4552196705583366144       0.00      0.00      0.00         1\n",
      "4552201078923738398       0.00      0.00      0.00         1\n",
      "4552214998913517438       0.00      0.00      0.00         1\n",
      "4552248137816735744       0.00      0.00      0.00         1\n",
      "4552275423743967232       0.00      0.00      0.00         1\n",
      "4552410766753398784       0.00      0.00      0.00         1\n",
      "4552427001729777664       0.00      0.00      0.00         1\n",
      "4552435355441168384       0.00      0.00      0.00         1\n",
      "4552525804208963164       0.00      0.00      0.00         1\n",
      "4552587551902269440       0.00      0.00      0.00         1\n",
      "4552591533336952832       0.00      0.00      0.00         1\n",
      "4552594674024565640       0.00      0.00      0.00         1\n",
      "4552598354804884767       0.00      0.00      0.00         1\n",
      "4552655619559313248       0.00      0.00      0.00         1\n",
      "4552659209136635904       0.00      0.00      0.00         1\n",
      "4552662086764724224       0.00      0.00      0.00         1\n",
      "4552666107917666288       0.00      0.00      0.00         1\n",
      "4552687066294517760       0.00      0.00      0.00         1\n",
      "4552687517266083840       0.00      0.00      0.00         1\n",
      "4552692736718612536       0.00      0.00      0.00         1\n",
      "4552695462955581440       0.00      0.00      0.00         1\n",
      "4552698955827523842       0.00      0.00      0.00         1\n",
      "4552701146240921746       0.00      0.00      0.00         1\n",
      "4552701958008724166       0.00      0.00      0.00         1\n",
      "4552712423781433344       0.00      0.00      0.00         1\n",
      "4552725773593484948       0.00      0.00      0.00         1\n",
      "4552736047149041906       0.00      0.00      0.00         1\n",
      "4552780061996631045       0.00      0.00      0.00         1\n",
      "4552787474039963648       0.00      0.00      0.00         1\n",
      "4552797598343920128       0.00      0.00      0.00         1\n",
      "4552801498167010446       0.00      0.00      0.00         1\n",
      "4552804143872957346       0.00      0.00      0.00         1\n",
      "4552815554536144896       0.00      0.00      0.00         1\n",
      "4552838825732379538       0.00      0.00      0.00         1\n",
      "4552843549132980224       0.00      0.00      0.00         1\n",
      "4552874953933848576       0.00      0.00      0.00         1\n",
      "4552963096314460940       0.00      0.00      0.00         1\n",
      "4552980266531946496       0.00      0.00      0.00         1\n",
      "4552983315958726656       0.00      0.00      0.00         1\n",
      "4552989475995402349       0.00      0.00      0.00         1\n",
      "4553000324029218816       0.00      0.00      0.00         1\n",
      "4553037433615690996       0.00      0.00      0.00         1\n",
      "4553047256174575280       0.00      0.00      0.00         1\n",
      "4553055317842290346       0.00      0.00      0.00         1\n",
      "4553070058164135368       0.00      0.00      0.00         1\n",
      "4553079013180194032       0.00      0.00      0.00         1\n",
      "4553124036267212800       0.00      0.00      0.00         1\n",
      "4553150537258210918       0.00      0.00      0.00         1\n",
      "4553179304977140919       0.00      0.00      0.00         1\n",
      "4553182874077803584       0.00      0.00      0.00         1\n",
      "4553183170441099435       0.00      0.00      0.00         1\n",
      "4553193850960609280       0.00      0.00      0.00         1\n",
      "4553196317316068041       0.00      0.00      0.00         1\n",
      "4553202265863963504       0.00      0.00      0.00         1\n",
      "4553208665371251596       0.00      0.00      0.00         1\n",
      "4553223388507929315       0.00      0.00      0.00         1\n",
      "4553238704346002850       0.00      0.00      0.00         1\n",
      "4553249380592779264       0.00      0.00      0.00         1\n",
      "4553294873926938900       0.00      0.00      0.00         1\n",
      "4553299512511830702       0.00      0.00      0.00         1\n",
      "4553332867218213532       0.00      0.00      0.00         1\n",
      "4553389749748924736       0.00      0.00      0.00         1\n",
      "4553391183233024000       0.00      0.00      0.00         1\n",
      "4553418843888421149       0.00      0.00      0.00         1\n",
      "4553426969967999747       0.00      0.00      0.00         1\n",
      "4553429632841128082       0.00      0.00      0.00         1\n",
      "4553430967515086848       0.00      0.00      0.00         1\n",
      "4553438762880729088       0.00      0.00      0.00         1\n",
      "4553485948444625328       0.00      0.00      0.00         1\n",
      "4553486316758630400       0.00      0.00      0.00         1\n",
      "4553503943304413184       0.00      0.00      0.00         1\n",
      "4553515502128038153       0.00      0.00      0.00         1\n",
      "4553540786601570230       0.00      0.00      0.00         1\n",
      "4553545128807708683       0.00      0.00      0.00         1\n",
      "4553546334631624704       0.00      0.00      0.00         1\n",
      "4553578770224644096       0.00      0.00      0.00         1\n",
      "4553612104515439978       0.00      0.00      0.00         1\n",
      "4553614650381434880       0.00      0.00      0.00         1\n",
      "4553616481102759507       0.00      0.00      0.00         1\n",
      "4553643921628773677       0.00      0.00      0.00         1\n",
      "4553654348764151808       0.00      0.00      0.00         1\n",
      "4553660281184054240       0.00      0.00      0.00         1\n",
      "4553705858306932736       0.00      0.00      0.00         1\n",
      "4553711356915699275       0.00      0.00      0.00         1\n",
      "4553741533323357696       0.00      0.00      0.00         1\n",
      "4553744024432800258       0.00      0.00      0.00         1\n",
      "4553744784629636648       0.00      0.00      0.00         1\n",
      "4553756308069437079       0.00      0.00      0.00         1\n",
      "4553774303971333798       0.00      0.00      0.00         1\n",
      "4553835477170829276       0.00      0.00      0.00         1\n",
      "4553840334782002036       0.00      0.00      0.00         1\n",
      "4553851995627492396       0.00      0.00      0.00         1\n",
      "4553886338183497070       0.00      0.00      0.00         1\n",
      "4553999815487730496       0.00      0.00      0.00         1\n",
      "4554063685978045485       0.00      0.00      0.00         1\n",
      "4554083880912666528       0.00      0.00      0.00         1\n",
      "4554086645812428800       0.00      0.00      0.00         1\n",
      "4554088600022548480       0.00      0.00      0.00         1\n",
      "4554088930735030272       0.00      0.00      0.00         1\n",
      "4554124621913260032       0.00      0.00      0.00         1\n",
      "4554217968732471296       0.00      0.00      0.00         1\n",
      "4554350860364614948       0.00      0.00      0.00         1\n",
      "4554376046046007072       0.00      0.00      0.00         1\n",
      "4554384166786957312       0.00      0.00      0.00         1\n",
      "4554426700899492536       0.00      0.00      0.00         1\n",
      "4554436097236533248       0.00      0.00      0.00         1\n",
      "4554463146940563456       0.00      0.00      0.00         1\n",
      "4554503890066830008       0.00      0.00      0.00         1\n",
      "4554504568648940872       0.00      0.00      0.00         1\n",
      "4554590682723021808       0.00      0.00      0.00         1\n",
      "4554612800781025280       0.00      0.00      0.00         1\n",
      "4554632042234511360       0.00      0.00      0.00         1\n",
      "4554659392586252288       0.00      0.00      0.00         1\n",
      "4554660647785092295       0.00      0.00      0.00         1\n",
      "4554674287532834816       0.00      0.00      0.00         1\n",
      "4554720124471763884       0.00      0.00      0.00         1\n",
      "4554726144967966720       0.00      0.00      0.00         1\n",
      "4554728495372481783       0.00      0.00      0.00         1\n",
      "4554736195191439360       0.00      0.00      0.00         1\n",
      "4554761270276424525       0.00      0.00      0.00         1\n",
      "4554786695416905728       0.00      0.00      0.00         1\n",
      "4554855888400535667       0.00      0.00      0.00         1\n",
      "4554882541907083264       0.00      0.00      0.00         1\n",
      "4554889973253788352       0.00      0.00      0.00         1\n",
      "4554941666426880000       0.00      0.00      0.00         1\n",
      "4555006830739855965       0.00      0.00      0.00         1\n",
      "4555010373018714112       0.00      0.00      0.00         1\n",
      "4555088581144084778       0.00      0.00      0.00         1\n",
      "4555104291068575744       0.00      0.00      0.00         1\n",
      "4555149766182305792       0.00      0.00      0.00         1\n",
      "4555150153801083334       0.00      0.00      0.00         1\n",
      "4555157226540498944       0.00      0.00      0.00         1\n",
      "4555171361277870080       0.00      0.00      0.00         1\n",
      "4555173869538770944       0.00      0.00      0.00         1\n",
      "4555236378492796928       0.00      0.00      0.00         1\n",
      "4555264734931939962       0.00      0.00      0.00         1\n",
      "4555311614483338728       0.00      0.00      0.00         1\n",
      "4555322038347032501       0.00      0.00      0.00         1\n",
      "4555328307972800512       0.00      0.00      0.00         1\n",
      "4555360709206081536       0.00      0.00      0.00         1\n",
      "4555373951655823583       0.00      0.00      0.00         1\n",
      "4555373954885222400       0.00      0.00      0.00         1\n",
      "4555417866630856704       0.00      0.00      0.00         1\n",
      "4555425736075033336       0.00      0.00      0.00         1\n",
      "4555457518833991317       0.00      0.00      0.00         1\n",
      "4555500837878601542       0.00      0.00      0.00         1\n",
      "4555517359548268544       0.00      0.00      0.00         1\n",
      "4555520594716769321       0.00      0.00      0.00         1\n",
      "4555523111562430130       0.00      0.00      0.00         1\n",
      "4555544861268265063       0.00      0.00      0.00         1\n",
      "4555546970086886624       0.00      0.00      0.00         1\n",
      "4555560442365214720       0.00      0.00      0.00         1\n",
      "4555581711043264512       0.00      0.00      0.00         1\n",
      "4555599239868548894       0.00      0.00      0.00         1\n",
      "4555602502979944448       0.00      0.00      0.00         1\n",
      "4555607791113402776       0.00      0.00      0.00         1\n",
      "4555615754012523265       0.00      0.00      0.00         1\n",
      "4555633569531884672       0.00      0.00      0.00         1\n",
      "4555633835818285987       0.00      0.00      0.00         1\n",
      "4555664677959936768       0.00      0.00      0.00         1\n",
      "4555677206352795136       0.00      0.00      0.00         1\n",
      "4555783906286560159       0.00      0.00      0.00         1\n",
      "4555784408802520678       0.00      0.00      0.00         1\n",
      "4555785272053588368       0.00      0.00      0.00         1\n",
      "4555793191979122088       0.00      0.00      0.00         1\n",
      "4555815787822974262       0.00      0.00      0.00         1\n",
      "4555844743430406144       0.00      0.00      0.00         1\n",
      "4555885065640401391       0.00      0.00      0.00         1\n",
      "4555897257995534336       0.00      0.00      0.00         1\n",
      "4555915276449818306       0.00      0.00      0.00         1\n",
      "4555925883952562176       0.00      0.00      0.00         1\n",
      "4555926310192135168       0.00      0.00      0.00         1\n",
      "4555958199286497280       0.00      0.00      0.00         1\n",
      "4555961923023142912       0.00      0.00      0.00         1\n",
      "4555981453302900788       0.00      0.00      0.00         1\n",
      "4556023312035113580       0.00      0.00      0.00         1\n",
      "4556046607954518420       0.00      0.00      0.00         1\n",
      "4556071548812673104       0.00      0.00      0.00         1\n",
      "4556074078564741914       0.00      0.00      0.00         1\n",
      "4556087216867720130       0.00      0.00      0.00         1\n",
      "4556105440416447846       0.00      0.00      0.00         1\n",
      "4556112660253879444       0.00      0.00      0.00         1\n",
      "4556124612089348096       0.00      0.00      0.00         1\n",
      "4556153556926299494       0.00      0.00      0.00         1\n",
      "4556159505457547248       0.00      0.00      0.00         1\n",
      "4556177534676369408       0.00      0.00      0.00         1\n",
      "4556196333748224000       0.00      0.00      0.00         1\n",
      "4556248952435932104       0.00      0.00      0.00         1\n",
      "4556252379827624300       0.00      0.00      0.00         1\n",
      "4556315447133352411       0.00      0.00      0.00         1\n",
      "4556376872698511360       0.00      0.00      0.00         1\n",
      "4556391141623939744       0.00      0.00      0.00         1\n",
      "4556397966334867958       0.00      0.00      0.00         1\n",
      "4556470280713669501       0.00      0.00      0.00         1\n",
      "4556470734913798144       0.00      0.00      0.00         1\n",
      "4556501469699768320       0.00      0.00      0.00         1\n",
      "4556514844227928064       0.00      0.00      0.00         1\n",
      "4556525423768535332       0.00      0.00      0.00         1\n",
      "4556536383488917504       0.00      0.00      0.00         1\n",
      "4556558447791722356       0.00      0.00      0.00         1\n",
      "4556570777587023872       0.00      0.00      0.00         1\n",
      "4556605198503827902       0.00      0.00      0.00         1\n",
      "4556614680742723584       0.00      0.00      0.00         1\n",
      "4556643118758606928       0.00      0.00      0.00         1\n",
      "4556679530453925888       0.00      0.00      0.00         1\n",
      "4556684977539453854       0.00      0.00      0.00         1\n",
      "4556694395335737344       0.00      0.00      0.00         1\n",
      "4556749886313201664       0.00      0.00      0.00         1\n",
      "4556755324803229694       0.00      0.00      0.00         1\n",
      "4556756779735711744       0.00      0.00      0.00         1\n",
      "4556761448365162496       0.00      0.00      0.00         1\n",
      "4556778698016203918       0.00      0.00      0.00         1\n",
      "4556785916793847808       0.00      0.00      0.00         1\n",
      "4556791148064014336       0.00      0.00      0.00         1\n",
      "4556817537409590861       0.00      0.00      0.00         1\n",
      "4556822322000763267       0.00      0.00      0.00         1\n",
      "4556823875714809856       0.00      0.00      0.00         1\n",
      "4556825599032240360       0.00      0.00      0.00         1\n",
      "4556831564766163210       0.00      0.00      0.00         1\n",
      "4556837250242969600       0.00      0.00      0.00         1\n",
      "4556854834906624972       0.00      0.00      0.00         1\n",
      "4556890332791290848       0.00      0.00      0.00         1\n",
      "4556905712021667840       0.00      0.00      0.00         1\n",
      "4556929249491218460       0.00      0.00      0.00         1\n",
      "4556942301890088436       0.00      0.00      0.00         1\n",
      "4557108705060978688       0.00      0.00      0.00         1\n",
      "4557121783236395008       0.00      0.00      0.00         1\n",
      "4557186130436423680       0.00      0.00      0.00         1\n",
      "4557186207745835008       0.00      0.00      0.00         1\n",
      "4557211255995105280       0.00      0.00      0.00         1\n",
      "4557222020225510444       0.00      0.00      0.00         1\n",
      "4557255392111217088       0.00      0.00      0.00         1\n",
      "4557328542962024448       0.00      0.00      0.00         1\n",
      "4557348610106326296       0.00      0.00      0.00         1\n",
      "4557404448986037986       0.00      0.00      0.00         1\n",
      "4557409023115832963       0.00      0.00      0.00         1\n",
      "4557423547638611968       0.00      0.00      0.00         1\n",
      "4557453390134884762       0.00      0.00      0.00         1\n",
      "4557465819750073084       0.00      0.00      0.00         1\n",
      "4557483403356188693       0.00      0.00      0.00         1\n",
      "4557502206660775168       0.00      0.00      0.00         1\n",
      "4557655287920895920       0.00      0.00      0.00         1\n",
      "4557656188837167104       0.00      0.00      0.00         1\n",
      "4557673763843342336       0.00      0.00      0.00         1\n",
      "4557743578536738816       0.00      0.00      0.00         1\n",
      "4557745730315354112       0.00      0.00      0.00         1\n",
      "4557748131202072576       0.00      0.00      0.00         1\n",
      "4557774675158187446       0.00      0.00      0.00         1\n",
      "4557827163944775172       0.00      0.00      0.00         1\n",
      "4557847546810073088       0.00      0.00      0.00         1\n",
      "4557865388104220672       0.00      0.00      0.00         1\n",
      "4557905839163649484       0.00      0.00      0.00         1\n",
      "4557949260225576960       0.00      0.00      0.00         1\n",
      "4557990831214034944       0.00      0.00      0.00         1\n",
      "4558072045806827668       0.00      0.00      0.00         1\n",
      "4558074055815995568       0.00      0.00      0.00         1\n",
      "4558092064656442889       0.00      0.00      0.00         1\n",
      "4558099147008667456       0.00      0.00      0.00         1\n",
      "4558120470506897408       0.00      0.00      0.00         1\n",
      "4558139415607640064       0.00      0.00      0.00         1\n",
      "4558157773351326380       0.00      0.00      0.00         1\n",
      "4558169218385707008       0.00      0.00      0.00         1\n",
      "4558172346185684994       0.00      0.00      0.00         1\n",
      "4558185097934787776       0.00      0.00      0.00         1\n",
      "4558187587960832000       0.00      0.00      0.00         1\n",
      "4558233523693619925       0.00      0.00      0.00         1\n",
      "4558312167782219776       0.00      0.00      0.00         1\n",
      "4558323515085815808       0.00      0.00      0.00         1\n",
      "4558372262964625408       0.00      0.00      0.00         1\n",
      "4558390757093801984       0.00      0.00      0.00         1\n",
      "4558393712031301632       0.00      0.00      0.00         1\n",
      "4558403276923469824       0.00      0.00      0.00         1\n",
      "4558419706215229136       0.00      0.00      0.00         1\n",
      "4558426681256150543       0.00      0.00      0.00         1\n",
      "4558441722227864686       0.00      0.00      0.00         1\n",
      "4558455775374565962       0.00      0.00      0.00         1\n",
      "4558475294935089152       0.00      0.00      0.00         1\n",
      "4558506030774612834       0.00      0.00      0.00         1\n",
      "4558511660423184384       0.00      0.00      0.00         1\n",
      "4558572980720221834       0.00      0.00      0.00         1\n",
      "4558574049118126080       0.00      0.00      0.00         1\n",
      "4558643053127219562       0.00      0.00      0.00         1\n",
      "4558660924479206175       0.00      0.00      0.00         1\n",
      "4558737989072361380       0.00      0.00      0.00         1\n",
      "4558747162069958656       0.00      0.00      0.00         1\n",
      "4558806699961676057       0.00      0.00      0.00         1\n",
      "4558807644864677092       0.00      0.00      0.00         1\n",
      "4558820773514444800       0.00      0.00      0.00         1\n",
      "4558833766855861663       0.00      0.00      0.00         1\n",
      "4558837175994548224       0.00      0.00      0.00         1\n",
      "4558872863909833718       0.00      0.00      0.00         1\n",
      "4558913661830968684       0.00      0.00      0.00         1\n",
      "4558946057710469120       0.00      0.00      0.00         1\n",
      "4558966326693343936       0.00      0.00      0.00         1\n",
      "4558982587457334809       0.00      0.00      0.00         1\n",
      "4558992954458374144       0.00      0.00      0.00         1\n",
      "4559096421271159406       0.00      0.00      0.00         1\n",
      "4559132194056299380       0.00      0.00      0.00         1\n",
      "4559158423413470680       0.00      0.00      0.00         1\n",
      "4559198803650936832       0.00      0.00      0.00         1\n",
      "4559213097302097920       0.00      0.00      0.00         1\n",
      "4559222066244866332       0.00      0.00      0.00         1\n",
      "4559229651142831660       0.00      0.00      0.00         1\n",
      "4559231107166402748       0.00      0.00      0.00         1\n",
      "4559242020678968200       0.00      0.00      0.00         1\n",
      "4559253796412194816       0.00      0.00      0.00         1\n",
      "4559279386880268336       0.00      0.00      0.00         1\n",
      "4559346122095709124       0.00      0.00      0.00         1\n",
      "4559350322574358126       0.00      0.00      0.00         1\n",
      "4559357915009384448       0.00      0.00      0.00         1\n",
      "4559436285277634560       0.00      0.00      0.00         1\n",
      "4559478736734388224       0.00      0.00      0.00         1\n",
      "4559482654810253162       0.00      0.00      0.00         1\n",
      "4559496148531806208       0.00      0.00      0.00         1\n",
      "4559514621186146304       0.00      0.00      0.00         1\n",
      "4559575791145559472       0.00      0.00      0.00         1\n",
      "4559592093806231552       0.00      0.00      0.00         1\n",
      "4559614291261474837       0.00      0.00      0.00         1\n",
      "4559627893423267360       0.00      0.00      0.00         1\n",
      "4559682349318862858       0.00      0.00      0.00         1\n",
      "4559747104526716864       0.00      0.00      0.00         1\n",
      "4559758613983264768       0.00      0.00      0.00         1\n",
      "4559763445821472768       0.00      0.00      0.00         1\n",
      "4559792601127290858       0.00      0.00      0.00         1\n",
      "4559810356495554476       0.00      0.00      0.00         1\n",
      "4559811301346563840       0.00      0.00      0.00         1\n",
      "4559828661648598048       0.00      0.00      0.00         1\n",
      "4559831018541940736       0.00      0.00      0.00         1\n",
      "4559869905175838720       0.00      0.00      0.00         1\n",
      "4559940024811913216       0.00      0.00      0.00         1\n",
      "4559947545299648512       0.00      0.00      0.00         1\n",
      "4559982892880494592       0.00      0.00      0.00         1\n",
      "4560037799742406656       0.00      0.00      0.00         1\n",
      "4560056534389751808       0.00      0.00      0.00         1\n",
      "4560114168555896832       0.00      0.00      0.00         1\n",
      "4560228699212382692       0.00      0.00      0.00         1\n",
      "4560229905039622144       0.00      0.00      0.00         1\n",
      "4560245736289075200       0.00      0.00      0.00         1\n",
      "4560302740143116320       0.00      0.00      0.00         1\n",
      "4560306330750564950       0.00      0.00      0.00         1\n",
      "4560381650529157120       0.00      0.00      0.00         1\n",
      "4560421001019523072       0.00      0.00      0.00         1\n",
      "4560583124217089178       0.00      0.00      0.00         1\n",
      "4560583956373700608       0.00      0.00      0.00         1\n",
      "4560587268858227614       0.00      0.00      0.00         1\n",
      "4560600075385962496       0.00      0.00      0.00         1\n",
      "4560604766547397366       0.00      0.00      0.00         1\n",
      "4560617853317042795       0.00      0.00      0.00         1\n",
      "4560662671285074528       0.00      0.00      0.00         1\n",
      "4560676534393765888       0.00      0.00      0.00         1\n",
      "4560691352030937088       0.00      0.00      0.00         1\n",
      "4560698074712909354       0.00      0.00      0.00         1\n",
      "4560722624746479740       0.00      0.00      0.00         1\n",
      "4560766678213973128       0.00      0.00      0.00         1\n",
      "4560780167659651072       0.00      0.00      0.00         1\n",
      "4560793899711687636       0.00      0.00      0.00         1\n",
      "4560817293356957696       0.00      0.00      0.00         1\n",
      "4560896346525007872       0.00      0.00      0.00         1\n",
      "4560953194712137728       0.00      0.00      0.00         1\n",
      "4560973309102026336       0.00      0.00      0.00         1\n",
      "4561008957312365168       0.00      0.00      0.00         1\n",
      "4561039412953322680       0.00      0.00      0.00         1\n",
      "4561093014129872605       0.00      0.00      0.00         1\n",
      "4561128180269711360       0.00      0.00      0.00         1\n",
      "4561156548528701440       0.00      0.00      0.00         1\n",
      "4561250175568921962       0.00      0.00      0.00         1\n",
      "4561303836910755769       0.00      0.00      0.00         1\n",
      "4561320581919670272       0.00      0.00      0.00         1\n",
      "4561325423405713413       0.00      0.00      0.00         1\n",
      "4561328128177209344       0.00      0.00      0.00         1\n",
      "4561416686107885568       0.00      0.00      0.00         1\n",
      "4561416708642544524       0.00      0.00      0.00         1\n",
      "4561425374826725376       0.00      0.00      0.00         1\n",
      "4561435812664631192       0.00      0.00      0.00         1\n",
      "4561437315896829095       0.00      0.00      0.00         1\n",
      "4561442133789114368       0.00      0.00      0.00         1\n",
      "4561461934646326323       0.00      0.00      0.00         1\n",
      "4561464768266764288       0.00      0.00      0.00         1\n",
      "4561467771508115371       0.00      0.00      0.00         1\n",
      "4561482705113767414       0.00      0.00      0.00         1\n",
      "4561485766361874432       0.00      0.00      0.00         1\n",
      "4561504449469612032       0.00      0.00      0.00         1\n",
      "4561606400167378802       0.00      0.00      0.00         1\n",
      "4561643774968335964       0.00      0.00      0.00         1\n",
      "4561646982254297088       0.00      0.00      0.00         1\n",
      "4561659347465142272       0.00      0.00      0.00         1\n",
      "4561679486566793216       0.00      0.00      0.00         1\n",
      "4561704578817166711       0.00      0.00      0.00         1\n",
      "4561723763384647680       0.00      0.00      0.00         1\n",
      "4561754257652449280       0.00      0.00      0.00         1\n",
      "4561822024695356468       0.00      0.00      0.00         1\n",
      "4561868693820979908       0.00      0.00      0.00         1\n",
      "4561917638208389120       0.00      0.00      0.00         1\n",
      "4561940882571395072       0.00      0.00      0.00         1\n",
      "4561941321724259447       0.00      0.00      0.00         1\n",
      "4561944773811765248       0.00      0.00      0.00         1\n",
      "4561947386203252162       0.00      0.00      0.00         1\n",
      "4561988635085229450       0.00      0.00      0.00         1\n",
      "4562082256772742752       0.00      0.00      0.00         1\n",
      "4562095700034542280       0.00      0.00      0.00         1\n",
      "4562099933800300544       0.00      0.00      0.00         1\n",
      "4562116083939722490       0.00      0.00      0.00         1\n",
      "4562126245795193732       0.00      0.00      0.00         1\n",
      "4562154390732317064       0.00      0.00      0.00         1\n",
      "4562160639919914690       0.00      0.00      0.00         1\n",
      "4562171427825909760       0.00      0.00      0.00         1\n",
      "4562225949189148200       0.00      0.00      0.00         1\n",
      "4562286907653172580       0.00      0.00      0.00         1\n",
      "4562297894182336758       0.00      0.00      0.00         1\n",
      "4562320428831342592       0.00      0.00      0.00         1\n",
      "4562327206289735680       0.00      0.00      0.00         1\n",
      "4562330844127035392       0.00      0.00      0.00         1\n",
      "4562349132097781760       0.00      0.00      0.00         1\n",
      "4562385318263970125       0.00      0.00      0.00         1\n",
      "4562450952887468032       0.00      0.00      0.00         1\n",
      "4562464172796805120       0.00      0.00      0.00         1\n",
      "4562466079762284544       0.00      0.00      0.00         1\n",
      "4562487571778633728       0.00      0.00      0.00         1\n",
      "4562547993378553856       0.00      0.00      0.00         1\n",
      "4562561118798610432       0.00      0.00      0.00         1\n",
      "4562638252116279296       0.00      0.00      0.00         1\n",
      "4562656253369926416       0.00      0.00      0.00         1\n",
      "4562657545109372928       0.00      0.00      0.00         1\n",
      "4562707689899675822       0.00      0.00      0.00         1\n",
      "4562724615318667264       0.00      0.00      0.00         1\n",
      "4562742045338446928       0.00      0.00      0.00         1\n",
      "4562742869961162720       0.00      0.00      0.00         1\n",
      "4562748414802702357       0.00      0.00      0.00         1\n",
      "4562796208128524288       0.00      0.00      0.00         1\n",
      "4562808221152051200       0.00      0.00      0.00         1\n",
      "4562809724390604800       0.00      0.00      0.00         1\n",
      "4562817313597816832       0.00      0.00      0.00         1\n",
      "4562817837583826944       0.00      0.00      0.00         1\n",
      "4562853345127129011       0.00      0.00      0.00         1\n",
      "4562874234799390720       0.00      0.00      0.00         1\n",
      "4562886995147227136       0.00      0.00      0.00         1\n",
      "4562900143096547550       0.00      0.00      0.00         1\n",
      "4562930001714624198       0.00      0.00      0.00         1\n",
      "4562945270321049354       0.00      0.00      0.00         1\n",
      "4562950448994058240       0.00      0.00      0.00         1\n",
      "4562980522355064832       0.00      0.00      0.00         1\n",
      "4562988425094889472       0.00      0.00      0.00         1\n",
      "4563023222919921664       0.00      0.00      0.00         1\n",
      "4563027304196415812       0.00      0.00      0.00         1\n",
      "4563028704362999487       0.00      0.00      0.00         1\n",
      "4563042740318306304       0.00      0.00      0.00         1\n",
      "4563050057875587072       0.00      0.00      0.00         1\n",
      "4563096315731511895       0.00      0.00      0.00         1\n",
      "4563096954623492096       0.00      0.00      0.00         1\n",
      "4563118271618408333       0.00      0.00      0.00         1\n",
      "4563162200536426898       0.00      0.00      0.00         1\n",
      "4563168332684984320       0.00      0.00      0.00         1\n",
      "4563170531708239872       0.00      0.00      0.00         1\n",
      "4563193536618963802       0.00      0.00      0.00         1\n",
      "4563221196180700592       0.00      0.00      0.00         1\n",
      "4563260910705049600       0.00      0.00      0.00         1\n",
      "4563271008173162496       0.00      0.00      0.00         1\n",
      "4563285414535765955       0.00      0.00      0.00         1\n",
      "4563292896358610544       0.00      0.00      0.00         1\n",
      "4563382695570659152       0.00      0.00      0.00         1\n",
      "4563387901042886434       0.00      0.00      0.00         1\n",
      "4563392840251261304       0.00      0.00      0.00         1\n",
      "4563421857077781304       0.00      0.00      0.00         1\n",
      "4563441256381808640       0.00      0.00      0.00         1\n",
      "4563441472195477094       0.00      0.00      0.00         1\n",
      "4563469126424592384       0.00      0.00      0.00         1\n",
      "4563472283225554944       0.00      0.00      0.00         1\n",
      "4563496889093193728       0.00      0.00      0.00         1\n",
      "4563519614833066378       0.00      0.00      0.00         1\n",
      "4563526073395970048       0.00      0.00      0.00         1\n",
      "4563529260261703680       0.00      0.00      0.00         1\n",
      "4563573761467054561       0.00      0.00      0.00         1\n",
      "4563614296319197184       0.00      0.00      0.00         1\n",
      "4563618917704007680       0.00      0.00      0.00         1\n",
      "4563619858301845504       0.00      0.00      0.00         1\n",
      "4563621851166670848       0.00      0.00      0.00         1\n",
      "4563659068111411689       0.00      0.00      0.00         1\n",
      "4563664522684496768       0.00      0.00      0.00         1\n",
      "4563675297739702272       0.00      0.00      0.00         1\n",
      "4563710065499963392       0.00      0.00      0.00         1\n",
      "4563721336555193530       0.00      0.00      0.00         1\n",
      "4563732318737115168       0.00      0.00      0.00         1\n",
      "4563740473868419072       0.00      0.00      0.00         1\n",
      "4563771517892034560       0.00      0.00      0.00         1\n",
      "4563774083049188840       0.00      0.00      0.00         1\n",
      "4563781739914199040       0.00      0.00      0.00         1\n",
      "4563802656404930560       0.00      0.00      0.00         1\n",
      "4563846423186217836       0.00      0.00      0.00         1\n",
      "4563872875892025276       0.00      0.00      0.00         1\n",
      "4563936341556985856       0.00      0.00      0.00         1\n",
      "4563984338866253433       0.00      0.00      0.00         1\n",
      "4564009484850036736       0.00      0.00      0.00         1\n",
      "4564011323096039424       0.00      0.00      0.00         1\n",
      "4564020652822132009       0.00      0.00      0.00         1\n",
      "4564032588538538841       0.00      0.00      0.00         1\n",
      "4564052026501103616       0.00      0.00      0.00         1\n",
      "4564067953306741546       0.00      0.00      0.00         1\n",
      "4564146624214509099       0.00      0.00      0.00         1\n",
      "4564149991465618396       0.00      0.00      0.00         1\n",
      "4564186125010071180       0.00      0.00      0.00         1\n",
      "4564283692742082560       0.00      0.00      0.00         1\n",
      "4564308526242988032       0.00      0.00      0.00         1\n",
      "4564329392266880760       0.00      0.00      0.00         1\n",
      "4564412169165470142       0.00      0.00      0.00         1\n",
      "4564432699086174832       0.00      0.00      0.00         1\n",
      "4564498407804586949       0.00      0.00      0.00         1\n",
      "4564509626232321984       0.00      0.00      0.00         1\n",
      "4564535721423011840       0.00      0.00      0.00         1\n",
      "4564551148945539072       0.00      0.00      0.00         1\n",
      "4564584556231034224       0.00      0.00      0.00         1\n",
      "4564591062076620800       0.00      0.00      0.00         1\n",
      "4564605172100990618       0.00      0.00      0.00         1\n",
      "4564609746252207321       0.00      0.00      0.00         1\n",
      "4564640021471430999       0.00      0.00      0.00         1\n",
      "4564648130367805076       0.00      0.00      0.00         1\n",
      "4564656942579974144       0.00      0.00      0.00         1\n",
      "4564665781622669312       0.00      0.00      0.00         1\n",
      "4564695602646562090       0.00      0.00      0.00         1\n",
      "4564712261758746624       0.00      0.00      0.00         1\n",
      "4564725774757491344       0.00      0.00      0.00         1\n",
      "4564737365842591744       0.00      0.00      0.00         1\n",
      "4564754180639555584       0.00      0.00      0.00         1\n",
      "4564804251368292352       0.00      0.00      0.00         1\n",
      "4564818330271088640       0.00      0.00      0.00         1\n",
      "4564869132207207431       0.00      0.00      0.00         1\n",
      "4564901656931598336       0.00      0.00      0.00         1\n",
      "4564950135291041220       0.00      0.00      0.00         1\n",
      "4565004474153697280       0.00      0.00      0.00         1\n",
      "4565023440729276416       0.00      0.00      0.00         1\n",
      "4565091362404614501       0.00      0.00      0.00         1\n",
      "4565106712626312150       0.00      0.00      0.00         1\n",
      "4565156310862283680       0.00      0.00      0.00         1\n",
      "4565156820938653696       0.00      0.00      0.00         1\n",
      "4565183549585291499       0.00      0.00      0.00         1\n",
      "4565191379301420797       0.00      0.00      0.00         1\n",
      "4565195823536668672       0.00      0.00      0.00         1\n",
      "4565227606294659072       0.00      0.00      0.00         1\n",
      "4565311547135492096       0.00      0.00      0.00         1\n",
      "4565421971788353832       0.00      0.00      0.00         1\n",
      "4565434872826429440       0.00      0.00      0.00         1\n",
      "4565440809534817556       0.00      0.00      0.00         1\n",
      "4565499410066430890       0.00      0.00      0.00         1\n",
      "4565543527937874760       0.00      0.00      0.00         1\n",
      "4565568932697691086       0.00      0.00      0.00         1\n",
      "4565588367432095494       0.00      0.00      0.00         1\n",
      "4565618930425212223       0.00      0.00      0.00         1\n",
      "4565656249377478576       0.00      0.00      0.00         1\n",
      "4565697382275522613       0.00      0.00      0.00         1\n",
      "4565734352306044928       0.00      0.00      0.00         1\n",
      "4565734628235578054       0.00      0.00      0.00         1\n",
      "4565744823436312576       0.00      0.00      0.00         1\n",
      "4565780326697492732       0.00      0.00      0.00         1\n",
      "4565825865174220800       0.00      0.00      0.00         1\n",
      "4565877202918309888       0.00      0.00      0.00         1\n",
      "4565957863433461888       0.00      0.00      0.00         1\n",
      "4565961392867246080       0.00      0.00      0.00         1\n",
      "4565967547555381248       0.00      0.00      0.00         1\n",
      "4565990213155547772       0.00      0.00      0.00         1\n",
      "4566030988517310464       0.00      0.00      0.00         1\n",
      "4566057893229980363       0.00      0.00      0.00         1\n",
      "4566095457021118452       0.00      0.00      0.00         1\n",
      "4566101873717488574       0.00      0.00      0.00         1\n",
      "4566103638943805888       0.00      0.00      0.00         1\n",
      "4566151535364407296       0.00      0.00      0.00         1\n",
      "4566215574372600580       0.00      0.00      0.00         1\n",
      "4566230962194612224       0.00      0.00      0.00         1\n",
      "4566231031975219202       0.00      0.00      0.00         1\n",
      "4566348705483001272       0.00      0.00      0.00         1\n",
      "4566368787695140864       0.00      0.00      0.00         1\n",
      "4566386703046667390       0.00      0.00      0.00         1\n",
      "4566400189256846648       0.00      0.00      0.00         1\n",
      "4566402898325405696       0.00      0.00      0.00         1\n",
      "4566455674883538944       0.00      0.00      0.00         1\n",
      "4566456525287063552       0.00      0.00      0.00         1\n",
      "4566459528533173018       0.00      0.00      0.00         1\n",
      "4566477490065099188       0.00      0.00      0.00         1\n",
      "4566516925412147200       0.00      0.00      0.00         1\n",
      "4566517334491609848       0.00      0.00      0.00         1\n",
      "4566594454933594175       0.00      0.00      0.00         1\n",
      "4566629291391962189       0.00      0.00      0.00         1\n",
      "4566703236798480384       0.00      0.00      0.00         1\n",
      "4566705771886217057       0.00      0.00      0.00         1\n",
      "4566713099102387858       0.00      0.00      0.00         1\n",
      "4566716486772588544       0.00      0.00      0.00         1\n",
      "4566792272521313432       0.00      0.00      0.00         1\n",
      "4566796867085533184       0.00      0.00      0.00         1\n",
      "4566800974140629652       0.00      0.00      0.00         1\n",
      "4566835650640216064       0.00      0.00      0.00         1\n",
      "4566907973594513408       0.00      0.00      0.00         1\n",
      "4566909091355679490       0.00      0.00      0.00         1\n",
      "4566916576414007296       0.00      0.00      0.00         1\n",
      "4566922092204062779       0.00      0.00      0.00         1\n",
      "4566937648593818755       0.00      0.00      0.00         1\n",
      "4566957756560441344       0.00      0.00      0.00         1\n",
      "4566980323382022218       0.00      0.00      0.00         1\n",
      "4566995055119577539       0.00      0.00      0.00         1\n",
      "4567030810709207790       0.00      0.00      0.00         1\n",
      "4567033171891191808       0.00      0.00      0.00         1\n",
      "4567076208531478747       0.00      0.00      0.00         1\n",
      "4567096227365612019       0.00      0.00      0.00         1\n",
      "4567123473578590208       0.00      0.00      0.00         1\n",
      "4567227619016262254       0.00      0.00      0.00         1\n",
      "4567235392889412258       0.00      0.00      0.00         1\n",
      "4567235577587474881       0.00      0.00      0.00         1\n",
      "4567236144516313350       0.00      0.00      0.00         1\n",
      "4567261213179772928       0.00      0.00      0.00         1\n",
      "4567332363607998464       0.00      0.00      0.00         1\n",
      "4567363081214099456       0.00      0.00      0.00         1\n",
      "4567364177490709439       0.00      0.00      0.00         1\n",
      "4567373793919993914       0.00      0.00      0.00         1\n",
      "4567464395197644800       0.00      0.00      0.00         1\n",
      "4567516308467351552       0.00      0.00      0.00         1\n",
      "4567591608908202785       0.00      0.00      0.00         1\n",
      "4567656664763361025       0.00      0.00      0.00         1\n",
      "4567681867606483328       0.00      0.00      0.00         1\n",
      "4567728527096414208       0.00      0.00      0.00         1\n",
      "4567764902243360819       0.00      0.00      0.00         1\n",
      "4567856727575232512       0.00      0.00      0.00         1\n",
      "4567954091240145754       0.00      0.00      0.00         1\n",
      "4567954596995006464       0.00      0.00      0.00         1\n",
      "4567994640041908371       0.00      0.00      0.00         1\n",
      "4568014637406971996       0.00      0.00      0.00         1\n",
      "4568049856135843801       0.00      0.00      0.00         1\n",
      "4568072154544865280       0.00      0.00      0.00         1\n",
      "4568143787078160888       0.00      0.00      0.00         1\n",
      "4568258711803481199       0.00      0.00      0.00         1\n",
      "4568269289248784384       0.00      0.00      0.00         1\n",
      "4568354433737013374       0.00      0.00      0.00         1\n",
      "4568393423451810561       0.00      0.00      0.00         1\n",
      "4568412414738956288       0.00      0.00      0.00         1\n",
      "4568412848530653184       0.00      0.00      0.00         1\n",
      "4568427388041397144       0.00      0.00      0.00         1\n",
      "4568504704996212736       0.00      0.00      0.00         1\n",
      "4568541166024133676       0.00      0.00      0.00         1\n",
      "4568556674100494336       0.00      0.00      0.00         1\n",
      "4568575692215681024       0.00      0.00      0.00         1\n",
      "4568597679226467536       0.00      0.00      0.00         1\n",
      "4568618586054066176       0.00      0.00      0.00         1\n",
      "4568729001073311744       0.00      0.00      0.00         1\n",
      "4568775189151612928       0.00      0.00      0.00         1\n",
      "4568793013265891328       0.00      0.00      0.00         1\n",
      "4568805181975857713       0.00      0.00      0.00         1\n",
      "4568821119531876352       0.00      0.00      0.00         1\n",
      "4568823112396701696       0.00      0.00      0.00         1\n",
      "4568829013681766400       0.00      0.00      0.00         1\n",
      "4568895526610073940       0.00      0.00      0.00         1\n",
      "4568919379793674240       0.00      0.00      0.00         1\n",
      "4568939506010423296       0.00      0.00      0.00         1\n",
      "4568978461363798016       0.00      0.00      0.00         1\n",
      "4568984809325461504       0.00      0.00      0.00         1\n",
      "4568993777217175552       0.00      0.00      0.00         1\n",
      "4569002548601675418       0.00      0.00      0.00         1\n",
      "4569024272530504084       0.00      0.00      0.00         1\n",
      "4569026028126601216       0.00      0.00      0.00         1\n",
      "4569141425307910144       0.00      0.00      0.00         1\n",
      "4569170910258397184       0.00      0.00      0.00         1\n",
      "4569194911593205225       0.00      0.00      0.00         1\n",
      "4569196267745312768       0.00      0.00      0.00         1\n",
      "4569221415840585812       0.00      0.00      0.00         1\n",
      "4569227188283137521       0.00      0.00      0.00         1\n",
      "4569248237908553370       0.00      0.00      0.00         1\n",
      "4569371108326585168       0.00      0.00      0.00         1\n",
      "4569385028317224154       0.00      0.00      0.00         1\n",
      "4569447313926299530       0.00      0.00      0.00         1\n",
      "4569454413523951179       0.00      0.00      0.00         1\n",
      "4569454679813492737       0.00      0.00      0.00         1\n",
      "4569456724193633780       0.00      0.00      0.00         1\n",
      "4569533260503583410       0.00      0.00      0.00         1\n",
      "4569548447537233664       0.00      0.00      0.00         1\n",
      "4569552819808787666       0.00      0.00      0.00         1\n",
      "4569625868594260304       0.00      0.00      0.00         1\n",
      "4569702691634151424       0.00      0.00      0.00         1\n",
      "4569707279708509696       0.00      0.00      0.00         1\n",
      "4569708894634685132       0.00      0.00      0.00         1\n",
      "4569717317065107278       0.00      0.00      0.00         1\n",
      "4569728591336710688       0.00      0.00      0.00         1\n",
      "4569777618404971397       0.00      0.00      0.00         1\n",
      "4569807828138590208       0.00      0.00      0.00         1\n",
      "4569826519836262400       0.00      0.00      0.00         1\n",
      "4569832030279303168       0.00      0.00      0.00         1\n",
      "4569855537683593136       0.00      0.00      0.00         1\n",
      "4569882527268361065       0.00      0.00      0.00         1\n",
      "4569892731052097536       0.00      0.00      0.00         1\n",
      "4569906089445781152       0.00      0.00      0.00         1\n",
      "4569907532560058856       0.00      0.00      0.00         1\n",
      "4569923930755384611       0.00      0.00      0.00         1\n",
      "4569943060548009653       0.00      0.00      0.00         1\n",
      "4569956794784284672       0.00      0.00      0.00         1\n",
      "4570009331870194220       0.00      0.00      0.00         1\n",
      "4570028671061983232       0.00      0.00      0.00         1\n",
      "4570031657106702928       0.00      0.00      0.00         1\n",
      "4570057727573337919       0.00      0.00      0.00         1\n",
      "4570082190649458688       0.00      0.00      0.00         1\n",
      "4570093409104035840       0.00      0.00      0.00         1\n",
      "4570106667668078592       0.00      0.00      0.00         1\n",
      "4570139644426977280       0.00      0.00      0.00         1\n",
      "4570143737530810368       0.00      0.00      0.00         1\n",
      "4570169164783967744       0.00      0.00      0.00         1\n",
      "4570193959643481274       0.00      0.00      0.00         1\n",
      "4570224578454666940       0.00      0.00      0.00         1\n",
      "4570241143094116352       0.00      0.00      0.00         1\n",
      "4570272582254723072       0.00      0.00      0.00         1\n",
      "4570290562016129024       0.00      0.00      0.00         1\n",
      "4570311199332092056       0.00      0.00      0.00         1\n",
      "4570372319985270784       0.00      0.00      0.00         1\n",
      "4570373673955876536       0.00      0.00      0.00         1\n",
      "4570376534403719104       0.00      0.00      0.00         1\n",
      "4570381688311091456       0.00      0.00      0.00         1\n",
      "4570394009570115584       0.00      0.00      0.00         1\n",
      "4570396901142141268       0.00      0.00      0.00         1\n",
      "4570406383370895360       0.00      0.00      0.00         1\n",
      "4570458412604719104       0.00      0.00      0.00         1\n",
      "4570502089179564050       0.00      0.00      0.00         1\n",
      "4570512323034218496       0.00      0.00      0.00         1\n",
      "4570545231073640448       0.00      0.00      0.00         1\n",
      "4570583173865243780       0.00      0.00      0.00         1\n",
      "4570686253063438688       0.00      0.00      0.00         1\n",
      "4570689846917464064       0.00      0.00      0.00         1\n",
      "4570838207972769792       0.00      0.00      0.00         1\n",
      "4570862548584996908       0.00      0.00      0.00         1\n",
      "4570903989691875328       0.00      0.00      0.00         1\n",
      "4570926336406716416       0.00      0.00      0.00         1\n",
      "4570943899590084894       0.00      0.00      0.00         1\n",
      "4570956303444721502       0.00      0.00      0.00         1\n",
      "4571051573358100480       0.00      0.00      0.00         1\n",
      "4571063454290133024       0.00      0.00      0.00         1\n",
      "4571081458808185296       0.00      0.00      0.00         1\n",
      "4571117064076797218       0.00      0.00      0.00         1\n",
      "4571156267480907776       0.00      0.00      0.00         1\n",
      "4571197383202832384       0.00      0.00      0.00         1\n",
      "4571278970401587200       0.00      0.00      0.00         1\n",
      "4571334053357158400       0.00      0.00      0.00         1\n",
      "4571353857451360256       0.00      0.00      0.00         1\n",
      "4571382298724794368       0.00      0.00      0.00         1\n",
      "4571399899500773376       0.00      0.00      0.00         1\n",
      "4571440221699900924       0.00      0.00      0.00         1\n",
      "4571513163149064278       0.00      0.00      0.00         1\n",
      "4571533641539510402       0.00      0.00      0.00         1\n",
      "4571563229555447076       0.00      0.00      0.00         1\n",
      "4571563792202866205       0.00      0.00      0.00         1\n",
      "4571578523955504460       0.00      0.00      0.00         1\n",
      "4571701642428153856       0.00      0.00      0.00         1\n",
      "4571723547828232824       0.00      0.00      0.00         1\n",
      "4571760260141809664       0.00      0.00      0.00         1\n",
      "4571811950073217024       0.00      0.00      0.00         1\n",
      "4571850325606006784       0.00      0.00      0.00         1\n",
      "4571879635523256036       0.00      0.00      0.00         1\n",
      "4571911400040955904       0.00      0.00      0.00         1\n",
      "4571919526119079936       0.00      0.00      0.00         1\n",
      "4571990547698286592       0.00      0.00      0.00         1\n",
      "4572021910608099314       0.00      0.00      0.00         1\n",
      "4572039571500602708       0.00      0.00      0.00         1\n",
      "4572052516527912984       0.00      0.00      0.00         1\n",
      "4572059524873060352       0.00      0.00      0.00         1\n",
      "4572062373506024594       0.00      0.00      0.00         1\n",
      "4572068518534578176       0.00      0.00      0.00         1\n",
      "4572150214170935426       0.00      0.00      0.00         1\n",
      "4572267100642476032       0.00      0.00      0.00         1\n",
      "4572292071582334976       0.00      0.00      0.00         1\n",
      "4572347632348765912       0.00      0.00      0.00         1\n",
      "4572351171397635593       0.00      0.00      0.00         1\n",
      "4572373307653294482       0.00      0.00      0.00         1\n",
      "4572413301329231872       0.00      0.00      0.00         1\n",
      "4572426234534280781       0.00      0.00      0.00         1\n",
      "4572539624907341824       0.00      0.00      0.00         1\n",
      "4572552741737463808       0.00      0.00      0.00         1\n",
      "4572571219750040820       0.00      0.00      0.00         1\n",
      "4572769921053753344       0.00      0.00      0.00         1\n",
      "4572884347572453376       0.00      0.00      0.00         1\n",
      "4572919244181733376       0.00      0.00      0.00         1\n",
      "4573004504614126562       0.00      0.00      0.00         1\n",
      "4573033478473857709       0.00      0.00      0.00         1\n",
      "4573129234732480832       0.00      0.00      0.00         1\n",
      "4573132854380199936       0.00      0.00      0.00         1\n",
      "4573160372235665408       0.00      0.00      0.00         1\n",
      "4573168392004734142       0.00      0.00      0.00         1\n",
      "4573233932140544000       0.00      0.00      0.00         1\n",
      "4573267975109289263       0.00      0.00      0.00         1\n",
      "4573278145590996635       0.00      0.00      0.00         1\n",
      "4573316910908702720       0.00      0.00      0.00         1\n",
      "4573334305526251520       0.00      0.00      0.00         1\n",
      "4573408239093284864       0.00      0.00      0.00         1\n",
      "4573478956789626737       0.00      0.00      0.00         1\n",
      "4573535869399123576       0.00      0.00      0.00         1\n",
      "4573539527653588992       0.00      0.00      0.00         1\n",
      "4573557042530222080       0.00      0.00      0.00         1\n",
      "4573579840216629248       0.00      0.00      0.00         1\n",
      "4573582460146679808       0.00      0.00      0.00         1\n",
      "4573590045058924544       0.00      0.00      0.00         1\n",
      "4573668979026059663       0.00      0.00      0.00         1\n",
      "4573673608995131820       0.00      0.00      0.00         1\n",
      "4573736486263848960       0.00      0.00      0.00         1\n",
      "4573783103838879744       0.00      0.00      0.00         1\n",
      "4573824358045475322       0.00      0.00      0.00         1\n",
      "4573915483320877056       0.00      0.00      0.00         1\n",
      "4574090374389170176       0.00      0.00      0.00         1\n",
      "4574122835751993344       0.00      0.00      0.00         1\n",
      "4574149508560590725       0.00      0.00      0.00         1\n",
      "4574174830626078720       0.00      0.00      0.00         1\n",
      "4574244817118167040       0.00      0.00      0.00         1\n",
      "4574263384261787648       0.00      0.00      0.00         1\n",
      "4574298246511329280       0.00      0.00      0.00         1\n",
      "4574310938139688960       0.00      0.00      0.00         1\n",
      "4574336601628555863       0.00      0.00      0.00         1\n",
      "4574353334823573998       0.00      0.00      0.00         1\n",
      "4574365442308999216       0.00      0.00      0.00         1\n",
      "4574387582869389072       0.00      0.00      0.00         1\n",
      "4574460253736597266       0.00      0.00      0.00         1\n",
      "4574467314632385568       0.00      0.00      0.00         1\n",
      "4574475870229926012       0.00      0.00      0.00         1\n",
      "4574497846526476288       0.00      0.00      0.00         1\n",
      "4574512562099927264       0.00      0.00      0.00         1\n",
      "4574591571302809600       0.00      0.00      0.00         1\n",
      "4574604765442342912       0.00      0.00      0.00         1\n",
      "4574632107204149248       0.00      0.00      0.00         1\n",
      "4574638433690976256       0.00      0.00      0.00         1\n",
      "4574640679958872064       0.00      0.00      0.00         1\n",
      "4574643798105128960       0.00      0.00      0.00         1\n",
      "4574741561212189411       0.00      0.00      0.00         1\n",
      "4574872822941220864       0.00      0.00      0.00         1\n",
      "4574914930800590848       0.00      0.00      0.00         1\n",
      "4574980584723313348       0.00      0.00      0.00         1\n",
      "4575013431580557312       0.00      0.00      0.00         1\n",
      "4575092467568738304       0.00      0.00      0.00         1\n",
      "4575105086182653952       0.00      0.00      0.00         1\n",
      "4575306796094367069       0.00      0.00      0.00         1\n",
      "4575351647370215424       0.00      0.00      0.00         1\n",
      "4575485406585392378       0.00      0.00      0.00         1\n",
      "4575495336560857898       0.00      0.00      0.00         1\n",
      "4575520486829588480       0.00      0.00      0.00         1\n",
      "4575535579344666624       0.00      0.00      0.00         1\n",
      "4575601568288053162       0.00      0.00      0.00         1\n",
      "4575650732771926855       0.00      0.00      0.00         1\n",
      "4575657080717077424       0.00      0.00      0.00         1\n",
      "4575675767077208064       0.00      0.00      0.00         1\n",
      "4575682913902788608       0.00      0.00      0.00         1\n",
      "4575725164535396546       0.00      0.00      0.00         1\n",
      "4575744572453289984       0.00      0.00      0.00         1\n",
      "4575790366455444075       0.00      0.00      0.00         1\n",
      "4575814517066508154       0.00      0.00      0.00         1\n",
      "4575822913690950792       0.00      0.00      0.00         1\n",
      "4575876802662013244       0.00      0.00      0.00         1\n",
      "4575879322757234688       0.00      0.00      0.00         1\n",
      "4575914442704814080       0.00      0.00      0.00         1\n",
      "4575936999873052672       0.00      0.00      0.00         1\n",
      "4575980572316270592       0.00      0.00      0.00         1\n",
      "4575996546360871062       0.00      0.00      0.00         1\n",
      "4576028316241372372       0.00      0.00      0.00         1\n",
      "4576034697494134784       0.00      0.00      0.00         1\n",
      "4576037567585747278       0.00      0.00      0.00         1\n",
      "4576045207279108096       0.00      0.00      0.00         1\n",
      "4576049954283606373       0.00      0.00      0.00         1\n",
      "4576055635459702784       0.00      0.00      0.00         1\n",
      "4576057878492220309       0.00      0.00      0.00         1\n",
      "4576097893642928128       0.00      0.00      0.00         1\n",
      "4576100410493763584       0.00      0.00      0.00         1\n",
      "4576102391538648808       0.00      0.00      0.00         1\n",
      "4576106235523971066       0.00      0.00      0.00         1\n",
      "4576118762889019392       0.00      0.00      0.00         1\n",
      "4576170608477754344       0.00      0.00      0.00         1\n",
      "4576181241778274304       0.00      0.00      0.00         1\n",
      "4576205512638464000       0.00      0.00      0.00         1\n",
      "4576225317778747720       0.00      0.00      0.00         1\n",
      "4576246384613863374       0.00      0.00      0.00         1\n",
      "4576265458568943262       0.00      0.00      0.00         1\n",
      "4576266025484791050       0.00      0.00      0.00         1\n",
      "4576300561324641655       0.00      0.00      0.00         1\n",
      "4576320768085852160       0.00      0.00      0.00         1\n",
      "4576324561610575493       0.00      0.00      0.00         1\n",
      "4576333490845342344       0.00      0.00      0.00         1\n",
      "4576349587316408320       0.00      0.00      0.00         1\n",
      "4576374507763190008       0.00      0.00      0.00         1\n",
      "4576393576371453952       0.00      0.00      0.00         1\n",
      "4576420686205026304       0.00      0.00      0.00         1\n",
      "4576436853520646417       0.00      0.00      0.00         1\n",
      "4576439494876148608       0.00      0.00      0.00         1\n",
      "4576440211126353920       0.00      0.00      0.00         1\n",
      "4576443708299364230       0.00      0.00      0.00         1\n",
      "4576451361913113828       0.00      0.00      0.00         1\n",
      "4576496149851811361       0.00      0.00      0.00         1\n",
      "4576522537059483648       0.00      0.00      0.00         1\n",
      "4576560191037767680       0.00      0.00      0.00         1\n",
      "4576610958603796358       0.00      0.00      0.00         1\n",
      "4576620102607063401       0.00      0.00      0.00         1\n",
      "4576620750076641280       0.00      0.00      0.00         1\n",
      "4576632021137198686       0.00      0.00      0.00         1\n",
      "4576664362231281632       0.00      0.00      0.00         1\n",
      "4576680179539116032       0.00      0.00      0.00         1\n",
      "4576680395363834025       0.00      0.00      0.00         1\n",
      "4576680995582902272       0.00      0.00      0.00         1\n",
      "4576699343683190784       0.00      0.00      0.00         1\n",
      "4576703175863936044       0.00      0.00      0.00         1\n",
      "4576747409707171000       0.00      0.00      0.00         1\n",
      "4576752472428642304       0.00      0.00      0.00         1\n",
      "4576768780419465216       0.00      0.00      0.00         1\n",
      "4576772247488373604       0.00      0.00      0.00         1\n",
      "4576779732586070016       0.00      0.00      0.00         1\n",
      "4576781524640710562       0.00      0.00      0.00         1\n",
      "4576809028557996032       0.00      0.00      0.00         1\n",
      "4576856560961060864       0.00      0.00      0.00         1\n",
      "4576864271455486672       0.00      0.00      0.00         1\n",
      "4576865276517925334       0.00      0.00      0.00         1\n",
      "4576882361866888096       0.00      0.00      0.00         1\n",
      "4576890658706423808       0.00      0.00      0.00         1\n",
      "4576926973696671232       0.00      0.00      0.00         1\n",
      "4576948787843996178       0.00      0.00      0.00         1\n",
      "4576968092671803392       0.00      0.00      0.00         1\n",
      "4576972761301254144       0.00      0.00      0.00         1\n",
      "4576975266319171860       0.00      0.00      0.00         1\n",
      "4576979543054614528       0.00      0.00      0.00         1\n",
      "4576984078540079104       0.00      0.00      0.00         1\n",
      "4576993471633555456       0.00      0.00      0.00         1\n",
      "4576997351054285664       0.00      0.00      0.00         1\n",
      "4577017172325768628       0.00      0.00      0.00         1\n",
      "4577017300112113664       0.00      0.00      0.00         1\n",
      "4577025774082588672       0.00      0.00      0.00         1\n",
      "4577032323907715072       0.00      0.00      0.00         1\n",
      "4577036717659258880       0.00      0.00      0.00         1\n",
      "4577067787452678144       0.00      0.00      0.00         1\n",
      "4577095873311611918       0.00      0.00      0.00         1\n",
      "4577118017095204864       0.00      0.00      0.00         1\n",
      "4577143950107738112       0.00      0.00      0.00         1\n",
      "4577149241507446784       0.00      0.00      0.00         1\n",
      "4577181565431316480       0.00      0.00      0.00         1\n",
      "4577200399922500858       0.00      0.00      0.00         1\n",
      "4577205755753667284       0.00      0.00      0.00         1\n",
      "4577220457426493500       0.00      0.00      0.00         1\n",
      "4577235617594736640       0.00      0.00      0.00         1\n",
      "4577235945055439456       0.00      0.00      0.00         1\n",
      "4577271622305579008       0.00      0.00      0.00         1\n",
      "4577282402673491968       0.00      0.00      0.00         1\n",
      "4577283490358899293       0.00      0.00      0.00         1\n",
      "4577294780769239040       0.00      0.00      0.00         1\n",
      "4577327165886883042       0.00      0.00      0.00         1\n",
      "4577327268952657072       0.00      0.00      0.00         1\n",
      "4577351097412794400       0.00      0.00      0.00         1\n",
      "4577428457331359744       0.00      0.00      0.00         1\n",
      "4577445260289055738       0.00      0.00      0.00         1\n",
      "4577452737836574977       0.00      0.00      0.00         1\n",
      "4577453334843261626       0.00      0.00      0.00         1\n",
      "4577484791180796096       0.00      0.00      0.00         1\n",
      "4577503378740871168       0.00      0.00      0.00         1\n",
      "4577528393699898196       0.00      0.00      0.00         1\n",
      "4577528401220337664       0.00      0.00      0.00         1\n",
      "4577578292617509254       0.00      0.00      0.00         1\n",
      "4577621090909552640       0.00      0.00      0.00         1\n",
      "4577639216738243616       0.00      0.00      0.00         1\n",
      "4577702871381835776       0.00      0.00      0.00         1\n",
      "4577712994619752448       0.00      0.00      0.00         1\n",
      "4577736543925436416       0.00      0.00      0.00         1\n",
      "4577769997425704960       0.00      0.00      0.00         1\n",
      "4577814415977480192       0.00      0.00      0.00         1\n",
      "4577905477874089984       0.00      0.00      0.00         1\n",
      "4577910847638173556       0.00      0.00      0.00         1\n",
      "4577911452173598720       0.00      0.00      0.00         1\n",
      "4577929457730552684       0.00      0.00      0.00         1\n",
      "4577937299286786048       0.00      0.00      0.00         1\n",
      "4577959199325028352       0.00      0.00      0.00         1\n",
      "4577995521863450624       0.00      0.00      0.00         1\n",
      "4578009682370625536       0.00      0.00      0.00         1\n",
      "4578029271716462592       0.00      0.00      0.00         1\n",
      "4578033301468307135       0.00      0.00      0.00         1\n",
      "4578167569663393792       0.00      0.00      0.00         1\n",
      "4578230907546107904       0.00      0.00      0.00         1\n",
      "4578265366068723712       0.00      0.00      0.00         1\n",
      "4578271039720521728       0.00      0.00      0.00         1\n",
      "4578302736579166208       0.00      0.00      0.00         1\n",
      "4578327961990143425       0.00      0.00      0.00         1\n",
      "4578347422469071090       0.00      0.00      0.00         1\n",
      "4578358609808719872       0.00      0.00      0.00         1\n",
      "4578368612787552256       0.00      0.00      0.00         1\n",
      "4578380764315708764       0.00      0.00      0.00         1\n",
      "4578390057559261184       0.00      0.00      0.00         1\n",
      "4578395898714783744       0.00      0.00      0.00         1\n",
      "4578407350164875089       0.00      0.00      0.00         1\n",
      "4578436056659001344       0.00      0.00      0.00         1\n",
      "4578437714516377600       0.00      0.00      0.00         1\n",
      "4578459267707519860       0.00      0.00      0.00         1\n",
      "4578460289924836107       0.00      0.00      0.00         1\n",
      "4578470493706780672       0.00      0.00      0.00         1\n",
      "4578553825735657609       0.00      0.00      0.00         1\n",
      "4578559997600919880       0.00      0.00      0.00         1\n",
      "4578563411029262336       0.00      0.00      0.00         1\n",
      "4578577310597073344       0.00      0.00      0.00         1\n",
      "4578584860095938560       0.00      0.00      0.00         1\n",
      "4578640119145168896       0.00      0.00      0.00         1\n",
      "4578668770872000512       0.00      0.00      0.00         1\n",
      "4578677649620702734       0.00      0.00      0.00         1\n",
      "4578688271074765706       0.00      0.00      0.00         1\n",
      "4578696692454391808       0.00      0.00      0.00         1\n",
      "4578717893457084442       0.00      0.00      0.00         1\n",
      "4578735201131167744       0.00      0.00      0.00         1\n",
      "4578749040571656374       0.00      0.00      0.00         1\n",
      "4578760494193573888       0.00      0.00      0.00         1\n",
      "4578783901765337088       0.00      0.00      0.00         1\n",
      "4578836791056948566       0.00      0.00      0.00         1\n",
      "4578838392015421440       0.00      0.00      0.00         1\n",
      "4578966623626841688       0.00      0.00      0.00         1\n",
      "4578981806310997008       0.00      0.00      0.00         1\n",
      "4578992848694806726       0.00      0.00      0.00         1\n",
      "4579002592910114816       0.00      0.00      0.00         1\n",
      "4579059647255674880       0.00      0.00      0.00         1\n",
      "4579091056351510528       0.00      0.00      0.00         1\n",
      "4579109438811537408       0.00      0.00      0.00         1\n",
      "4579115219837517824       0.00      0.00      0.00         1\n",
      "4579138280567271408       0.00      0.00      0.00         1\n",
      "4579152998369853440       0.00      0.00      0.00         1\n",
      "4579162212108058400       0.00      0.00      0.00         1\n",
      "4579163538219597824       0.00      0.00      0.00         1\n",
      "4579206977518829568       0.00      0.00      0.00         1\n",
      "4579210199781104192       0.00      0.00      0.00         1\n",
      "4579237507200710558       0.00      0.00      0.00         1\n",
      "4579240341881275813       0.00      0.00      0.00         1\n",
      "4579241096739028992       0.00      0.00      0.00         1\n",
      "4579273876994051536       0.00      0.00      0.00         1\n",
      "4579295025389434548       0.00      0.00      0.00         1\n",
      "4579332772815962112       0.00      0.00      0.00         1\n",
      "4579335101749165284       0.00      0.00      0.00         1\n",
      "4579340353433239552       0.00      0.00      0.00         1\n",
      "4579352612331075104       0.00      0.00      0.00         1\n",
      "4579352809895081824       0.00      0.00      0.00         1\n",
      "4579408845276708864       0.00      0.00      0.00         1\n",
      "4579429754213439510       0.00      0.00      0.00         1\n",
      "4579440211422871552       0.00      0.00      0.00         1\n",
      "4579489359800959212       0.00      0.00      0.00         1\n",
      "4579489853699243408       0.00      0.00      0.00         1\n",
      "4579500719923631728       0.00      0.00      0.00         1\n",
      "4579504301978168296       0.00      0.00      0.00         1\n",
      "4579525008013210882       0.00      0.00      0.00         1\n",
      "4579526008744491242       0.00      0.00      0.00         1\n",
      "4579539846074204160       0.00      0.00      0.00         1\n",
      "4579585222403686400       0.00      0.00      0.00         1\n",
      "4579626158796711557       0.00      0.00      0.00         1\n",
      "4579642676181204992       0.00      0.00      0.00         1\n",
      "4579645433550209024       0.00      0.00      0.00         1\n",
      "4579673355132600320       0.00      0.00      0.00         1\n",
      "4579680463303475200       0.00      0.00      0.00         1\n",
      "4579685880321677475       0.00      0.00      0.00         1\n",
      "4579704073776804384       0.00      0.00      0.00         1\n",
      "4579704227357523968       0.00      0.00      0.00         1\n",
      "4579704456056364347       0.00      0.00      0.00         1\n",
      "4579723924077543424       0.00      0.00      0.00         1\n",
      "4579743620797562880       0.00      0.00      0.00         1\n",
      "4579745519173107712       0.00      0.00      0.00         1\n",
      "4579765002195902856       0.00      0.00      0.00         1\n",
      "4579782813424808700       0.00      0.00      0.00         1\n",
      "4579842804477329408       0.00      0.00      0.00         1\n",
      "4579863558818659202       0.00      0.00      0.00         1\n",
      "4579865005163282432       0.00      0.00      0.00         1\n",
      "4579865400300273664       0.00      0.00      0.00         1\n",
      "4579873806602518632       0.00      0.00      0.00         1\n",
      "4579902371378757632       0.00      0.00      0.00         1\n",
      "4579936215721050112       0.00      0.00      0.00         1\n",
      "4579948848286030739       0.00      0.00      0.00         1\n",
      "4579953640403369984       0.00      0.00      0.00         1\n",
      "4579963150505379808       0.00      0.00      0.00         1\n",
      "4579999390395006976       0.00      0.00      0.00         1\n",
      "4580029189944151808       0.00      0.00      0.00         1\n",
      "4580044978241097171       0.00      0.00      0.00         1\n",
      "4580051154397528416       0.00      0.00      0.00         1\n",
      "4580078082785804288       0.00      0.00      0.00         1\n",
      "4580141021236559872       0.00      0.00      0.00         1\n",
      "4580163497841109566       0.00      0.00      0.00         1\n",
      "4580200635382628352       0.00      0.00      0.00         1\n",
      "4580201916339437006       0.00      0.00      0.00         1\n",
      "4580229910947567362       0.00      0.00      0.00         1\n",
      "4580240007347830784       0.00      0.00      0.00         1\n",
      "4580286659282599936       0.00      0.00      0.00         1\n",
      "4580309882170769408       0.00      0.00      0.00         1\n",
      "4580310474876256256       0.00      0.00      0.00         1\n",
      "4580321788873659928       0.00      0.00      0.00         1\n",
      "4580321960680332162       0.00      0.00      0.00         1\n",
      "4580363338382621919       0.00      0.00      0.00         1\n",
      "4580369509201739776       0.00      0.00      0.00         1\n",
      "4580371782269312448       0.00      0.00      0.00         1\n",
      "4580376635616145098       0.00      0.00      0.00         1\n",
      "4580378542583346463       0.00      0.00      0.00         1\n",
      "4580383249866644512       0.00      0.00      0.00         1\n",
      "4580386143610077184       0.00      0.00      0.00         1\n",
      "4580397161228807296       0.00      0.00      0.00         1\n",
      "4580442679317578608       0.00      0.00      0.00         1\n",
      "4580445061971443712       0.00      0.00      0.00         1\n",
      "4580470831775219712       0.00      0.00      0.00         1\n",
      "4580482741719531520       0.00      0.00      0.00         1\n",
      "4580488141542968426       0.00      0.00      0.00         1\n",
      "4580547003020214272       0.00      0.00      0.00         1\n",
      "4580554094011219968       0.00      0.00      0.00         1\n",
      "4580572576316244659       0.00      0.00      0.00         1\n",
      "4580578507671367164       0.00      0.00      0.00         1\n",
      "4580634685848830444       0.00      0.00      0.00         1\n",
      "4580659218696364138       0.00      0.00      0.00         1\n",
      "4580668444288133332       0.00      0.00      0.00         1\n",
      "4580670448970235904       0.00      0.00      0.00         1\n",
      "4580676771162095616       0.00      0.00      0.00         1\n",
      "4580682101216509952       0.00      0.00      0.00         1\n",
      "4580715533241942016       0.00      0.00      0.00         1\n",
      "4580742174924079104       0.00      0.00      0.00         1\n",
      "4580759622143188524       0.00      0.00      0.00         1\n",
      "4580785052650939716       0.00      0.00      0.00         1\n",
      "4580788780663597083       0.00      0.00      0.00         1\n",
      "4580804950166077440       0.00      0.00      0.00         1\n",
      "4580846200091680448       0.00      0.00      0.00         1\n",
      "4580874335362744320       0.00      0.00      0.00         1\n",
      "4580897340249823308       0.00      0.00      0.00         1\n",
      "4580908424518172672       0.00      0.00      0.00         1\n",
      "4580937153554415616       0.00      0.00      0.00         1\n",
      "4581041074583109632       0.00      0.00      0.00         1\n",
      "4581082869975408435       0.00      0.00      0.00         1\n",
      "4581087795237355520       0.00      0.00      0.00         1\n",
      "4581094796034048000       0.00      0.00      0.00         1\n",
      "4581115106934390784       0.00      0.00      0.00         1\n",
      "4581139974795034624       0.00      0.00      0.00         1\n",
      "4581167502305802466       0.00      0.00      0.00         1\n",
      "4581203519858237088       0.00      0.00      0.00         1\n",
      "4581207792328638464       0.00      0.00      0.00         1\n",
      "4581222052686056914       0.00      0.00      0.00         1\n",
      "4581240928001327104       0.00      0.00      0.00         1\n",
      "4581256442480733004       0.00      0.00      0.00         1\n",
      "4581302759411951689       0.00      0.00      0.00         1\n",
      "4581314287099616394       0.00      0.00      0.00         1\n",
      "4581316635389853696       0.00      0.00      0.00         1\n",
      "4581317769261219840       0.00      0.00      0.00         1\n",
      "4581329212090642964       0.00      0.00      0.00         1\n",
      "4581366418355781632       0.00      0.00      0.00         1\n",
      "4581376899135092682       0.00      0.00      0.00         1\n",
      "4581383641174638592       0.00      0.00      0.00         1\n",
      "4581464293130297434       0.00      0.00      0.00         1\n",
      "4581492111626226432       0.00      0.00      0.00         1\n",
      "4581516643426893824       0.00      0.00      0.00         1\n",
      "4581523194305495061       0.00      0.00      0.00         1\n",
      "4581533334716305771       0.00      0.00      0.00         1\n",
      "4581559674704232448       0.00      0.00      0.00         1\n",
      "4581563553059700736       0.00      0.00      0.00         1\n",
      "4581565180852305920       0.00      0.00      0.00         1\n",
      "4581573716011847746       0.00      0.00      0.00         1\n",
      "4581603586449866752       0.00      0.00      0.00         1\n",
      "4581694554909923922       0.00      0.00      0.00         1\n",
      "4581718949271437312       0.00      0.00      0.00         1\n",
      "4581719238080287552       0.00      0.00      0.00         1\n",
      "4581720770337570816       0.00      0.00      0.00         1\n",
      "4581720943185318301       0.00      0.00      0.00         1\n",
      "4581734742931070786       0.00      0.00      0.00         1\n",
      "4581763986298503168       0.00      0.00      0.00         1\n",
      "4581764840996995072       0.00      0.00      0.00         1\n",
      "4581853270078652416       0.00      0.00      0.00         1\n",
      "4581857789442827451       0.00      0.00      0.00         1\n",
      "4581866245174853632       0.00      0.00      0.00         1\n",
      "4581870451016905723       0.00      0.00      0.00         1\n",
      "4581906046636785664       0.00      0.00      0.00         1\n",
      "4581929889044951122       0.00      0.00      0.00         1\n",
      "4581966903079445540       0.00      0.00      0.00         1\n",
      "4581975161250512896       0.00      0.00      0.00         1\n",
      "4581999170117697536       0.00      0.00      0.00         1\n",
      "4581999957166732628       0.00      0.00      0.00         1\n",
      "4582009568233521152       0.00      0.00      0.00         1\n",
      "4582014950876543712       0.00      0.00      0.00         1\n",
      "4582063621464287553       0.00      0.00      0.00         1\n",
      "4582067061713822116       0.00      0.00      0.00         1\n",
      "4582120498648842240       0.00      0.00      0.00         1\n",
      "4582183505819074560       0.00      0.00      0.00         1\n",
      "4582188858421532055       0.00      0.00      0.00         1\n",
      "4582229496328880128       0.00      0.00      0.00         1\n",
      "4582231142350606954       0.00      0.00      0.00         1\n",
      "4582240329254683696       0.00      0.00      0.00         1\n",
      "4582288504884559872       0.00      0.00      0.00         1\n",
      "4582315787575352267       0.00      0.00      0.00         1\n",
      "4582322315921326686       0.00      0.00      0.00         1\n",
      "4582350426449654432       0.00      0.00      0.00         1\n",
      "4582406762514087936       0.00      0.00      0.00         1\n",
      "4582469262878179328       0.00      0.00      0.00         1\n",
      "4582474489853378560       0.00      0.00      0.00         1\n",
      "4582490754894528512       0.00      0.00      0.00         1\n",
      "4582493168666148864       0.00      0.00      0.00         1\n",
      "4582520661812831981       0.00      0.00      0.00         1\n",
      "4582525283179492098       0.00      0.00      0.00         1\n",
      "4582528825484640256       0.00      0.00      0.00         1\n",
      "4582534010560434848       0.00      0.00      0.00         1\n",
      "4582555883778605056       0.00      0.00      0.00         1\n",
      "4582699422649086775       0.00      0.00      0.00         1\n",
      "4582742981143953408       0.00      0.00      0.00         1\n",
      "4582787460882284711       0.00      0.00      0.00         1\n",
      "4582838634360602624       0.00      0.00      0.00         1\n",
      "4582904936824915206       0.00      0.00      0.00         1\n",
      "4582917692882782540       0.00      0.00      0.00         1\n",
      "4582932708100655192       0.00      0.00      0.00         1\n",
      "4582946417614239623       0.00      0.00      0.00         1\n",
      "4582971537828610048       0.00      0.00      0.00         1\n",
      "4582978714718961664       0.00      0.00      0.00         1\n",
      "4582997914280783010       0.00      0.00      0.00         1\n",
      "4583021991879461965       0.00      0.00      0.00         1\n",
      "4583051721647368721       0.00      0.00      0.00         1\n",
      "4583058619345110522       0.00      0.00      0.00         1\n",
      "4583060087169351680       0.00      0.00      0.00         1\n",
      "4583105064066875392       0.00      0.00      0.00         1\n",
      "4583113960000120986       0.00      0.00      0.00         1\n",
      "4583165237606106256       0.00      0.00      0.00         1\n",
      "4583191135211487232       0.00      0.00      0.00         1\n",
      "4583208191584210394       0.00      0.00      0.00         1\n",
      "4583214787596386304       0.00      0.00      0.00         1\n",
      "4583221299806363120       0.00      0.00      0.00         1\n",
      "4583235609597837312       0.00      0.00      0.00         1\n",
      "4583264128180682752       0.00      0.00      0.00         1\n",
      "4583322703996573202       0.00      0.00      0.00         1\n",
      "4583323669312307200       0.00      0.00      0.00         1\n",
      "4583337331603275776       0.00      0.00      0.00         1\n",
      "4583413283804938240       0.00      0.00      0.00         1\n",
      "4583415134935842816       0.00      0.00      0.00         1\n",
      "4583506090516294386       0.00      0.00      0.00         1\n",
      "4583541570183102464       0.00      0.00      0.00         1\n",
      "4583563805228793856       0.00      0.00      0.00         1\n",
      "4583565072244146176       0.00      0.00      0.00         1\n",
      "4583574121740238848       0.00      0.00      0.00         1\n",
      "4583673193750855680       0.00      0.00      0.00         1\n",
      "4583715134106501120       0.00      0.00      0.00         1\n",
      "4583720357841786355       0.00      0.00      0.00         1\n",
      "4583722969181529560       0.00      0.00      0.00         1\n",
      "4583746521727500288       0.00      0.00      0.00         1\n",
      "4583755734432350208       0.00      0.00      0.00         1\n",
      "4583757638170375287       0.00      0.00      0.00         1\n",
      "4583785246195231544       0.00      0.00      0.00         1\n",
      "4583791650012032902       0.00      0.00      0.00         1\n",
      "4583821851158904832       0.00      0.00      0.00         1\n",
      "4583822290310812816       0.00      0.00      0.00         1\n",
      "4583878829243723092       0.00      0.00      0.00         1\n",
      "4583917217653812616       0.00      0.00      0.00         1\n",
      "4583987550997184512       0.00      0.00      0.00         1\n",
      "4584044872692306721       0.00      0.00      0.00         1\n",
      "4584052663760549912       0.00      0.00      0.00         1\n",
      "4584127086960434256       0.00      0.00      0.00         1\n",
      "4584159157462869136       0.00      0.00      0.00         1\n",
      "4584189654978265088       0.00      0.00      0.00         1\n",
      "4584219235488097340       0.00      0.00      0.00         1\n",
      "4584267205936042720       0.00      0.00      0.00         1\n",
      "4584284230158123008       0.00      0.00      0.00         1\n",
      "4584325573513314304       0.00      0.00      0.00         1\n",
      "4584443840798208366       0.00      0.00      0.00         1\n",
      "4584453228531286016       0.00      0.00      0.00         1\n",
      "4584456862073618432       0.00      0.00      0.00         1\n",
      "4584629726971797908       0.00      0.00      0.00         1\n",
      "4584690233416613888       0.00      0.00      0.00         1\n",
      "4584692944598209556       0.00      0.00      0.00         1\n",
      "4584708728618978056       0.00      0.00      0.00         1\n",
      "4584719122426768613       0.00      0.00      0.00         1\n",
      "4584760808319221760       0.00      0.00      0.00         1\n",
      "4584790877385261056       0.00      0.00      0.00         1\n",
      "4584836133455659008       0.00      0.00      0.00         1\n",
      "4584840472437809164       0.00      0.00      0.00         1\n",
      "4584879380541748589       0.00      0.00      0.00         1\n",
      "4584879767076015351       0.00      0.00      0.00         1\n",
      "4585025173212863189       0.00      0.00      0.00         1\n",
      "4585028626349910728       0.00      0.00      0.00         1\n",
      "4585041351288029184       0.00      0.00      0.00         1\n",
      "4585149413725389995       0.00      0.00      0.00         1\n",
      "4585174973052781156       0.00      0.00      0.00         1\n",
      "4585192907799003136       0.00      0.00      0.00         1\n",
      "4585212471375036416       0.00      0.00      0.00         1\n",
      "4585292877457784832       0.00      0.00      0.00         1\n",
      "4585326950496895753       0.00      0.00      0.00         1\n",
      "4585350593228308480       0.00      0.00      0.00         1\n",
      "4585363104468041728       0.00      0.00      0.00         1\n",
      "4585410517646596048       0.00      0.00      0.00         1\n",
      "4585420353148144409       0.00      0.00      0.00         1\n",
      "4585423053621559296       0.00      0.00      0.00         1\n",
      "4585439722389635072       0.00      0.00      0.00         1\n",
      "4585506440411611136       0.00      0.00      0.00         1\n",
      "4585523560151252992       0.00      0.00      0.00         1\n",
      "4585560187632353280       0.00      0.00      0.00         1\n",
      "4585565135434678272       0.00      0.00      0.00         1\n",
      "4585608438349651278       0.00      0.00      0.00         1\n",
      "4585630054916274334       0.00      0.00      0.00         1\n",
      "4585654045552672768       0.00      0.00      0.00         1\n",
      "4585710739120979968       0.00      0.00      0.00         1\n",
      "4585762094044938240       0.00      0.00      0.00         1\n",
      "4585797957021859840       0.00      0.00      0.00         1\n",
      "4585818332346712064       0.00      0.00      0.00         1\n",
      "4585944489486096960       0.00      0.00      0.00         1\n",
      "4586050493569026736       0.00      0.00      0.00         1\n",
      "4586052889100681216       0.00      0.00      0.00         1\n",
      "4586101353511649280       0.00      0.00      0.00         1\n",
      "4586108905121593586       0.00      0.00      0.00         1\n",
      "4586157935410806784       0.00      0.00      0.00         1\n",
      "4586162695296236847       0.00      0.00      0.00         1\n",
      "4586222140876914688       0.00      0.00      0.00         1\n",
      "4586325019287554778       0.00      0.00      0.00         1\n",
      "4586348627663781888       0.00      0.00      0.00         1\n",
      "4586373083207565312       0.00      0.00      0.00         1\n",
      "4586388193968596586       0.00      0.00      0.00         1\n",
      "4586408358837102856       0.00      0.00      0.00         1\n",
      "4586415178182033408       0.00      0.00      0.00         1\n",
      "4586442770115785790       0.00      0.00      0.00         1\n",
      "4586445617678841896       0.00      0.00      0.00         1\n",
      "4586462444297125888       0.00      0.00      0.00         1\n",
      "4586495451120795648       0.00      0.00      0.00         1\n",
      "4586535226812923904       0.00      0.00      0.00         1\n",
      "4586606441665658880       0.00      0.00      0.00         1\n",
      "4586646815419476135       0.00      0.00      0.00         1\n",
      "4586706454274113536       0.00      0.00      0.00         1\n",
      "4586749817317500689       0.00      0.00      0.00         1\n",
      "4586829621051260928       0.00      0.00      0.00         1\n",
      "4586860506161086464       0.00      0.00      0.00         1\n",
      "4586871144795078656       0.00      0.00      0.00         1\n",
      "4586889210493754298       0.00      0.00      0.00         1\n",
      "4586934663066419200       0.00      0.00      0.00         1\n",
      "4586937222866927616       0.00      0.00      0.00         1\n",
      "4586948734424938988       0.00      0.00      0.00         1\n",
      "4586975246212399104       0.00      0.00      0.00         1\n",
      "4587004894371643392       0.00      0.00      0.00         1\n",
      "4587076671865094144       0.00      0.00      0.00         1\n",
      "4587120527776153600       0.00      0.00      0.00         1\n",
      "4587134215836925952       0.00      0.00      0.00         1\n",
      "4587150090036051968       0.00      0.00      0.00         1\n",
      "4587174730263429120       0.00      0.00      0.00         1\n",
      "4587177089265017040       0.00      0.00      0.00         1\n",
      "4587211328740943995       0.00      0.00      0.00         1\n",
      "4587246967318380544       0.00      0.00      0.00         1\n",
      "4587284399028032412       0.00      0.00      0.00         1\n",
      "4587326024781398016       0.00      0.00      0.00         1\n",
      "4587357915977586244       0.00      0.00      0.00         1\n",
      "4587364117896283175       0.00      0.00      0.00         1\n",
      "4587364288645038080       0.00      0.00      0.00         1\n",
      "4587379944867361526       0.00      0.00      0.00         1\n",
      "4587406925851456566       0.00      0.00      0.00         1\n",
      "4587437024950923648       0.00      0.00      0.00         1\n",
      "4587554388192526336       0.00      0.00      0.00         1\n",
      "4587588113333231036       0.00      0.00      0.00         1\n",
      "4587678907884371968       0.00      0.00      0.00         1\n",
      "4587731956078987728       0.00      0.00      0.00         1\n",
      "4587793945321463008       0.00      0.00      0.00         1\n",
      "4587796452549328896       0.00      0.00      0.00         1\n",
      "4587820263848017920       0.00      0.00      0.00         1\n",
      "4587873663176409088       0.00      0.00      0.00         1\n",
      "4587873741543514099       0.00      0.00      0.00         1\n",
      "4587890336239452160       0.00      0.00      0.00         1\n",
      "4587899140922408960       0.00      0.00      0.00         1\n",
      "4587953437898964992       0.00      0.00      0.00         1\n",
      "4588070206245189760       0.00      0.00      0.00         1\n",
      "4588232318715428864       0.00      0.00      0.00         1\n",
      "4588247098770893203       0.00      0.00      0.00         1\n",
      "4588263720290128944       0.00      0.00      0.00         1\n",
      "4588303998474566522       0.00      0.00      0.00         1\n",
      "4588306180300489544       0.00      0.00      0.00         1\n",
      "4588306802038276096       0.00      0.00      0.00         1\n",
      "4588318034444200209       0.00      0.00      0.00         1\n",
      "4588408300705415168       0.00      0.00      0.00         1\n",
      "4588492380037830117       0.00      0.00      0.00         1\n",
      "4588571307599200256       0.00      0.00      0.00         1\n",
      "4588599522242818816       0.00      0.00      0.00         1\n",
      "4588604181278883840       0.00      0.00      0.00         1\n",
      "4588616958806589440       0.00      0.00      0.00         1\n",
      "4588647092297138176       0.00      0.00      0.00         1\n",
      "4588751860494032007       0.00      0.00      0.00         1\n",
      "4588827013830547863       0.00      0.00      0.00         1\n",
      "4588881160483807990       0.00      0.00      0.00         1\n",
      "4588885945075886136       0.00      0.00      0.00         1\n",
      "4588900216194727936       0.00      0.00      0.00         1\n",
      "4588903506139676672       0.00      0.00      0.00         1\n",
      "4589063778166653504       0.00      0.00      0.00         1\n",
      "4589068167643852336       0.00      0.00      0.00         1\n",
      "4589092661857766048       0.00      0.00      0.00         1\n",
      "4589109720404459520       0.00      0.00      0.00         1\n",
      "4589133240713527080       0.00      0.00      0.00         1\n",
      "4589210185046374929       0.00      0.00      0.00         1\n",
      "4589220840849192868       0.00      0.00      0.00         1\n",
      "4589459250170269254       0.00      0.00      0.00         1\n",
      "4589477339540226048       0.00      0.00      0.00         1\n",
      "4589517647808299008       0.00      0.00      0.00         1\n",
      "4589593548470353920       0.00      0.00      0.00         1\n",
      "4589614036529883092       0.00      0.00      0.00         1\n",
      "4589680548410865798       0.00      0.00      0.00         1\n",
      "4589701168536267166       0.00      0.00      0.00         1\n",
      "4589711153264852992       0.00      0.00      0.00         1\n",
      "4589722852755767296       0.00      0.00      0.00         1\n",
      "4589777085307813888       0.00      0.00      0.00         1\n",
      "4589988390160683968       0.00      0.00      0.00         1\n",
      "4590077935948918451       0.00      0.00      0.00         1\n",
      "4590104872916877312       0.00      0.00      0.00         1\n",
      "4590122564955263992       0.00      0.00      0.00         1\n",
      "4590152521284059136       0.00      0.00      0.00         1\n",
      "4590177638252806144       0.00      0.00      0.00         1\n",
      "4590302992227097196       0.00      0.00      0.00         1\n",
      "4590341045631985294       0.00      0.00      0.00         1\n",
      "4590428332264709176       0.00      0.00      0.00         1\n",
      "4590475721868050432       0.00      0.00      0.00         1\n",
      "4590518211979509760       0.00      0.00      0.00         1\n",
      "4590531828052544448       0.00      0.00      0.00         1\n",
      "4590548878046003200       0.00      0.00      0.00         1\n",
      "4590639768143921152       0.00      0.00      0.00         1\n",
      "4590683904299996598       0.00      0.00      0.00         1\n",
      "4590782039993279675       0.00      0.00      0.00         1\n",
      "4590817091196544960       0.00      0.00      0.00         1\n",
      "4590844766932959232       0.00      0.00      0.00         1\n",
      "4590846013535564365       0.00      0.00      0.00         1\n",
      "4590868175573028222       0.00      0.00      0.00         1\n",
      "4590886166122725376       0.00      0.00      0.00         1\n",
      "4590958631878796046       0.00      0.00      0.00         1\n",
      "4591003187867447098       0.00      0.00      0.00         1\n",
      "4591059240419852288       0.00      0.00      0.00         1\n",
      "4591066138137329664       0.00      0.00      0.00         1\n",
      "4591071266328281088       0.00      0.00      0.00         1\n",
      "4591076537319580129       0.00      0.00      0.00         1\n",
      "4591111505876877312       0.00      0.00      0.00         1\n",
      "4591157706840080384       0.00      0.00      0.00         1\n",
      "4591271339841929900       0.00      0.00      0.00         1\n",
      "4591352105649831936       0.00      0.00      0.00         1\n",
      "4591362388873983230       0.00      0.00      0.00         1\n",
      "4591395829466295203       0.00      0.00      0.00         1\n",
      "4591460511684778980       0.00      0.00      0.00         1\n",
      "4591495342809153536       0.00      0.00      0.00         1\n",
      "4591577527008362496       0.00      0.00      0.00         1\n",
      "4591589144894898176       0.00      0.00      0.00         1\n",
      "4591696382693688406       0.00      0.00      0.00         1\n",
      "4591699001568395264       0.00      0.00      0.00         1\n",
      "4591793320115652219       0.00      0.00      0.00         1\n",
      "4591830609005755925       0.00      0.00      0.00         1\n",
      "4591843643182022656       0.00      0.00      0.00         1\n",
      "4591852284656222208       0.00      0.00      0.00         1\n",
      "4591974158648213504       0.00      0.00      0.00         1\n",
      "4592032943865593856       0.00      0.00      0.00         1\n",
      "4592070645088518144       0.00      0.00      0.00         1\n",
      "4592118986017442177       0.00      0.00      0.00         1\n",
      "4592270825974542384       0.00      0.00      0.00         1\n",
      "4592277086986567680       0.00      0.00      0.00         1\n",
      "4592307483537239470       0.00      0.00      0.00         1\n",
      "4592339996421184384       0.00      0.00      0.00         1\n",
      "4592354057095479296       0.00      0.00      0.00         1\n",
      "4592463282408783872       0.00      0.00      0.00         1\n",
      "4592488523931582464       0.00      0.00      0.00         1\n",
      "4592532087784865792       0.00      0.00      0.00         1\n",
      "4592771825322093639       0.00      0.00      0.00         1\n",
      "4592783791118605341       0.00      0.00      0.00         1\n",
      "4592801301679401092       0.00      0.00      0.00         1\n",
      "4592806132468154368       0.00      0.00      0.00         1\n",
      "4592845273570633255       0.00      0.00      0.00         1\n",
      "4592847597145817382       0.00      0.00      0.00         1\n",
      "4592865902293654806       0.00      0.00      0.00         1\n",
      "4592945968013377536       0.00      0.00      0.00         1\n",
      "4592978437966135296       0.00      0.00      0.00         1\n",
      "4593040926476617456       0.00      0.00      0.00         1\n",
      "4593088535157800960       0.00      0.00      0.00         1\n",
      "4593104581155618816       0.00      0.00      0.00         1\n",
      "4593118277806325760       0.00      0.00      0.00         1\n",
      "4593258835958488142       0.00      0.00      0.00         1\n",
      "4593262847432467688       0.00      0.00      0.00         1\n",
      "4593308318275410298       0.00      0.00      0.00         1\n",
      "4593379992707140981       0.00      0.00      0.00         1\n",
      "4593439721748692992       0.00      0.00      0.00         1\n",
      "4593488181864693760       0.00      0.00      0.00         1\n",
      "4593674554445350036       0.00      0.00      0.00         1\n",
      "4593678397376299008       0.00      0.00      0.00         1\n",
      "4593778848071417856       0.00      0.00      0.00         1\n",
      "4593839351275716608       0.00      0.00      0.00         1\n",
      "4593861844019445760       0.00      0.00      0.00         1\n",
      "4593875665224204288       0.00      0.00      0.00         1\n",
      "4593901778625363968       0.00      0.00      0.00         1\n",
      "4593906521332774782       0.00      0.00      0.00         1\n",
      "4593994395300134912       0.00      0.00      0.00         1\n",
      "4594029847027823108       0.00      0.00      0.00         1\n",
      "4594053860178329386       0.00      0.00      0.00         1\n",
      "4594069007472001024       0.00      0.00      0.00         1\n",
      "4594074547979812864       0.00      0.00      0.00         1\n",
      "4594080054127886336       0.00      0.00      0.00         1\n",
      "4594149349130240000       0.00      0.00      0.00         1\n",
      "4594263801418743808       0.00      0.00      0.00         1\n",
      "4594285096926489637       0.00      0.00      0.00         1\n",
      "4594287681436909568       0.00      0.00      0.00         1\n",
      "4594308443308818432       0.00      0.00      0.00         1\n",
      "4594309349546917888       0.00      0.00      0.00         1\n",
      "4594462392116576256       0.00      0.00      0.00         1\n",
      "4594509065526181888       0.00      0.00      0.00         1\n",
      "4594535797402632192       0.00      0.00      0.00         1\n",
      "4594557436493132159       0.00      0.00      0.00         1\n",
      "4594569337864822402       0.00      0.00      0.00         1\n",
      "4594584742849937408       0.00      0.00      0.00         1\n",
      "4594588411809832298       0.00      0.00      0.00         1\n",
      "4594749786620853118       0.00      0.00      0.00         1\n",
      "4594773313389068288       0.00      0.00      0.00         1\n",
      "4594943699036667904       0.00      0.00      0.00         1\n",
      "4595204743912850224       0.00      0.00      0.00         1\n",
      "4595297136190423040       0.00      0.00      0.00         1\n",
      "4595310648157536256       0.00      0.00      0.00         1\n",
      "4595355783968849920       0.00      0.00      0.00         1\n",
      "4595420127936820054       0.00      0.00      0.00         1\n",
      "4595560108448022528       0.00      0.00      0.00         1\n",
      "4595601212344643698       0.00      0.00      0.00         1\n",
      "4595701890681437724       0.00      0.00      0.00         1\n",
      "4595730884937646080       0.00      0.00      0.00         1\n",
      "4595758103214049506       0.00      0.00      0.00         1\n",
      "4595775751225685458       0.00      0.00      0.00         1\n",
      "4595803822072266752       0.00      0.00      0.00         1\n",
      "4596050135219168504       0.00      0.00      0.00         1\n",
      "4596357756889333760       0.00      0.00      0.00         1\n",
      "4596458483514774632       0.00      0.00      0.00         1\n",
      "4596481108350074880       0.00      0.00      0.00         1\n",
      "4596485695375147008       0.00      0.00      0.00         1\n",
      "4596576327775027200       0.00      0.00      0.00         1\n",
      "4596725096852226048       0.00      0.00      0.00         1\n",
      "4596953907997342520       0.00      0.00      0.00         1\n",
      "4596960298911525148       0.00      0.00      0.00         1\n",
      "4597298832538222276       0.00      0.00      0.00         1\n",
      "4597603345703101738       0.00      0.00      0.00         1\n",
      "4597606239457771520       0.00      0.00      0.00         1\n",
      "4597625396061792014       0.00      0.00      0.00         1\n",
      "4597645174393476923       0.00      0.00      0.00         1\n",
      "4597705500446949376       0.00      0.00      0.00         1\n",
      "4597753462346743808       0.00      0.00      0.00         1\n",
      "4597803996931948544       0.00      0.00      0.00         1\n",
      "4597842326281460292       0.00      0.00      0.00         1\n",
      "4597976285250060288       0.00      0.00      0.00         1\n",
      "4598063250790211980       0.00      0.00      0.00         1\n",
      "4598132884052639744       0.00      0.00      0.00         1\n",
      "4598151489850966016       0.00      0.00      0.00         1\n",
      "4598303338419716096       0.00      0.00      0.00         1\n",
      "4598555766532603904       0.00      0.00      0.00         1\n",
      "4598779505312636752       0.00      0.00      0.00         1\n",
      "4598888179821445120       0.00      0.00      0.00         1\n",
      "4598898869995044864       0.00      0.00      0.00         1\n",
      "4598927951218606080       0.00      0.00      0.00         1\n",
      "4599034643553982848       0.00      0.00      0.00         1\n",
      "4599046097178984448       0.00      0.00      0.00         1\n",
      "4599120460242747392       0.00      0.00      0.00         1\n",
      "4599163672950475544       0.00      0.00      0.00         1\n",
      "4599329166654740353       0.00      0.00      0.00         1\n",
      "4599421717838823424       0.00      0.00      0.00         1\n",
      "4599571891370328064       0.00      0.00      0.00         1\n",
      "4599602866674466816       0.00      0.00      0.00         1\n",
      "4599664233167192064       0.00      0.00      0.00         1\n",
      "4599787662985951088       0.00      0.00      0.00         1\n",
      "4600001810066563752       0.00      0.00      0.00         1\n",
      "4600048899028156416       0.00      0.00      0.00         1\n",
      "4600370463229607936       0.00      0.00      0.00         1\n",
      "4600432977532119100       0.00      0.00      0.00         1\n",
      "4600491817530556416       0.00      0.00      0.00         1\n",
      "4600670204702228480       0.00      0.00      0.00         1\n",
      "4600933825499889664       0.00      0.00      0.00         1\n",
      "4600961859805997865       0.00      0.00      0.00         1\n",
      "4601030732846989312       0.00      0.00      0.00         1\n",
      "4601031296554556100       0.00      0.00      0.00         1\n",
      "4601115438192001024       0.00      0.00      0.00         1\n",
      "4601354929863393280       0.00      0.00      0.00         1\n",
      "4601364568838851647       0.00      0.00      0.00         1\n",
      "4601413462745992615       0.00      0.00      0.00         1\n",
      "4601482894119010304       0.00      0.00      0.00         1\n",
      "4601521600364281856       0.00      0.00      0.00         1\n",
      "4601614612176044032       0.00      0.00      0.00         1\n",
      "4601763767800299520       0.00      0.00      0.00         1\n",
      "4601828999763591168       0.00      0.00      0.00         1\n",
      "4601998200000217088       0.00      0.00      0.00         1\n",
      "4602204793286448738       0.00      0.00      0.00         1\n",
      "4602608975485001324       0.00      0.00      0.00         1\n",
      "4602613402525761536       0.00      0.00      0.00         1\n",
      "4602680627353878528       0.00      0.00      0.00         1\n",
      "4602684673213071360       0.00      0.00      0.00         1\n",
      "4603045416106196992       0.00      0.00      0.00         1\n",
      "4603112440139673793       0.00      0.00      0.00         1\n",
      "4603393880739032040       0.00      0.00      0.00         1\n",
      "4603448984118231040       0.00      0.00      0.00         1\n",
      "4603677838225138547       0.00      0.00      0.00         1\n",
      "4603679314624380928       0.00      0.00      0.00         1\n",
      "4603737606981399952       0.00      0.00      0.00         1\n",
      "4603789326964480724       0.00      0.00      0.00         1\n",
      "4603872442123812864       0.00      0.00      0.00         1\n",
      "4603946702108360704       0.00      0.00      0.00         1\n",
      "4603986731203559424       0.00      0.00      0.00         1\n",
      "4603986895468004100       0.00      0.00      0.00         1\n",
      "4604010349228720128       0.00      0.00      0.00         1\n",
      "4604069023851504274       0.00      0.00      0.00         1\n",
      "4604178356528849666       0.00      0.00      0.00         1\n",
      "4604184016231333888       0.00      0.00      0.00         1\n",
      "4604351048569902702       0.00      0.00      0.00         1\n",
      "4604356626671992832       0.00      0.00      0.00         1\n",
      "4604558542724149582       0.00      0.00      0.00         1\n",
      "4604704407353819136       0.00      0.00      0.00         1\n",
      "4604742581023145984       0.00      0.00      0.00         1\n",
      "4604778831589404200       0.00      0.00      0.00         1\n",
      "4604829249168211968       0.00      0.00      0.00         1\n",
      "4604869617565827072       0.00      0.00      0.00         1\n",
      "4604941476663656448       0.00      0.00      0.00         1\n",
      "4605376960577667072       0.00      0.00      0.00         1\n",
      "4605569796019322880       0.00      0.00      0.00         1\n",
      "4605570435969449984       0.00      0.00      0.00         1\n",
      "4605697047310368768       0.00      0.00      0.00         1\n",
      "4606168807584330125       0.00      0.00      0.00         1\n",
      "4606348131066257829       0.00      0.00      0.00         1\n",
      "4606443379482427392       0.00      0.00      0.00         1\n",
      "4607502548482392064       0.00      0.00      0.00         1\n",
      "4607535285762726088       0.00      0.00      0.00         1\n",
      "4607767023978545152       0.00      0.00      0.00         1\n",
      "4608115424200669994       0.00      0.00      0.00         1\n",
      "4608155735698702336       0.00      0.00      0.00         1\n",
      "4608219335574421504       0.00      0.00      0.00         1\n",
      "4608244461133103104       0.00      0.00      0.00         1\n",
      "4608468496279817890       0.00      0.00      0.00         1\n",
      "4608549701212712522       0.00      0.00      0.00         1\n",
      "4609071190797975552       0.00      0.00      0.00         1\n",
      "4609401934404039335       0.00      0.00      0.00         1\n",
      "4609701855205785600       0.00      0.00      0.00         1\n",
      "4610518741863508930       0.00      0.00      0.00         1\n",
      "4610819491595550720       0.00      0.00      0.00         1\n",
      "4610829039307849728       0.00      0.00      0.00         1\n",
      "4610950643782950050       0.00      0.00      0.00         1\n",
      "4611195517034093936       0.00      0.00      0.00         1\n",
      "4611783668803829760       0.00      0.00      0.00         1\n",
      "4611864694418884368       0.00      0.00      0.00         1\n",
      "4611868564177337528       0.00      0.00      0.00         1\n",
      "4611903860217904720       0.00      0.00      0.00         1\n",
      "4612023499777638400       0.00      0.00      0.00         1\n",
      "4612948244891172864       0.00      0.00      0.00         1\n",
      "4613031820659785728       0.00      0.00      0.00         1\n",
      "4613191512904223549       0.00      0.00      0.00         1\n",
      "4613293969342201714       0.00      0.00      0.00         1\n",
      "4613297572826217872       0.00      0.00      0.00         1\n",
      "4613330947952082944       0.00      0.00      0.00         1\n",
      "4613537492929347584       0.00      0.00      0.00         1\n",
      "4614135812994314964       0.00      0.00      0.00         1\n",
      "4614398274170646057       0.00      0.00      0.00         1\n",
      "4614468908127092736       0.00      0.00      0.00         1\n",
      "4614683537301294321       0.00      0.00      0.00         1\n",
      "4614805504714080256       0.00      0.00      0.00         1\n",
      "4614878244280205312       0.00      0.00      0.00         1\n",
      "4615227846028165120       0.00      0.00      0.00         1\n",
      "4616016526577762304       0.00      0.00      0.00         1\n",
      "4617423347410534400       0.00      0.00      0.00         1\n",
      "4618161249636131060       0.00      0.00      0.00         1\n",
      "4618389212540960768       0.00      0.00      0.00         1\n",
      "4618581133154582528       0.00      0.00      0.00         1\n",
      "4618672720087818520       0.00      0.00      0.00         1\n",
      "4619392835769789518       0.00      0.00      0.00         1\n",
      "4619405251479339008       0.00      0.00      0.00         1\n",
      "4619886486450578784       0.00      0.00      0.00         1\n",
      "4621223817941876736       0.00      0.00      0.00         1\n",
      "4621587018609500200       0.00      0.00      0.00         1\n",
      "4622335553046577152       0.00      0.00      0.00         1\n",
      "4622714988715974774       0.00      0.00      0.00         1\n",
      "4623028881027235840       0.00      0.00      0.00         1\n",
      "4624313857932787712       0.00      0.00      0.00         1\n",
      "4624706044281487360       0.00      0.00      0.00         1\n",
      "4625705229768196096       0.00      0.00      0.00         1\n",
      "4628961218705489920       0.00      0.00      0.00         1\n",
      "4629766212578713016       0.00      0.00      0.00         1\n",
      "4631322054853918720       0.00      0.00      0.00         1\n",
      "4632260038124442282       0.00      0.00      0.00         1\n",
      "4632545553597595648       0.00      0.00      0.00         1\n",
      "4633331894469851897       0.00      0.00      0.00         1\n",
      "4636616246291333120       0.00      0.00      0.00         1\n",
      "4639803824989536256       0.00      0.00      0.00         1\n",
      "4640077599089885184       0.00      0.00      0.00         1\n",
      "4640134899322450430       0.00      0.00      0.00         1\n",
      "4640582622819385344       0.00      0.00      0.00         1\n",
      "4641129380746100736       0.00      0.00      0.00         1\n",
      "4641372488779956224       0.00      0.00      0.00         1\n",
      "4641373180269690880       0.00      0.00      0.00         1\n",
      "4643093851542650880       0.00      0.00      0.00         1\n",
      "4646176298031382528       0.00      0.00      0.00         1\n",
      "4647194111867090780       0.00      0.00      0.00         1\n",
      "4648705028746379264       0.00      0.00      0.00         1\n",
      "4649182972707078144       0.00      0.00      0.00         1\n",
      "4650211926612115456       0.00      0.00      0.00         1\n",
      "4650763607640644240       0.00      0.00      0.00         1\n",
      "\n",
      "           accuracy                           0.04      8656\n",
      "          macro avg       0.00      0.00      0.00      8656\n",
      "       weighted avg       0.05      0.04      0.04      8656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from dataset import TextDataset, EmbeddingDataset\n",
    "from embedder import Embedder\n",
    "from classifiers import assess_model, get_dataloader, DNN\n",
    "from Config.dataset_config import *\n",
    "from Config.classifiers_config import *\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"[Dataset Status]: Building datasets and dataloaders...\")\n",
    "text_dataset = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,  # TRAIN / VAL / TEST column\n",
    "    augmented_classes = [],                 # ‑‑ no aug\n",
    "    augmentation_ratio= 0,\n",
    "    undersampling_targets = {},             # ‑‑ no undersampling\n",
    ")\n",
    "\n",
    "embedding_dataset = EmbeddingDataset(\n",
    "    text_dataset=text_dataset,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method=EMBEDDING_METHOD\n",
    ")\n",
    "\n",
    "train_ds = embedding_dataset.get_subset('TRAIN')\n",
    "val_ds   = embedding_dataset.get_subset('VAL')\n",
    "test_ds  = embedding_dataset.get_subset('TEST')\n",
    "\n",
    "train_package = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_package   = get_dataloader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_package  = get_dataloader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Unpack for sklearn metrics\n",
    "_, (X_val, y_val)   = val_package\n",
    "_, (X_test, y_test) = test_package\n",
    "\n",
    "\n",
    "print(\"[Model Fit Status]: Training direct DNN...\")\n",
    "model_config = {\n",
    "        'num_epochs': 30, \n",
    "        'learning_rate': 3e-4, \n",
    "        'weight_decay': 1e-4, \n",
    "        'batch_norm': True, \n",
    "        'drop_out': 0.5, \n",
    "        'layers': [768, 512, 256, 128, 64, 3]\n",
    "    }\n",
    "dnn = DNN(model_config).to(DEVICE)\n",
    "train_loader, (X_train, y_train) = train_package\n",
    "val_loader, _                     = val_package\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(dnn.parameters(), lr=model_config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "best_f1 = -1\n",
    "best_model_path = os.path.join(CHECKPOINTS, \"best_dnn.pt\")\n",
    "\n",
    "train_losses, val_losses, val_f1_scores = [], [], []\n",
    "\n",
    "for epoch in range(model_config['num_epochs']):\n",
    "    dnn.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = dnn(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    dnn.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            preds = dnn(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            total_val_loss += loss.item() * xb.size(0)\n",
    "            val_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            val_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Macro-F1\n",
    "    val_f1 = f1_score(np.array(val_labels), np.array(val_preds), average=\"macro\", zero_division=0)\n",
    "    val_f1_scores.append(val_f1)\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(dnn.state_dict(), best_model_path)\n",
    "        print(f\"Epoch {epoch+1}/{model_config['num_epochs']}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | F1: {val_f1:.4f} | ↪️ Saved new best model.\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{model_config['num_epochs']}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n[Evaluation on Test Set]\")\n",
    "dnn.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_package[0]:\n",
    "        xb = xb.to(DEVICE)\n",
    "        preds = dnn(xb)\n",
    "        all_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "_, _ = assess_model(all_preds, test_package, valid_labels=[0, 1, 2])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
