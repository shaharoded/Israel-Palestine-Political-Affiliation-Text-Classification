{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5VNBSOv-kT"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(r'C:\\Users\\shaha\\Projects\\Python Projects\\Israel-Palestine-Political-Affiliation-Text-Classification')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD9uGUUSvuxq"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local Code\n",
    "from classifiers import *\n",
    "from dataset import EmbeddingDataset, TextDataset\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75rhsfaiwuOx"
   },
   "source": [
    "# Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper dataset\n",
    "class HelperDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(HelperDataset).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.y[item]\n",
    "    \n",
    "# Custom tqdm callback\n",
    "class TqdmCallback:\n",
    "    def __init__(self, n_trials):\n",
    "        self.pbar = tqdm(total=n_trials)\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0.  Utilities and Hyperparameters Space\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "model_hyperparameters = {\n",
    "    'logistic_regression': {\n",
    "        'num_epochs': (5, 20, 'int'),\n",
    "        'learning_rate': (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform')\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': (1e-4, 1e2, 'loguniform'),\n",
    "        'kernel': (['linear', 'rbf', 'sigmoid'], 'categorical'),\n",
    "        'degree': (2, 4, 'int'),\n",
    "        'gamma': (['scale', 'auto'], 'categorical')\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': (50, 200, 'int'),\n",
    "        'learning_rate': (1e-3, 0.2, 'loguniform'),\n",
    "        'booster': (['gbtree', 'gblinear', 'dart'], 'categorical'),\n",
    "        'max_depth': (3, 10, 'int'),\n",
    "        'min_child_weight': (2, 10, 'int'),\n",
    "        'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "        'subsample': (0.5, 1.0, 'uniform'),\n",
    "        'reg_alpha': (1e-8, 10.0, 'loguniform'),\n",
    "        'reg_lambda': (1e-8, 10.0, 'loguniform'),\n",
    "        'gamma': (1e-8, 1.0, 'loguniform')\n",
    "    },\n",
    "    'dnn': {\n",
    "        \"num_epochs\": (5, 20, 'int'),  # Adjust after trial and error\n",
    "        \"learning_rate\": (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform'),\n",
    "        \"batch_norm\": ([True, False], 'categorical'),\n",
    "        \"drop_out\": (0.0, 0.5, 'uniform'),\n",
    "        \"layers\": ([[768, 64, 3],\n",
    "                    [768, 128, 3],\n",
    "                    [768, 64, 64, 3],\n",
    "                    [768, 128, 64, 3],\n",
    "                    [768, 512, 32, 3],\n",
    "                    [768, 512, 128, 3],\n",
    "                    [768, 512, 128, 64, 3]], 'categorical')  # Layer dimensions, including an input and output layer.\n",
    "    }\n",
    "}\n",
    "\n",
    "def _build_estimator(model_name: str, params: dict) -> Classifier:\n",
    "    \"\"\"\n",
    "    Always create a **fresh** classifier for a trial.\n",
    "    \"\"\"\n",
    "    if model_name not in {\"logistic_regression\", \"svm\", \"xgboost\", \"dnn\"}:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    return Classifier(params, model_type=model_name, log=False)\n",
    "\n",
    "def macro_f1_01(y_true, y_pred_or_proba, threshold_0=0.5, threshold_1=0.5):\n",
    "    \"\"\"\n",
    "    Macro‑F1 for classes 0 & 1 (class 2 ignored).\n",
    "    Used in order to optimize the classifier towards the important classes: 0 & 1.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred_or_proba)\n",
    "\n",
    "    # probs → hard labels\n",
    "    if yp.ndim == 2:\n",
    "        if threshold_0 != 0.5 or threshold_1 != 0.5:    # custom cut‑offs\n",
    "            y_pred = np.full(len(yp), 2, dtype=int)\n",
    "            y_pred[yp[:, 0] >= threshold_0] = 0\n",
    "            y_pred[yp[:, 1] >= threshold_1] = 1\n",
    "        else:                                           # argmax\n",
    "            y_pred = yp.argmax(axis=1)\n",
    "    else:\n",
    "        y_pred = yp.astype(int)\n",
    "\n",
    "    return f1_score(y_true, y_pred, labels=[0, 1],\n",
    "                    average=\"macro\", zero_division=0)\n",
    "\n",
    "def _bootstrap_ci(y_true, y_pred, n_iter: int = 1000, alpha: float = .05):\n",
    "    \"\"\"basic percentile bootstrap CI around macro‑F1(0,1)\"\"\"\n",
    "    n       = len(y_true)\n",
    "    stats   = []\n",
    "    for _ in range(n_iter):\n",
    "        idx  = resample(np.arange(n), replace=True, n_samples=n)\n",
    "        stats.append(macro_f1_01(y_true[idx], y_pred[idx]))\n",
    "    lower, upper = np.percentile(stats, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return float(lower), float(upper)\n",
    "\n",
    "def _score_on_validation(estimator: Classifier,\n",
    "                         X_tr, y_tr, X_val, y_val, return_ci: bool = False):\n",
    "    \"\"\"\n",
    "    Fit on TRAIN → score on VAL.\n",
    "    Handles both scikit‑learn and PyTorch heads.\n",
    "    return_ci flag will return both score and confidence interval.\n",
    "    \"\"\"\n",
    "    # ‑‑‑ prepare loaders   (HelperDataset just wraps (X,y) tensors/ndarrays)\n",
    "    train_loader = DataLoader(HelperDataset(X_tr,  y_tr),\n",
    "                              batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(HelperDataset(X_val, y_val),\n",
    "                              batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # -- fit + predict ------------------------------------------------------\n",
    "    estimator.fit((train_loader, (X_tr, y_tr)))\n",
    "    preds = estimator.predict((val_loader, (X_val, y_val)))\n",
    "\n",
    "    # -- score --------------------------------------------------------------\n",
    "    score = macro_f1_01(y_val, preds)\n",
    "\n",
    "    if not return_ci:\n",
    "        return score\n",
    "\n",
    "    ci_low, ci_high = _bootstrap_ci(y_val, preds)\n",
    "    return score, ci_low, ci_high\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Optuna — hyper‑param suggestion helper\n",
    "# ------------------------------------------------------------------\n",
    "def suggest_hyperparameters(trial, hp_space):\n",
    "    \"\"\"\n",
    "    hp_space is the dict that lives in model_hyperparameters[…]\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    for key, spec in hp_space.items():\n",
    "        if len(spec) == 2 and spec[1] == \"categorical\":\n",
    "            params[key] = trial.suggest_categorical(key, spec[0])\n",
    "\n",
    "        elif len(spec) == 3:\n",
    "            low, high, kind = spec\n",
    "            if kind == \"loguniform\":\n",
    "                params[key] = trial.suggest_float(key, low, high, log=True)\n",
    "            elif kind == \"uniform\":\n",
    "                params[key] = trial.suggest_float(key, low, high)\n",
    "            elif kind == \"int\":\n",
    "                params[key] = trial.suggest_int(key, low, high)\n",
    "            elif kind == \"categorical\":\n",
    "                params[key] = trial.suggest_categorical(key, low)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown param type: {kind}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Malformed spec for {key}: {spec}\")\n",
    "    return params\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Optuna objective  (no CV — uses VAL split)\n",
    "# ------------------------------------------------------------------\n",
    "def objective(trial, model_name: str, X_train, y_train, X_val,   y_val) -> float:\n",
    "\n",
    "    # -------- suggest hyper‑parameters\n",
    "    params = suggest_hyperparameters(trial,\n",
    "                                     model_hyperparameters[model_name])\n",
    "\n",
    "    # logistic_regression  ≡  one‑layer DNN, override a few things\n",
    "    if model_name == \"logistic_regression\":\n",
    "        params.update(batch_norm=False, drop_out=0.0, layers=[768, 3])\n",
    "\n",
    "    # booster‑specific extras for XGBoost\n",
    "    if model_name == \"xgboost\" and params[\"booster\"] in {\"gbtree\", \"dart\"}:\n",
    "        params.update(\n",
    "            grow_policy=trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            gamma=trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            min_child_weight=trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        )\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params.update(\n",
    "                sample_type   = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"]),\n",
    "                normalize_type= trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"]),\n",
    "                rate_drop     = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True),\n",
    "                skip_drop     = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "            )\n",
    "\n",
    "    # -------- build, fit, score\n",
    "    estimator = _build_estimator(model_name, params)\n",
    "    eval_score = _score_on_validation(estimator, X_train, y_train, X_val, y_val, return_ci=False)\n",
    "    return eval_score\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Optimiser wrapper  (no CV, keeps history)\n",
    "# ------------------------------------------------------------------\n",
    "def optimize_model(model_name: str, X_train, y_train, X_val,   y_val, n_trials: int = 50, timeout: int  = 36000):\n",
    "    \"\"\"\n",
    "    Tune hyper‑parameters by maximising macro‑F1 (0+1 classes) on VAL set.\n",
    "    Returns: best_params, best_score, all_trial_scores\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "\n",
    "    def _optuna_obj(trial):\n",
    "        score = objective(trial, model_name, X_train, y_train, X_val,   y_val)\n",
    "        all_scores.append(score)\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(_optuna_obj, n_trials=n_trials, timeout=timeout, callbacks=[TqdmCallback(n_trials)], n_jobs=-1) # Uses all available cores\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score  = study.best_value\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # refit *best* on full TRAIN and compute bootstrap CI --------------\n",
    "    # ------------------------------------------------------------------\n",
    "    best_est   = _build_estimator(model_name, best_params)\n",
    "\n",
    "    best_score, ci_low, ci_high = _score_on_validation(\n",
    "            best_est, X_train, y_train, X_val, y_val, return_ci=True)\n",
    "    \n",
    "    print(f\"[{model_name}]  best F1={best_score:.4f}   \"\n",
    "            f\"Best hyperparameters: {best_params}\",\n",
    "            f\"bootstrap 95% CI=({ci_low:.4f}, {ci_high:.4f})\")\n",
    "\n",
    "    return best_params, best_score, (ci_low, ci_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Optimize models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ekXAYOkw0w_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create 6 different EmbeddingDataset datasets out of the 3 TextDataset datasets:\n",
    "augmented dataset - with distilbert embedding or tfidf, \n",
    "undersampled balanced dataset - with distilbert embedding or tfidf,\n",
    "regular dataset - with distilbert embedding or tfidf\n",
    "'''\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1.  Build the three text‑level datasets\n",
    "# ---------------------------------------------\n",
    "txt_regular = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,  # TRAIN / VAL / TEST column\n",
    "    augmented_classes = [],                 # ‑‑ no aug\n",
    "    augmentation_ratio= 0,\n",
    "    undersampling_targets = {},             # ‑‑ no undersampling\n",
    ")\n",
    "\n",
    "txt_undersampled = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,\n",
    "    augmented_classes = [],\n",
    "    augmentation_ratio= 0,\n",
    "    undersampling_targets = UNDERSAMPLING_TARGETS,   # e.g. {\"Undefined\":8000, …}\n",
    ")\n",
    "\n",
    "txt_augmented = TextDataset(\n",
    "    csv_path          = DATA_PATH,\n",
    "    id_column_idx     = ID_COLUMN_IDX,\n",
    "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
    "    label_column_idx  = LABEL_COLUMN_IDX,\n",
    "    split_column_idx  = SUBSET_COLUMN_IDX,\n",
    "    augmented_classes = AUGMENTED_CLASSES,           # e.g. [\"Pro‑Israel\"]\n",
    "    augmentation_ratio= 3,                            # add 3 extra copies\n",
    "    undersampling_targets = {},                       # keep original class sizes\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2.  Wrap each text set in 1‑EmbeddingDataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "EMB_METHODS = [\"distilbert\", \"tf‑idf\"]    # add more if you wish\n",
    "embedder    = Embedder()                  # your wrapper around HF / TF‑IDF\n",
    "\n",
    "embedding_sets: dict[str, dict[str, EmbeddingDataset]] = {}   # {method : {variant : EmbeddingDataset}}\n",
    "\n",
    "for method in EMB_METHODS:\n",
    "    embedding_sets[method] = {\n",
    "        \"regular\"      : EmbeddingDataset(txt_regular,      embedder, method),\n",
    "        \"undersampled\" : EmbeddingDataset(txt_undersampled, embedder, method),\n",
    "        \"augmented\"    : EmbeddingDataset(txt_augmented,    embedder, method),\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3.  Fetch TRAIN and VAL tensors for each dataset\n",
    "# ---------------------------------------------\n",
    "(X_bert_reg_tr ,  y_bert_reg_tr ,\n",
    " X_bert_reg_val,  y_bert_reg_val) = (\n",
    "     *embedding_sets[\"distilbert\"][\"regular\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"distilbert\"][\"regular\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_bert_us_tr ,   y_bert_us_tr ,\n",
    " X_bert_us_val,   y_bert_us_val)  = (\n",
    "     *embedding_sets[\"distilbert\"][\"undersampled\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"distilbert\"][\"undersampled\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_bert_aug_tr ,  y_bert_aug_tr ,\n",
    " X_bert_aug_val,  y_bert_aug_val) = (\n",
    "     *embedding_sets[\"distilbert\"][\"augmented\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"distilbert\"][\"augmented\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_tfidf_reg_tr , y_tfidf_reg_tr ,\n",
    " X_tfidf_reg_val, y_tfidf_reg_val) = (\n",
    "     *embedding_sets[\"tf‑idf\"][\"regular\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"tf‑idf\"][\"regular\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_tfidf_us_tr ,  y_tfidf_us_tr ,\n",
    " X_tfidf_us_val,  y_tfidf_us_val)  = (\n",
    "     *embedding_sets[\"tf‑idf\"][\"undersampled\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"tf‑idf\"][\"undersampled\"].get_subset(\"VAL\")[:] )\n",
    "\n",
    "(X_tfidf_aug_tr , y_tfidf_aug_tr ,\n",
    " X_tfidf_aug_val, y_tfidf_aug_val) = (\n",
    "     *embedding_sets[\"tf‑idf\"][\"augmented\"].get_subset(\"TRAIN\")[:],\n",
    "     *embedding_sets[\"tf‑idf\"][\"augmented\"].get_subset(\"VAL\")[:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "lr_results['bert_without_augmentation'] = optimize_model(\n",
    "    model_name = 'logistic_regression',\n",
    "    X_train    = X_bert_reg_tr,\n",
    "    y_train    = y_bert_reg_tr,\n",
    "    X_val      = X_bert_reg_val,\n",
    "    y_val      = y_bert_reg_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['bert_with_undersampling'] = optimize_model('logistic_regression', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['bert_with_augmentation'] = optimize_model('logistic_regression', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['tfidf_without_augmentation'] = optimize_model('logistic_regression', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['tfidf_with_undersampling'] = optimize_model('logistic_regression', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_results['tfidf_with_augmentation'] = optimize_model('logistic_regression', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'Margin': value[2],\n",
    "        'STD': value[3],\n",
    "        'Scores': value[4]\n",
    "    }\n",
    "    for key, value in lr_results.items()\n",
    "])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_results = {}\n",
    "svm_results['bert_without_augmentation'] = optimize_model('svm', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['bert_with_undersampling'] = optimize_model('svm', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['bert_with_augmentation'] = optimize_model('svm', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['tfidf_without_augmentation'] = optimize_model('svm', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['tfidf_with_undersampling'] = optimize_model('svm', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dvg-ZLIiyunH"
   },
   "outputs": [],
   "source": [
    "svm_results['tfidf_with_augmentation'] = optimize_model('svm', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"SVM results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'STD': value[2],\n",
    "        'Scores': value[3]\n",
    "    }\n",
    "    for key, value in svm_results.items()\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_results = {}\n",
    "xgb_results['bert_without_augmentation'] = optimize_model('xgboost', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['bert_with_undersampling'] = optimize_model('xgboost', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['bert_with_augmentation'] = optimize_model('xgboost', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['tfidf_without_augmentation'] = optimize_model('xgboost', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['tfidf_with_undersampling'] = optimize_model('xgboost', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ze5Iu-hQyvkX"
   },
   "outputs": [],
   "source": [
    "xgb_results['tfidf_with_augmentation'] = optimize_model('xgboost', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"XGBoost results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'STD': value[2],\n",
    "        'Scores': value[3]\n",
    "    }\n",
    "    for key, value in xgb_results.items()\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results = {}\n",
    "dnn_results['bert_without_augmentation'] = optimize_model('dnn', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['bert_with_undersampling'] = optimize_model('dnn', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['bert_with_augmentation'] = optimize_model('dnn', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['tfidf_without_augmentation'] = optimize_model('dnn', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['tfidf_with_undersampling'] = optimize_model('dnn', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn_results['tfidf_with_augmentation'] = optimize_model('dnn', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"DNN results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'STD': value[2],\n",
    "        'Scores': value[3]\n",
    "    }\n",
    "    for key, value in dnn_results.items()\n",
    "])\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
