{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi5VNBSOv-kT"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(r'C:\\Users\\shaha\\Projects\\Python Projects\\Israel-Palestine-Political-Affiliation-Text-Classification')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD9uGUUSvuxq"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from classifiers import *\n",
    "from dataset import EmbeddingDataset\n",
    "from embedder import Embedder\n",
    "from Config.dataset_config import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75rhsfaiwuOx"
   },
   "source": [
    "# Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper dataset\n",
    "class HelperDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(HelperDataset).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.y[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom tqdm callback\n",
    "class TqdmCallback:\n",
    "    def __init__(self, n_trials):\n",
    "        self.pbar = tqdm(total=n_trials)\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_hyperparameters = {\n",
    "    'logistic_regression': {\n",
    "        'num_epochs': (5, 20, 'int'),\n",
    "        'learning_rate': (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform')\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': (1e-4, 1e2, 'loguniform'),\n",
    "        'kernel': (['linear', 'rbf', 'sigmoid'], 'categorical'),\n",
    "        'degree': (2, 4, 'int'),\n",
    "        'gamma': (['scale', 'auto'], 'categorical')\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': (50, 200, 'int'),\n",
    "        'learning_rate': (1e-3, 0.2, 'loguniform'),\n",
    "        'booster': (['gbtree', 'gblinear', 'dart'], 'categorical'),\n",
    "        'max_depth': (3, 10, 'int'),\n",
    "        'min_child_weight': (2, 10, 'int'),\n",
    "        'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "        'subsample': (0.5, 1.0, 'uniform'),\n",
    "        'reg_alpha': (1e-8, 10.0, 'loguniform'),\n",
    "        'reg_lambda': (1e-8, 10.0, 'loguniform'),\n",
    "        'gamma': (1e-8, 1.0, 'loguniform')\n",
    "    },\n",
    "    'dnn': {\n",
    "        \"num_epochs\": (5, 20, 'int'),  # Adjust after trial and error\n",
    "        \"learning_rate\": (1e-5, 1e-3, 'loguniform'),\n",
    "        'weight_decay': (1e-5, 1e-3, 'loguniform'),\n",
    "        \"batch_norm\": ([True, False], 'categorical'),\n",
    "        \"drop_out\": (0.0, 0.5, 'uniform'),\n",
    "        \"layers\": ([[768, 64, 3],\n",
    "                    [768, 128, 3],\n",
    "                    [768, 64, 64, 3],\n",
    "                    [768, 128, 64, 3],\n",
    "                    [768, 512, 32, 3],\n",
    "                    [768, 512, 128, 3],\n",
    "                    [768, 512, 128, 64, 3]], 'categorical')  # Layer dimensions, including an input and output layer.\n",
    "    }\n",
    "}\n",
    "\n",
    "def suggest_hyperparameters(trial, hyperparams):\n",
    "    params = {}\n",
    "    for key, value in hyperparams.items():\n",
    "        if len(value) == 2 and value[1] == 'categorical':\n",
    "            params[key] = trial.suggest_categorical(key, value[0])\n",
    "        elif len(value) == 3:\n",
    "            if value[2] == 'loguniform':\n",
    "                params[key] = trial.suggest_float(key, value[0], value[1], log=True)\n",
    "            elif value[2] == 'uniform':\n",
    "                params[key] = trial.suggest_float(key, value[0], value[1])\n",
    "            elif value[2] == 'int':\n",
    "                params[key] = trial.suggest_int(key, value[0], value[1])\n",
    "            elif value[2] == 'categorical':\n",
    "                params[key] = trial.suggest_categorical(key, value[0])\n",
    "        else:\n",
    "            raise ValueError(f\"Hyperparameter tuple for {key} is not in the expected format: {value}\")\n",
    "    return params\n",
    "\n",
    "def cross_validation(estimator, X, y, n_splits=5):\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for i, (train_index, val_index) in enumerate(cv.split(X, y)):\n",
    "        # Split to train and validation sets\n",
    "        x_train, x_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        help_train_dataset = HelperDataset(x_train, y_train)\n",
    "        help_val_dataset = HelperDataset(x_val, y_val)\n",
    "\n",
    "        train_dataloader = DataLoader(help_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_dataloader = DataLoader(help_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # Fit to the Classifier train and predict data type\n",
    "        train = (train_dataloader, (x_train, y_train))\n",
    "        val = (val_dataloader, (x_val, y_val))\n",
    "\n",
    "        estimator.fit(train)\n",
    "        pred = estimator.predict(val)\n",
    "        score = f1_score(y_val, pred, average='micro')\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Define objective function for optuna. The function include all models, and should be called with the model name. The function optimize the Classifier class hyperparameters.\n",
    "def objective(trial, model_name, X, y, folds_scores):\n",
    "        # Use suggest_hyperparameters to handle standard parameters\n",
    "    params = suggest_hyperparameters(trial, model_hyperparameters[model_name])\n",
    "\n",
    "    # Add unique overrides for specific models\n",
    "    if model_name == 'logistic_regression':\n",
    "        params['batch_norm'] = False\n",
    "        params['drop_out'] = 0.0\n",
    "        params['layers'] = [768, 3]\n",
    "\n",
    "    # Add unique parameters for XGBoost (booster-specific) based on booster\n",
    "    if model_name == 'xgboost':\n",
    "        if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "            params[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "            params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "\n",
    "    model = Classifier(params, model_type=model_name, log=False)\n",
    "\n",
    "    # Perform cross validation\n",
    "    scores = cross_validation(model, X, y)\n",
    "\n",
    "    folds_scores.append(scores)     # Save scores for statistic tests\n",
    "    return np.mean(scores)\n",
    "\n",
    "def optimize_model(model_name, X, y, n_trials=10, timout=36000):\n",
    "    \"\"\"\n",
    "    The actual optimization.\n",
    "    \"\"\"\n",
    "    folds_scores = []   # create a list to store the scores from each trial folds\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    progress_bar = TqdmCallback(n_trials)\n",
    "    study.optimize(lambda trial: objective(trial, model_name, X, y, folds_scores), n_trials=n_trials, timeout=timout, callbacks=[progress_bar])\n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    attempt_stats = [(np.mean(scores), np.std(scores, ddof=1), scores) for scores in folds_scores]\n",
    "    best_attempt = sorted(attempt_stats, key=lambda x: (-x[0], x[1]))[0]\n",
    "\n",
    "    n = len(best_attempt[2])\n",
    "    z = 1.96  # For 95% confidence interval\n",
    "\n",
    "    # Calculate margin of error\n",
    "    margin_of_error = z * (best_attempt[1] / np.sqrt(n))\n",
    "    print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best F1 score for {model_name}: {best_value}\")\n",
    "    print(f\"Attempts Stats: Avg: {best_attempt[0]}, Margin: +-{margin_of_error}, STD: {best_attempt[1]}, Scores: {best_attempt[2]}\")\n",
    "    \n",
    "\n",
    "    return best_params, best_value, margin_of_error, best_attempt[1], best_attempt[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Optimize models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ekXAYOkw0w_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create 6 different datasets:\n",
    "augmented dataset - with distilbert embedding or tfidf, \n",
    "undersampled balanced dataset - with distilbert embedding or tfidf,\n",
    "regular dataset - with distilbert embedding or tfidf\n",
    "'''\n",
    "bert_embedding_no_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets={},\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "bert_embedding_undersampled_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets=UNDERSAMPLING_TARGETS,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "bert_embedding_with_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=3,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets={},\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='distilbert'\n",
    ")\n",
    "\n",
    "tfidf_embedding_no_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets={},\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='tf-idf'\n",
    ")\n",
    "\n",
    "tfidf_embedding_undersampled_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=0,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets=UNDERSAMPLING_TARGETS,\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='tf-idf'\n",
    ")\n",
    "\n",
    "tfidf_embedding_with_augmentation_data = EmbeddingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    subset=SUBSET,\n",
    "    id_column_idx=ID_COLUMN_IDX,\n",
    "    comment_column_idx=COMMENT_COLUMN_IDX,\n",
    "    label_column_idx=LABEL_COLUMN_IDX,\n",
    "    subset_column_idx=SUBSET_COLUMN_IDX,\n",
    "    augmented_classes=AUGMENTED_CLASSES,\n",
    "    augmentation_ratio=3,\n",
    "    augmentation_methods=AUGMENTATION_METHODS,\n",
    "    adversation_ratio = ADVERSATION_RATIO,\n",
    "    undersampling_targets={},\n",
    "    embedder=Embedder(),\n",
    "    embedding_method='tf-idf'\n",
    ")\n",
    "\n",
    "# Get X,y\n",
    "X_bert_no_augmentation, y_bert_no_augmentation = bert_embedding_no_augmentation_data.embeddings, bert_embedding_no_augmentation_data.labels\n",
    "X_bert_undersampled, y_bert_undersampled = bert_embedding_undersampled_data.embeddings, bert_embedding_undersampled_data.labels\n",
    "X_bert_with_augmentation, y_bert_with_augmentation = bert_embedding_with_augmentation_data.embeddings, bert_embedding_with_augmentation_data.labels\n",
    "X_tfidf_no_augmentation, y_tfidf_no_augmentation = tfidf_embedding_no_augmentation_data.embeddings, tfidf_embedding_no_augmentation_data.labels\n",
    "X_tfidf_undersampled, y_tfidf_undersampled = tfidf_embedding_undersampled_data.embeddings, tfidf_embedding_undersampled_data.labels\n",
    "X_tfidf_with_augmentation, y_tfidf_with_augmentation = tfidf_embedding_with_augmentation_data.embeddings, tfidf_embedding_with_augmentation_data.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "lr_results['bert_without_augmentation'] = optimize_model('logistic_regression', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['bert_with_undersampling'] = optimize_model('logistic_regression', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['bert_with_augmentation'] = optimize_model('logistic_regression', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['tfidf_without_augmentation'] = optimize_model('logistic_regression', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results['tfidf_with_undersampling'] = optimize_model('logistic_regression', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_results['tfidf_with_augmentation'] = optimize_model('logistic_regression', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'Margin': value[2],\n",
    "        'STD': value[3],\n",
    "        'Scores': value[4]\n",
    "    }\n",
    "    for key, value in lr_results.items()\n",
    "])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_results = {}\n",
    "svm_results['bert_without_augmentation'] = optimize_model('svm', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['bert_with_undersampling'] = optimize_model('svm', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['bert_with_augmentation'] = optimize_model('svm', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['tfidf_without_augmentation'] = optimize_model('svm', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results['tfidf_with_undersampling'] = optimize_model('svm', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dvg-ZLIiyunH"
   },
   "outputs": [],
   "source": [
    "svm_results['tfidf_with_augmentation'] = optimize_model('svm', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"SVM results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'STD': value[2],\n",
    "        'Scores': value[3]\n",
    "    }\n",
    "    for key, value in svm_results.items()\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_results = {}\n",
    "xgb_results['bert_without_augmentation'] = optimize_model('xgboost', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['bert_with_undersampling'] = optimize_model('xgboost', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['bert_with_augmentation'] = optimize_model('xgboost', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['tfidf_without_augmentation'] = optimize_model('xgboost', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results['tfidf_with_undersampling'] = optimize_model('xgboost', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ze5Iu-hQyvkX"
   },
   "outputs": [],
   "source": [
    "xgb_results['tfidf_with_augmentation'] = optimize_model('xgboost', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"XGBoost results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'STD': value[2],\n",
    "        'Scores': value[3]\n",
    "    }\n",
    "    for key, value in xgb_results.items()\n",
    "])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results = {}\n",
    "dnn_results['bert_without_augmentation'] = optimize_model('dnn', X_bert_no_augmentation, y_bert_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['bert_with_undersampling'] = optimize_model('dnn', X_bert_undersampled, y_bert_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['bert_with_augmentation'] = optimize_model('dnn', X_bert_with_augmentation, y_bert_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['tfidf_without_augmentation'] = optimize_model('dnn', X_tfidf_no_augmentation, y_tfidf_no_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_results['tfidf_with_undersampling'] = optimize_model('dnn', X_tfidf_undersampled, y_tfidf_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn_results['tfidf_with_augmentation'] = optimize_model('dnn', X_tfidf_with_augmentation, y_tfidf_with_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"DNN results:\\n\\n\")\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': key,\n",
    "        'Best Parameters': value[0],\n",
    "        'Best Avg': value[1],\n",
    "        'STD': value[2],\n",
    "        'Scores': value[3]\n",
    "    }\n",
    "    for key, value in dnn_results.items()\n",
    "])\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
