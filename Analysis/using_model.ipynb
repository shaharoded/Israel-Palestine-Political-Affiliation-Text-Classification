{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#   Intro\n",
        "\n",
        "This notebook is built to use GPU in order to apply the stance classification on larger scales datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxpcg34F3Jiy"
      },
      "source": [
        "# Part 1: Create Embedding Dataset\n",
        "\n",
        "A flow that connects to Google Drive in order to run the embedder and dataset class with GPU on larger scales of data.\n",
        "\n",
        "Output is a merged .pkl file saved in your mounted Drive for the Embedding Dataset Object.\n",
        "\n",
        "Be sure to upload the trained embedding resources to this local env before running the flow:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J0Z4ByXz-rP"
      },
      "source": [
        "NOTE: The code in this notebook is an adapted replica of the code from the main project files:\n",
        "- dataset.py\n",
        "- embedder.py\n",
        "- classifiers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbPOlvfNwSER",
        "outputId": "91f8f0e1-cc02-4fd9-9337-2b99fe008ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTWCEwNMyC9O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Data & Embeddings\n",
        "# Get the project root directory (move up one level from the 'Config' folder)\n",
        "project_root = \"/content\"\n",
        "\n",
        "# Base directories (relative to project root)\n",
        "EMBEDDING_DIR_PATH = os.path.join(project_root, \"Embedding\")\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_DIR_PATH, \"distilbert-finetuned\")  # Full path to the embedding directory\n",
        "TFIDF_PATH = os.path.join(EMBEDDING_DIR_PATH, \"tfidf\", \"tfidf_vectorizer.pkl\")  # Full path to the TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrcSHtOtuxhU"
      },
      "outputs": [],
      "source": [
        "# embedder.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import joblib\n",
        "import warnings\n",
        "from sklearn.exceptions import InconsistentVersionWarning\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Embedder object with a DistilBERT model and a TF-IDF vectorizer.,\n",
        "        both loaded from pretrained files.\n",
        "        \"\"\"\n",
        "        # Set device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Load the DistilBERT tokenizer and model\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(EMBEDDING_PATH)\n",
        "        self.distilbert_model = DistilBertModel.from_pretrained(EMBEDDING_PATH).to(self.device)\n",
        "\n",
        "        # Load the TF-IDF vectorizer\n",
        "        self.tfidf_vectorizer = joblib.load(TFIDF_PATH)\n",
        "\n",
        "\n",
        "    def embed(self, input_text, method=\"distilbert\"):\n",
        "        \"\"\"\n",
        "        Generate embeddings for the input text using the specified method.\n",
        "        Generate embeddings by sentence chunking + mean pooling per sentence.\n",
        "        Longer comments benefit from dividing into smaller semantic units.\n",
        "\n",
        "        Parameters:\n",
        "        - input_text (str): The text to be embedded.\n",
        "        - method (str): The embedding method (\"distilbert\" or \"tf-idf\").\n",
        "\n",
        "        Returns:\n",
        "        - embedding (np.ndarray): The embedding for the full input text.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(input_text)\n",
        "        sentence_embeddings = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            if method == \"distilbert\":\n",
        "                sent_emb = self._distilbert_embedding(sent)\n",
        "            elif method == \"tf-idf\":\n",
        "                sent_emb = self._tfidf_embedding(sent)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported embedding method. Use 'distilbert' or 'tf-idf'.\")\n",
        "            sentence_embeddings.append(sent_emb)\n",
        "\n",
        "        # Mean-pool sentence-level embeddings\n",
        "        if len(sentence_embeddings) == 0:\n",
        "            return np.zeros_like(self._distilbert_embedding(\"\"))  # fallback for empty input\n",
        "\n",
        "        stacked = np.stack(sentence_embeddings)  # shape: (n_sentences, emb_dim)\n",
        "        mean_embedding = stacked.mean(axis=0)\n",
        "        return mean_embedding\n",
        "\n",
        "\n",
        "    def _distilbert_embedding(self, input_text):\n",
        "        \"\"\"\n",
        "        Generate embeddings using the DistilBERT model.\n",
        "\n",
        "        Parameters:\n",
        "        - input_text (str): The text to be embedded.\n",
        "\n",
        "        Returns:\n",
        "        - embedding (torch.Tensor): A mean pooling vector from the model over the entire input -> Embedding for the input text (comment).\n",
        "        \"\"\"\n",
        "        # Tokenize and encode the input text\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Pass the input through the DistilBERT model\n",
        "            outputs = self.distilbert_model(**inputs)\n",
        "            last_hidden = outputs.last_hidden_state # shape: (1, seq_len, hidden_size)\n",
        "            attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "            # Mean pooling over all non-padding tokens\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden * mask_expanded, dim=1)\n",
        "            sum_mask = mask_expanded.sum(dim=1)\n",
        "\n",
        "            mean_pooled = sum_embeddings / sum_mask\n",
        "            return mean_pooled.cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "    def _tfidf_embedding(self, input_text):\n",
        "        \"\"\"\n",
        "        Generate embeddings using the TF-IDF model.\n",
        "\n",
        "        Parameters:\n",
        "        - input_text (str): The text to be embedded.\n",
        "\n",
        "        Returns:\n",
        "        - embedding (np.ndarray): The embedding for the input text.\n",
        "        \"\"\"\n",
        "        # Transform the input text using the TF-IDF vectorizer\n",
        "        embedding = self.tfidf_vectorizer.transform([input_text])\n",
        "        return embedding.toarray().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNyZbmUmuxVw",
        "outputId": "be98d1e3-ee42-47ac-9f76-a07669c6cb16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# dataset.py\n",
        "\n",
        "from logging import exception\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import random\n",
        "import re\n",
        "import contractions\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "LABELS_ENCODER = {\n",
        "    \"Pro-Palestine\": 0,\n",
        "    \"Pro-Israel\": 1,\n",
        "    \"Undefined\": 2\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 0.  Tiny CSV cache (load once, reuse for every split)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "class _CSVCache:\n",
        "    '''\n",
        "    Use to avoid re-loading a CSV file multiple times\n",
        "    '''\n",
        "    df = None                 # class attribute\n",
        "\n",
        "def _load_csv(path, encoding='utf-8'):\n",
        "    if _CSVCache.df is None:             # load once, reuse for all splits\n",
        "        try:\n",
        "            _CSVCache.df = pd.read_csv(path, encoding=encoding)\n",
        "        except UnicodeDecodeError:\n",
        "            _CSVCache.df = pd.read_csv(path, encoding='ISO‑8859‑1')\n",
        "    return _CSVCache.df\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "#  Text Augmentation methods, applied in TextDataset\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "class TextAugmenter:\n",
        "    def __init__(self, adversation_ratio=0.1, methods=None):\n",
        "        self.adversation_ratio = adversation_ratio\n",
        "        self.methods = methods or ['wordnet']\n",
        "\n",
        "    def random_deletion(self, sentence):\n",
        "        '''\n",
        "        Randomally delete words from the sentence\n",
        "        '''\n",
        "        words = word_tokenize(sentence)\n",
        "        return \" \".join([word for word in words if random.random() > self.adversation_ratio])\n",
        "\n",
        "    def random_swap(self, sentence):\n",
        "        '''\n",
        "        Randomally swap 2 words of the sentence\n",
        "        '''\n",
        "        words = word_tokenize(sentence)\n",
        "        for _ in range(int(len(words) * self.adversation_ratio)):\n",
        "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def get_wordnet_synonyms(self, word, pos=None):\n",
        "        '''\n",
        "        Randomally replace a word with a synonim\n",
        "        '''\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word, pos=pos):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name().replace('_', ' '))\n",
        "        synonyms.discard(word)\n",
        "        return list(synonyms)\n",
        "\n",
        "    def get_wordnet_pos(self, treebank_tag):\n",
        "        '''\n",
        "        Use POS to choose the word to replace\n",
        "        '''\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def synonym_replacement(self, sentence):\n",
        "        '''\n",
        "        Randomally replace a word with a synonim\n",
        "        '''\n",
        "        words = word_tokenize(sentence)\n",
        "        pos_tags = pos_tag(words)\n",
        "        augmented_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            if word.lower() in STOPWORDS:\n",
        "                augmented_words.append(word)\n",
        "                continue\n",
        "            wordnet_pos = self.get_wordnet_pos(pos)\n",
        "            if wordnet_pos and random.random() < self.adversation_ratio:\n",
        "                synonyms = self.get_wordnet_synonyms(word, pos=wordnet_pos)\n",
        "                if synonyms:\n",
        "                    augmented_words.append(random.choice(synonyms))\n",
        "                else:\n",
        "                    augmented_words.append(word)\n",
        "            else:\n",
        "                augmented_words.append(word)\n",
        "        return \" \".join(augmented_words)\n",
        "\n",
        "    def augment_comment(self, comment):\n",
        "        method = random.choice(self.methods)\n",
        "        if method == 'deletion':\n",
        "            return self.random_deletion(comment)\n",
        "        elif method == 'swap':\n",
        "            return self.random_swap(comment)\n",
        "        elif method == 'wordnet':\n",
        "            return self.synonym_replacement(comment)\n",
        "        else:\n",
        "            return comment  # Fallback\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Hold ALL rows (TRAIN / VAL / TEST) internally.\n",
        "    Augmentation / undersampling are applied **only** to the TRAIN rows.\n",
        "    Call get_subset('TRAIN'|'VAL'|'TEST') to obtain a view that behaves\n",
        "    like a normal torch Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path, id_column_idx, comment_column_idx, label_column_idx, split_column_idx,\n",
        "                 augmented_classes=[], augmentation_ratio=3, augmentation_methods = ['wordnet'], adversation_ratio=0.1, undersampling_targets={}):\n",
        "        \"\"\"\n",
        "        Initiates the dataset, which is the base Dataset for the embeddings.\n",
        "\n",
        "        Args:\n",
        "            csv_path (str): The path to the .csv data file.\n",
        "            id_column_idx (int): Idx for the ID column in the dataframe.\n",
        "            comment_column_idx (int): Idx for the text column in the dataframe.\n",
        "            label_column_idx (int): Idx for the label column in the dataframe.\n",
        "            split_column_idx (int): Idx for the subset column in the dataframe, indicating how to split it.\n",
        "            augmented_classes (list): A list of the classes to augment. Chose from ['Pro-Israel', 'Pro-Palestine', 'Undefined']\n",
        "            augmentation_ratio (int): Increase in the comments number. Meaning -> 1 comments turns to 1 + AUGMENTATION_RATIO comments.\n",
        "            augmentation_methods (list): Choose from ['deletion', 'swap', 'wordnet'].\n",
        "            adversation_ratio (float): Replacement ratio within the comment.\n",
        "            undersampling_targets (dict): A mapping object of how much to undersample each class.\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.id_column_idx = id_column_idx\n",
        "        self.comment_column_idx = comment_column_idx\n",
        "        self.label_column_idx = label_column_idx\n",
        "        self.split_column_idx = split_column_idx\n",
        "        self.action = 'regular'\n",
        "\n",
        "        # ------------ load --------------------------------------------------\n",
        "        df = _load_csv(csv_path)\n",
        "\n",
        "        # ------------ basic text cleaning ------------------------------------------\n",
        "        df = self.__preprocess(df)\n",
        "\n",
        "        # ------------ TRAIN‑only operations ----------------------------------------\n",
        "        mask_train = df.iloc[:, split_column_idx] == \"TRAIN\"\n",
        "\n",
        "        if undersampling_targets:\n",
        "            df.loc[mask_train] = self._undersample(\n",
        "                df.loc[mask_train], undersampling_targets)\n",
        "            self.action = 'undersampled'\n",
        "\n",
        "        if (augmented_classes and augmentation_ratio > 0\n",
        "                and adversation_ratio > 0):\n",
        "            df = self._augment(df, mask_train,\n",
        "                             augmented_classes, augmentation_ratio,\n",
        "                             augmentation_methods, adversation_ratio)\n",
        "            self.action = 'augmented'\n",
        "\n",
        "        df.reset_index(drop=True, inplace=True)  # keep indices clean\n",
        "\n",
        "        # Encode labels if not yet encoded\n",
        "        label_col = df.columns[self.label_column_idx]\n",
        "\n",
        "        # Handle string labels: map to integers via LABELS_ENCODER\n",
        "        if df[label_col].dtype == object:\n",
        "            mapped = df[label_col].map(LABELS_ENCODER)\n",
        "            unknowns = df[~df[label_col].isin(LABELS_ENCODER)]\n",
        "            if not unknowns.empty:\n",
        "                raise ValueError(f\"[Label Error] Found unknown string labels: {unknowns[label_col].unique().tolist()}\")\n",
        "            df[label_col] = mapped\n",
        "        # Handle numeric labels: check that they’re valid\n",
        "        else:\n",
        "            invalid_labels = df.loc[~df[label_col].isin(LABELS_ENCODER.values()), label_col].unique()\n",
        "            if len(invalid_labels) > 0:\n",
        "                raise ValueError(f\"[Label Error] Found invalid numeric labels: {invalid_labels}\")\n",
        "\n",
        "        # Save df as class attr\n",
        "        self.data = df\n",
        "\n",
        "        # pre‑compute row indices per split for fast lookup\n",
        "        self.idx_split = {\n",
        "            s: np.flatnonzero(df.iloc[:, split_column_idx] == s)\n",
        "            for s in (\"TRAIN\", \"VAL\", \"TEST\")\n",
        "        }\n",
        "\n",
        "        print(f\"[TextDataset] rows: \"\n",
        "              f\"train={len(self.idx_split['TRAIN'])}, \"\n",
        "              f\"val={len(self.idx_split['VAL'])}, \"\n",
        "              f\"test={len(self.idx_split['TEST'])}\")\n",
        "\n",
        "    # ------------------------------------------------------------------ helpers ---\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Perform basic text normalization:\n",
        "        - Replace quotes with a placeholder (to mark them)\n",
        "        - Replace URLs with <URL>.\n",
        "        - Replace user mentions with <USER>.\n",
        "        - Clean hashtags, retaining the word only.\n",
        "        - Remove irrelevant characters (e.g., special symbols, emojis).\n",
        "        - Normalize whitespace.\n",
        "        \"\"\"\n",
        "        replacements = {\n",
        "            '“': '\"', '”': '\"', '‘': \"'\", '’': \"'\",\n",
        "            'â\\x80\\x9c': '\"', 'â\\x80\\x9d': '\"', 'â\\x80\\x99': \"'\", '&#x200B;': ' ',\n",
        "            '&amp;': '&', '&lt;': '<', '&gt;': '>'\n",
        "        }\n",
        "\n",
        "        for bad_char, good_char in replacements.items():\n",
        "            text = text.replace(bad_char, good_char)\n",
        "\n",
        "        text = re.sub(r'(^|\\n)\\s*(?:>|\\&gt;).*?(?=\\n|$)', '<QUOTE>', text) # Any line that starts with > or &gt; is a quoted parent text\n",
        "        try:\n",
        "          text = contractions.fix(text)  # Expand contractions\n",
        "        except Exception:\n",
        "          text = text\n",
        "        text = \" \".join(text.split())  # Normalize whitespace\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)  # Replace URLs\n",
        "        text = re.sub(r'@\\w+', '<USER>', text)  # Replace user mentions\n",
        "        text = re.sub(r'#(\\w+)', r'\\1', text)  # Clean hashtags, retain the word\n",
        "        text = re.sub(r'[^\\w\\s.,!?\\'\"\\{\\(\\[\\-\\\\/:;]', '', text)  # Remove irrelevant characters\n",
        "        return text\n",
        "\n",
        "    def __preprocess(self, df):\n",
        "        \"\"\"\n",
        "        Apply text preprocessing to the comment column with a progress bar.\n",
        "        In addition, drop irrelevant comments and nans.\n",
        "        \"\"\"\n",
        "        # Wrap the progress bar around the column iteration\n",
        "        tqdm.pandas(desc=\"Cleaning Comments\")\n",
        "        df.iloc[:, self.comment_column_idx] = df.iloc[:, self.comment_column_idx].progress_apply(self._normalize)\n",
        "        df = df.dropna(subset=[df.columns[self.comment_column_idx]])\n",
        "        df = df[\n",
        "        df[df.columns[self.comment_column_idx]].apply(lambda x: len(x.split()) >= 2)\n",
        "        ]\n",
        "        return df\n",
        "\n",
        "    def _undersample(self, df, targets: dict):\n",
        "        \"\"\"\n",
        "        Performs undersampling so that all the labels will have a predefined number of rows in df (later to be self.data).\n",
        "        Args:\n",
        "            df (pd.DataFrame): The df for processing.\n",
        "            targets (Dict(str:int)): A dictionary that defined the max number of rows for each label in the output like:\n",
        "                {\n",
        "                    \"Pro-Palestine\": 5500,\n",
        "                    \"Pro-Israel\": 5500,\n",
        "                    \"Undefined\": 5500\n",
        "                }\n",
        "        \"\"\"\n",
        "        dfs = []\n",
        "        for lab, n in targets.items():\n",
        "            lab_df = df[df.iloc[:, self.label_column_idx] == lab]\n",
        "            dfs.append(lab_df.sample(min(n, len(lab_df)), random_state=42))\n",
        "        return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    def _augment(self, df, mask_train, classes, ratio, methods, adv_ratio):\n",
        "        \"\"\"\n",
        "        Performs augmentation using the TextAugmenter class to the classes that needs augmentation.\n",
        "        Augmentation will create |ratio| adversed copies of comments from the augmented class, giving them unique UIDs.\n",
        "        Args:\n",
        "            df (pd.DataFrame): The df for processing.\n",
        "            mask_train (pd.DataFrame): boolean mask selecting TRAIN rows inside *df*.\n",
        "            classes (List(str)): The classes to augment (class labels like \"Pro-Israel\").\n",
        "            ratio (int): Number of new adversed copies to add.\n",
        "            methods (List(str)): A list with the augmentation methods.\n",
        "            adv_ratio (float): Ratio of each comment (words from total comment) to adverse.\n",
        "\n",
        "        NOTE: All of the params are handled in the llm_config.py file.\n",
        "        \"\"\"\n",
        "        aug = TextAugmenter(adv_ratio, methods)\n",
        "        train_df = df.loc[mask_train]\n",
        "        extra = []\n",
        "\n",
        "        for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Augment\", unit=\"row\"):\n",
        "            lab = row.iloc[self.label_column_idx]\n",
        "            if lab not in classes:\n",
        "                continue\n",
        "            for i in range(int(ratio)):\n",
        "                new = row.copy()\n",
        "                new.iloc[self.comment_column_idx] = aug.augment_comment(row.iloc[self.comment_column_idx])\n",
        "                new.iloc[self.id_column_idx]  = f\"{row.iloc[self.id_column_idx]}_aug{i+1}\"\n",
        "                extra.append(new)\n",
        "        if extra:\n",
        "            df = pd.concat([df, pd.DataFrame(extra, columns=df.columns)], ignore_index=True)\n",
        "        return df\n",
        "\n",
        "    def save_to_csv(self, output_repo: str = \"Data\"):\n",
        "        \"\"\"\n",
        "        Save the dataset to a CSV file for inspection.\n",
        "        \"\"\"\n",
        "        action_tag = getattr(self, \"action\", \"regular\")     # augmented / undersampled / regular\n",
        "        filename  = f\"{action_tag}_research_data_for_inspection.csv\"\n",
        "        out_path = Path(output_repo) / filename\n",
        "        self.data.to_csv(out_path, index=False)\n",
        "        print(f\"[TextDataset] saved CSV → {out_path}\")\n",
        "\n",
        "    # -------------------------- Dataset API -------------------\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            return [self[i] for i in range(*idx.indices(len(self)))]\n",
        "\n",
        "        row = self.data.iloc[idx]\n",
        "        comment_id = row.iloc[self.id_column_idx]\n",
        "        comment    = row.iloc[self.comment_column_idx]\n",
        "        encoded_label = LABELS_ENCODER.get(row.iloc[self.label_column_idx])\n",
        "        return comment_id, comment, encoded_label\n",
        "\n",
        "\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class to handle embedding generation.\n",
        "    For modularity, as this object is the needed object for classification task,\n",
        "    it is responsible for the creation of the TextDatasets and their modification to Embedding\n",
        "    based one.\n",
        "    Caching of dataset memory is defined to avoid re-calculations of embeddings.\n",
        "    The embeddings are precomputed on init to support non NN models that cannot handle\n",
        "    a dataloader, so beware of memory usage.\n",
        "    If a strict NN based model is selected as best model, there is no need for the pre-computation.\n",
        "\n",
        "    NOTE: One dataset will be created from a .csv file and will have TRAIN, VAL and TEST callable subsets (using _View).\n",
        "    '''\n",
        "    class _View(Dataset):\n",
        "        def __init__(self, emb, labels, idx):\n",
        "            self.embeddings, self.labels = emb[idx], labels[idx]\n",
        "        def __len__(self):        return len(self.labels)\n",
        "        def __getitem__(self, i): return self.embeddings[i], self.labels[i]\n",
        "\n",
        "    def __init__(self, text_dataset, embedder, embedding_method, cache_dir=r\"Data\\cache\"):\n",
        "        \"\"\"\n",
        "        Creates the Dataset instance which fits the classification task.\n",
        "        Most recurrent parameters are used to initiate the TextDataset in the init.\n",
        "        Args:\n",
        "            text_dataset (TextDataset): A pre-computed TextDataset instance\n",
        "            embedder (Embedder): Embedder instance for generating embeddings.\n",
        "            embedding_method (str): Method for embedding generation (e.g., 'distilbert', 'tf-idf').\n",
        "            cache_dir (str): Directory to save the pre-computed embeddings instead of re-calculate.\n",
        "        \"\"\"\n",
        "        # ---- load CSV once -------------------------------------------------\n",
        "        self.text_dataset = text_dataset\n",
        "        self.embedder, self.embedding_method = embedder, embedding_method\n",
        "        cache_dir = Path(cache_dir); cache_dir.mkdir(exist_ok=True)\n",
        "        self.action = self.text_dataset.action\n",
        "        self.cache_file = cache_dir / f\"{embedding_method}_embeddings_{self.action}.pkl\"\n",
        "\n",
        "        if self.cache_file.exists():\n",
        "            print(f\"[EmbeddingDataset]: Loading precomputed embeddings from {self.cache_file}...\")\n",
        "            with open(self.cache_file, \"rb\") as f:\n",
        "                blob = pickle.load(f)\n",
        "            self.embeddings = blob[\"embeddings\"]          # (N,D) torch tensor\n",
        "            self.labels = blob[\"labels\"]           # (N,)  torch tensor\n",
        "        else:\n",
        "            print(f\"[EmbeddingDataset]: Precomputing embeddings and saving to {self.cache_file}...\")\n",
        "            self.embeddings, self.labels = self._build_and_cache()\n",
        "        print(\"[EmbeddingDataset Status]: Embedding generation complete.\")\n",
        "\n",
        "\n",
        "    def _build_and_cache(self):\n",
        "        emb, lab = [], []\n",
        "        for txt in tqdm(self.text_dataset.data.iloc[:, self.text_dataset.comment_column_idx],\n",
        "                        total=len(self.text_dataset.data), desc=\"Embedding Comments\"):\n",
        "            v = self.embedder.embed(txt, method=self.embedding_method)\n",
        "            emb.append(torch.as_tensor(v, dtype=torch.float32))\n",
        "        emb = torch.stack(emb)\n",
        "        lab = torch.tensor(\n",
        "            self.text_dataset.data.iloc[:, self.text_dataset.label_column_idx].to_numpy(),\n",
        "            dtype=torch.long)\n",
        "        with open(self.cache_file, \"wb\") as f:\n",
        "            pickle.dump({\"embeddings\": emb, \"labels\": lab}, f)\n",
        "        return emb, lab\n",
        "    # --------------------- Public Methods ---------------------------------\n",
        "    def get_subset(self, split: str) -> Dataset:\n",
        "        \"\"\"\n",
        "        Get a desired split from the Dataset -> TRAIN, VAL, TEST or any combination using '+'.\n",
        "\n",
        "        Example:\n",
        "            get_subset(\"TRAIN\") → standard\n",
        "            get_subset(\"TRAIN+VAL\") → merged dataset\n",
        "            get_subset(\"TRAIN+VAL+TEST\") → full dataset\n",
        "        \"\"\"\n",
        "        if \"+\" in split:\n",
        "            splits = list(set(split.split(\"+\")))\n",
        "            all_idx = []\n",
        "            for sub in splits:\n",
        "                sub = sub.strip().upper()\n",
        "                if sub not in self.text_dataset.idx_split:\n",
        "                    raise ValueError(f\"[Split Error] Unknown split name: {sub}\")\n",
        "                all_idx.append(self.text_dataset.idx_split[sub])\n",
        "            idx = np.concatenate(all_idx)\n",
        "        else:\n",
        "            split = split.strip().upper()\n",
        "            if split not in self.text_dataset.idx_split:\n",
        "                raise ValueError(f\"[Split Error] Unknown split name: {split}\")\n",
        "            idx = self.text_dataset.idx_split[split]\n",
        "\n",
        "        return self._View(self.embeddings, self.labels, idx)\n",
        "\n",
        "    # --------------------- Dataset API ---------------------------------\n",
        "    def __len__(self):\n",
        "        return len(self.text_dataset)  # Length is based on the original TextDataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return the precomputed embedding and label for the given index.\n",
        "        \"\"\"\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "def _to_numpy(t):\n",
        "    \"\"\"\n",
        "    Helper:  tensor  ➜  numpy (always on CPU, detached).\n",
        "    Accepts torch.Tensor or anything that is already a numpy array.\n",
        "    \"\"\"\n",
        "    if isinstance(t, torch.Tensor):\n",
        "        return t.detach().cpu().numpy()\n",
        "    return np.asarray(t)\n",
        "\n",
        "def get_dataloader(dataset, batch_size=32, shuffle=True, num_workers=2):\n",
        "    '''\n",
        "    Will create the DataLoader object.\n",
        "    Assumption is that a Dataset object is passed. Function will response if the dataset is TextDataset or EmbeddingDataset.\n",
        "    If EmbeddingDataset, this function will return a Dataloader and a (X, y) tuple for other scikit models.\n",
        "    Else, if dataset is TextDataset it will return a text dataset for it, which is designed for analysis of Transformer model's feed.\n",
        "    The function is GPU‑safe:   `.cpu()` before `.numpy()`.\n",
        "\n",
        "    Args:\n",
        "        dataset (TextDataset or EmbeddingDataset): The original dataset object.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        shuffle (bool): Whether to shuffle the dataset.\n",
        "        num_workers (int): Number of subprocesses for data loading.\n",
        "\n",
        "    Returns:\n",
        "        if type(dataset) == 'TextDataset':\n",
        "            DataLoader: A PyTorch DataLoader object for text output.\n",
        "        elif type(dataset) == 'EmbeddingDataset':\n",
        "            1. DataLoader: A PyTorch DataLoader object for embedding output.\n",
        "            2. tuple: An (X, y) tuple for other scikit models.\n",
        "    '''\n",
        "    print(f'[Dataloader Status]: Preparing the dataloader...')\n",
        "    pin = torch.cuda.is_available()          # use pinned memory when GPU present\n",
        "    dl = DataLoader(dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=shuffle,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=pin)\n",
        "\n",
        "    # --- TextDataset: nothing else to do ---------------------------------\n",
        "    if 'TextDataset' in dataset.__class__.__name__:\n",
        "        return dl\n",
        "\n",
        "    # -------- EmbeddingDataset or its _View -------------------------------------\n",
        "    if isinstance(dataset, EmbeddingDataset) or isinstance(dataset, EmbeddingDataset._View):\n",
        "        # quick peek at first batch\n",
        "        for b, (_, y_b) in enumerate(dl):\n",
        "            print(f\"[DL] peek batch {b}: y[:5] =\", _to_numpy(y_b)[:5])\n",
        "            break\n",
        "\n",
        "        X = _to_numpy(dataset.embeddings)\n",
        "        y = _to_numpy(dataset.labels)\n",
        "        print(f\"[DL] EmbeddingDataset ready. X shape {X.shape}, y len {len(y)}\")\n",
        "        return dl, (X, y)\n",
        "\n",
        "    # -------- unknown dataset ---------------------------------------------------\n",
        "    raise ValueError(f\"[DL] Unrecognized dataset type: {type(dataset)}\")\n",
        "\n",
        "def patch_cached_labels(pkl_path):\n",
        "    '''\n",
        "    Had a reccurring problem with the datasets so added this utility to fix the label encodings\n",
        "    of a cached dataset without having to re-calculate.\n",
        "\n",
        "    Use like:\n",
        "    patch_cached_labels(\"Data/cache/distilbert_embeddings_regular.pkl\")\n",
        "    '''\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        blob = pickle.load(f)\n",
        "\n",
        "    labels = blob[\"labels\"]\n",
        "    if not torch.is_tensor(labels):\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Fix labels that are outside expected range\n",
        "    corrected_labels = []\n",
        "    for lbl in labels:\n",
        "        lbl_int = int(lbl)\n",
        "        if lbl_int in LABELS_ENCODER.values():\n",
        "            corrected_labels.append(lbl_int)\n",
        "        else:\n",
        "            print(f\"[Warning] Skipping invalid label: {lbl_int}\")\n",
        "            corrected_labels.append(-1)  # or raise error if strict\n",
        "\n",
        "    blob[\"labels\"] = torch.tensor(corrected_labels, dtype=torch.long)\n",
        "\n",
        "    with open(pkl_path, 'wb') as f:\n",
        "        pickle.dump(blob, f)\n",
        "\n",
        "    print(f\"[Patch] Labels fixed and saved back to: {pkl_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJfEzovg2YsZ",
        "outputId": "07d0866c-fe35-45f1-b9e0-5e9fef7ab1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_EgGWzf08e2"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZUhjJlOuK9J",
        "outputId": "6e50c991-ffc2-4f65-de0b-fa1b4d098f74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cleaning Comments: 100%|██████████| 2697930/2697930 [01:46<00:00, 25222.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TextDataset] rows: train=0, val=0, test=2683230\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = \"/content/drive/My Drive/reddit_opinion_PSE_ISR_cleaned_for_embedding.csv\"\n",
        "ID_COLUMN_IDX = 0\n",
        "COMMENT_COLUMN_IDX = 1\n",
        "LABEL_COLUMN_IDX = 2\n",
        "SUBSET_COLUMN_IDX = 3\n",
        "\n",
        "txt_regular = TextDataset(\n",
        "    csv_path          = DATA_PATH,\n",
        "    id_column_idx     = ID_COLUMN_IDX,\n",
        "    comment_column_idx= COMMENT_COLUMN_IDX,\n",
        "    label_column_idx  = LABEL_COLUMN_IDX,\n",
        "    split_column_idx  = SUBSET_COLUMN_IDX,  # TRAIN / VAL / TEST column\n",
        "    augmented_classes = [],                 # ‑‑ no aug\n",
        "    augmentation_ratio= 0,\n",
        "    undersampling_targets = {},             # ‑‑ no undersampling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa3Gwt-x4uwh"
      },
      "outputs": [],
      "source": [
        "EMB_METHODS = [\"distilbert\"]\n",
        "embedder    = Embedder()                  # your wrapper around HF / TF‑IDF\n",
        "\n",
        "# embedding_sets: dict[str, dict[str, EmbeddingDataset]] = {}   # {method : {variant : EmbeddingDataset}}\n",
        "\n",
        "# for method in EMB_METHODS:\n",
        "#     embedding_sets[method] = {\n",
        "#         \"regular\"      : EmbeddingDataset(txt_regular,      embedder, method),\n",
        "#     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWyIASZvFcsU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from google.colab import runtime\n",
        "\n",
        "def embed_in_chunks_resumable(dataset, embedder, method: str,\n",
        "                              chunk_size: int = 100_000,\n",
        "                              output_dir: str = \"/content/cache_embeddings\",\n",
        "                              drive_dir: str = \"/content/drive/MyDrive/reddit_embeddings\",\n",
        "                              merge_after: bool = True,\n",
        "                              disconnect: bool = False):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(drive_dir, exist_ok=True)\n",
        "\n",
        "    total_chunks = (len(dataset) + chunk_size - 1) // chunk_size\n",
        "\n",
        "    for chunk_id in range(total_chunks):\n",
        "        chunk_filename = f\"{method}_chunk_{chunk_id:03d}.pkl\"\n",
        "        local_path = os.path.join(output_dir, chunk_filename)\n",
        "        drive_path = os.path.join(drive_dir, chunk_filename)\n",
        "\n",
        "        if os.path.exists(drive_path):  # <-- Only check drive_path now\n",
        "            print(f\"✓ Skipping existing chunk {chunk_id + 1}: {chunk_filename}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"→ Embedding chunk {chunk_id + 1}/{total_chunks}\")\n",
        "        chunk = dataset[chunk_id * chunk_size: (chunk_id + 1) * chunk_size]\n",
        "        ids, texts = zip(*[(ex[0], ex[1]) for ex in chunk])  # ex: (id, text, label)\n",
        "\n",
        "        embeddings = []\n",
        "        for txt in tqdm(texts, desc=\"Embedding texts\", leave=False):\n",
        "            emb = embedder.embed(txt, method)\n",
        "            embeddings.append(emb)\n",
        "\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            pickle.dump((ids, embeddings), f)\n",
        "\n",
        "        os.system(f\"cp '{local_path}' '{drive_path}'\")\n",
        "        print(f\"✓ Saved {chunk_filename} to both local and Drive.\")\n",
        "\n",
        "    print(\"✅ All chunks processed.\")\n",
        "\n",
        "    if merge_after:\n",
        "        print(\"🔄 Merging all chunks from Drive into a single file...\")\n",
        "        all_ids, all_embs = [], []\n",
        "\n",
        "        # Scan Drive directory for all matching chunk files\n",
        "        all_files = os.listdir(drive_dir)\n",
        "        chunk_files = sorted([f for f in all_files if f.startswith(f\"{method}_chunk_\") and f.endswith(\".pkl\")])\n",
        "\n",
        "        for filename in chunk_files:\n",
        "            full_path = os.path.join(drive_dir, filename)\n",
        "            try:\n",
        "                with open(full_path, \"rb\") as f:\n",
        "                    ids, embs = pickle.load(f)\n",
        "                    all_ids.extend(ids)\n",
        "                    all_embs.extend(embs)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Failed to load {filename}: {e}\")\n",
        "\n",
        "        merged_path = os.path.join(drive_dir, f\"{method}_FULL.pkl\")\n",
        "        with open(merged_path, \"wb\") as f:\n",
        "            pickle.dump((all_ids, all_embs), f)\n",
        "\n",
        "        print(f\"✅ Merged embedding file saved to: {merged_path}\")\n",
        "\n",
        "    if disconnect:\n",
        "        print(\"🚪 Disconnecting Colab runtime...\")\n",
        "        runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdRgyQjoMiV_",
        "outputId": "25258f41-0c68-4feb-b3fc-c547c7045f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Skipping existing chunk 1: distilbert_chunk_000.pkl\n",
            "✓ Skipping existing chunk 2: distilbert_chunk_001.pkl\n",
            "✓ Skipping existing chunk 3: distilbert_chunk_002.pkl\n",
            "✓ Skipping existing chunk 4: distilbert_chunk_003.pkl\n",
            "✓ Skipping existing chunk 5: distilbert_chunk_004.pkl\n",
            "✓ Skipping existing chunk 6: distilbert_chunk_005.pkl\n",
            "✓ Skipping existing chunk 7: distilbert_chunk_006.pkl\n",
            "✓ Skipping existing chunk 8: distilbert_chunk_007.pkl\n",
            "✓ Skipping existing chunk 9: distilbert_chunk_008.pkl\n",
            "✓ Skipping existing chunk 10: distilbert_chunk_009.pkl\n",
            "✓ Skipping existing chunk 11: distilbert_chunk_010.pkl\n",
            "✓ Skipping existing chunk 12: distilbert_chunk_011.pkl\n",
            "✓ Skipping existing chunk 13: distilbert_chunk_012.pkl\n",
            "✓ Skipping existing chunk 14: distilbert_chunk_013.pkl\n",
            "✅ All chunks processed.\n",
            "🔄 Merging all chunks from Drive into a single file...\n",
            "✅ Merged embedding file saved to: /content/drive/MyDrive/reddit_embeddings/distilbert_FULL.pkl\n"
          ]
        }
      ],
      "source": [
        "embed_in_chunks_resumable(\n",
        "    dataset    = txt_regular,\n",
        "    embedder   = embedder,\n",
        "    method     = \"distilbert\",\n",
        "    chunk_size = 200_000,  # or smaller/larger\n",
        "    output_dir = \"/content/cache_embeddings\",\n",
        "    drive_dir  = \"/content/drive/MyDrive/reddit_embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn5vslz4ztE7"
      },
      "source": [
        "# Part 2: Getting Stance Classification on the DB\n",
        "\n",
        "Use this flow to activate an inference flow on the large VDB. Do not train a fresh classifier head model from here (it's a waste of resources and you'll need to further adjust the structure for this)\n",
        "\n",
        "Be sure to upload the trained classifier head to this local env before running the flow:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: For the final version I used local env, as the 2.5M records classification took roughly 10h, and colab wasn't cooperative. You can still use the drive.mount functionality by un-hiding the relevant code piece. Notive that the classification head does not benefit from GPU usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvH9GjTy19AW",
        "outputId": "c65adedf-628a-4d48-831e-3edb8607c69c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "project_root = \"C:\\\\Users\\\\yonat\\\\CodeProjects\\\\Israel-Palestine-Political-Affiliation-Text-Classification\"\n",
        "CHECKPOINTS = os.path.join(project_root, 'Checkpoint')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vudLWR7sz46_"
      },
      "outputs": [],
      "source": [
        "# classifiers.py\n",
        "\n",
        "'''\n",
        "Class to contain the different model, each should be easiliy called for initialization,\n",
        "fit and predict.\n",
        "'''\n",
        "import os\n",
        "import ast\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setting random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)  # For CUDA (GPU) if you're using it\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initializes a Deep Neural Network (DNN) with the given configuration.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Configuration for the DNN.\n",
        "                {\n",
        "                    \"learning_rate\": float,\n",
        "                    \"batch_norm\": bool,\n",
        "                    \"drop_out\": float,\n",
        "                    \"layers\": str(list[int])\n",
        "                }\n",
        "        \"\"\"\n",
        "        super(DNN, self).__init__()\n",
        "        layers = []\n",
        "        if isinstance(config[\"layers\"], str):\n",
        "            config[\"layers\"] = list(ast.literal_eval(config[\"layers\"]))\n",
        "        input_size = config[\"layers\"][0]\n",
        "\n",
        "        # Iterate through the hidden layers\n",
        "        for output_size in config[\"layers\"][1:-1]:  # Skip the last layer (number of classes)\n",
        "            layers.append(nn.Linear(input_size, output_size))\n",
        "            if config.get(\"batch_norm\", False):\n",
        "                layers.append(nn.BatchNorm1d(output_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            if config.get(\"drop_out\", 0.0) > 0:\n",
        "                layers.append(nn.Dropout(config[\"drop_out\"]))\n",
        "            input_size = output_size\n",
        "\n",
        "        # Final classification layer\n",
        "        num_classes = config[\"layers\"][-1]\n",
        "        layers.append(nn.Linear(input_size, num_classes))  # Last layer matches num_classes\n",
        "        # layers.append(nn.Softmax(dim=1))  # Softmax activation for multi-class output\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.learning_rate = config[\"learning_rate\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self, config, model_type, log=True, init_model=True):\n",
        "        \"\"\"\n",
        "        Initializes the classifier based on the model type and configuration.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Configuration for the model. Format varies based on the model type.\n",
        "            model_type (str): One of \"logistic_regression\", \"svm\", \"xgboost\", or \"dnn\".\n",
        "            log (bool): Print the loss progress? Redundent if epochs are optimized externally.\n",
        "            init_model (bool): Allows model initialization for loaded model. Called like clf.load() after init.\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.model_params = config          # save for cloning\n",
        "        self.log = log\n",
        "\n",
        "        if not init_model:\n",
        "            self.model = None\n",
        "            return\n",
        "\n",
        "        if model_type in [\"logistic_regression\", \"dnn\"]:\n",
        "            # A one layered logistic regression implementation using the DNN class\n",
        "            self.model = DNN(config)\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "            self.criterion = None  # Will be created in fit() based on label distribution\n",
        "            self.num_epochs = config[\"num_epochs\"]\n",
        "        elif model_type == \"svm\":\n",
        "            config.setdefault(\"class_weight\", \"balanced\")  # Balance class weights\n",
        "            self.model = SVC(random_state=42, **config)\n",
        "        elif model_type == \"xgboost\":\n",
        "            self.model = XGBClassifier(random_state=42, **config)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "\n",
        "    def fit(self, train_data_package):\n",
        "        \"\"\"\n",
        "        Fits the model to the training data.\n",
        "        Allows to load an existing model, if training already reached a best saved model.\n",
        "        Works with the output of the function get_dataloader which gets the desired datashape\n",
        "        per model.\n",
        "\n",
        "        Args:\n",
        "            train_data_package (tuple): A tuple containing (DataLoader, (X, y))\n",
        "        \"\"\"\n",
        "        # ----------------- Determine checkpoint path -----------------\n",
        "        checkpoint_path = os.path.join(CHECKPOINTS, f'best_{self.model_type}.pt' if self.model_type in [\"logistic_regression\", \"dnn\"] else f'best_{self.model_type}.pkl')\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            self.log and print(f\"[Model Fit Status]: Loading pre-trained {self.model_type} from checkpoint.\")\n",
        "\n",
        "            if self.model_type in [\"logistic_regression\", \"dnn\"]:\n",
        "                self.model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
        "                self.model.to(DEVICE)\n",
        "                self.model.eval()\n",
        "            else:  # sklearn/xgboost\n",
        "                with open(checkpoint_path, \"rb\") as f:\n",
        "                    self.model = pickle.load(f)\n",
        "            return\n",
        "\n",
        "        # ----------------- Otherwise, train the model -----------------\n",
        "        train_dataloader, (X_train, y_train) = train_data_package\n",
        "        if self.model_type == \"svm\":\n",
        "            self.log and print(f'[Model Fit Status]: Fitting the model...')\n",
        "            self.model.fit(X_train, y_train)\n",
        "        elif self.model_type == \"xgboost\":\n",
        "            self.log and print(f'[Model Fit Status]: Fitting the model...')\n",
        "            y_train = y_train.detach().cpu().numpy() if isinstance(y_train, torch.Tensor) else y_train\n",
        "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train).astype(int), y=y_train.astype(int))\n",
        "            sample_weight = np.array([class_weights[label] for label in y_train])\n",
        "            self.model.fit(X_train, y_train, sample_weight=sample_weight)\n",
        "        elif self.model_type in [\"logistic_regression\", \"dnn\"]:\n",
        "            self.log and print(f'[Model Fit Status]: Fitting the model...')\n",
        "            y_train = y_train.detach().cpu().numpy() if isinstance(y_train, torch.Tensor) else y_train\n",
        "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train).astype(int), y=y_train.astype(int))\n",
        "            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "            self.criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "            self.model.train()\n",
        "            for epoch in range(self.num_epochs):\n",
        "                for _, (features, labels) in enumerate(train_dataloader):\n",
        "                    self.optimizer.zero_grad()\n",
        "                    outputs = self.model(features.float())\n",
        "                    loss = self.criterion(outputs.squeeze(), labels.long())\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                self.log and print(f\"Epoch {epoch + 1}: Training Loss = {loss.item()}\")\n",
        "\n",
        "\n",
        "    def _batch_predict_sklearn(self, X, batch_size=1024, proba=False):\n",
        "        preds, probs = [], []\n",
        "        for i in tqdm(range(0, len(X), batch_size), desc=\"Predicting\", ncols=100):\n",
        "            X_batch = X[i:i+batch_size]\n",
        "            if proba and hasattr(self.model, \"predict_proba\"):\n",
        "                batch_probs = self.model.predict_proba(X_batch)\n",
        "                probs.append(batch_probs)\n",
        "                preds.append(batch_probs.argmax(axis=1))\n",
        "            else:\n",
        "                preds.append(self.model.predict(X_batch))\n",
        "        predictions = np.concatenate(preds)\n",
        "        if proba and probs:\n",
        "            probas = np.concatenate(probs)\n",
        "            return predictions, probas\n",
        "        return predictions, None\n",
        "\n",
        "    def predict(self, test_data_package, proba=False):\n",
        "        \"\"\"\n",
        "        Predicts labels for the given test data. Optionally returns class probabilities.\n",
        "\n",
        "        Args:\n",
        "            test_data_package (tuple): (DataLoader, (X, y))\n",
        "            proba (bool): If True, also return class probabilities.\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted labels\n",
        "            (optional) list: Predicted class probabilities\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        probas = []\n",
        "\n",
        "        test_dataloader, (X_test, _) = test_data_package\n",
        "\n",
        "        if self.model_type in [\"svm\", \"xgboost\"]:\n",
        "            self.log and print(f'[Model Pred Status]: Generating predictions...')\n",
        "            predictions, probas = self._batch_predict_sklearn(X_test, batch_size=1024, proba=proba)\n",
        "            return (predictions.tolist(), probas.tolist()) if proba else predictions.tolist()\n",
        "\n",
        "        elif self.model_type in [\"logistic_regression\", \"dnn\"]:\n",
        "            self.log and print(f'[Model Pred Status]: Generating predictions...')\n",
        "            predictions = []\n",
        "            probas = []\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                for _, (features, _) in enumerate(tqdm(test_dataloader, desc=\"Predicting\", ncols=100)):\n",
        "                    features = features.to(DEVICE)\n",
        "                    outputs = self.model(features.float())  # shape: (batch_size, num_classes)\n",
        "                    if proba:\n",
        "                        probs = torch.softmax(outputs, dim=1)\n",
        "                        probas.extend(probs.cpu().tolist())\n",
        "                        preds = torch.argmax(probs, dim=1)\n",
        "                    else:\n",
        "                        preds = torch.argmax(outputs, dim=1)\n",
        "                    predictions.extend(preds.cpu().tolist())\n",
        "            return (predictions, probas) if proba else predictions\n",
        "\n",
        "    def save(self, path):\n",
        "        if self.model_type in [\"svm\", \"xgboost\"]:\n",
        "            with open(path, \"wb\") as f:\n",
        "                pickle.dump(self.model, f)\n",
        "        elif self.model_type in [\"logistic_regression\", \"dnn\"]:\n",
        "            torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load(self, checkpoint_path=None):\n",
        "        \"\"\"\n",
        "        Loads a pre-trained model from disk.\n",
        "        Args:\n",
        "            checkpoint_path (str, optional): Path to checkpoint file. If None, uses default from config.\n",
        "        \"\"\"\n",
        "        if checkpoint_path is None:\n",
        "            checkpoint_path = os.path.join(CHECKPOINTS, f'best_{self.model_type}.pt' if self.model_type in [\"logistic_regression\", \"dnn\"] else f'best_{self.model_type}.pkl')\n",
        "\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            raise FileNotFoundError(f\"[Load Error] No checkpoint found at: {checkpoint_path}\")\n",
        "\n",
        "        if self.model_type in [\"logistic_regression\", \"dnn\"]:\n",
        "            self.model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
        "            self.model.to(DEVICE)\n",
        "            self.model.eval()\n",
        "        else:\n",
        "            with open(checkpoint_path, \"rb\") as f:\n",
        "                self.model = pickle.load(f)\n",
        "\n",
        "        self.log and print(f\"[Model Load Status]: Loaded pretrained {self.model_type} from {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H8oRxI2g4DZd"
      },
      "outputs": [],
      "source": [
        "# embedder_utils from embedder.py\n",
        "\n",
        "def _to_numpy(t):\n",
        "    \"\"\"\n",
        "    Helper:  tensor  ➜  numpy (always on CPU, detached).\n",
        "    Accepts torch.Tensor or anything that is already a numpy array.\n",
        "    \"\"\"\n",
        "    if isinstance(t, torch.Tensor):\n",
        "        return t.detach().cpu().numpy()\n",
        "    return np.asarray(t)\n",
        "\n",
        "def get_dataloader(dataset, batch_size=32, shuffle=True, num_workers=2):\n",
        "    '''\n",
        "    Will create the DataLoader object.\n",
        "    Assumption is that a Dataset object is passed. Function will response if the dataset is TextDataset or EmbeddingDataset.\n",
        "    If EmbeddingDataset, this function will return a Dataloader and a (X, y) tuple for other scikit models.\n",
        "    Else, if dataset is TextDataset it will return a text dataset for it, which is designed for analysis of Transformer model's feed.\n",
        "    The function is GPU‑safe:   `.cpu()` before `.numpy()`.\n",
        "\n",
        "    Args:\n",
        "        dataset (TextDataset or EmbeddingDataset): The original dataset object.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        shuffle (bool): Whether to shuffle the dataset.\n",
        "        num_workers (int): Number of subprocesses for data loading.\n",
        "\n",
        "    Returns:\n",
        "        if type(dataset) == 'TextDataset':\n",
        "            DataLoader: A PyTorch DataLoader object for text output.\n",
        "        elif type(dataset) == 'EmbeddingDataset':\n",
        "            1. DataLoader: A PyTorch DataLoader object for embedding output.\n",
        "            2. tuple: An (X, y) tuple for other scikit models.\n",
        "    '''\n",
        "    print(f'[Dataloader Status]: Preparing the dataloader...')\n",
        "    pin = torch.cuda.is_available()          # use pinned memory when GPU present\n",
        "    dl = DataLoader(dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=shuffle,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=pin)\n",
        "\n",
        "    # --- TextDataset: nothing else to do ---------------------------------\n",
        "    if 'TextDataset' in dataset.__class__.__name__:\n",
        "        return dl\n",
        "\n",
        "    # -------- EmbeddingDataset or its _View -------------------------------------\n",
        "    if isinstance(dataset, EmbeddingDataset) or isinstance(dataset, EmbeddingDataset._View):\n",
        "        # quick peek at first batch\n",
        "        for b, (_, y_b) in enumerate(dl):\n",
        "            print(f\"[DL] peek batch {b}: y[:5] =\", _to_numpy(y_b)[:5])\n",
        "            break\n",
        "\n",
        "        X = _to_numpy(dataset.embeddings)\n",
        "        y = _to_numpy(dataset.labels)\n",
        "        print(f\"[DL] EmbeddingDataset ready. X shape {X.shape}, y len {len(y)}\")\n",
        "        return dl, (X, y)\n",
        "\n",
        "    # -------- unknown dataset ---------------------------------------------------\n",
        "    raise ValueError(f\"[DL] Unrecognized dataset type: {type(dataset)}\")\n",
        "\n",
        "def patch_cached_labels(pkl_path):\n",
        "    '''\n",
        "    Had a reccurring problem with the datasets so added this utility to fix the label encodings\n",
        "    of a cached dataset without having to re-calculate.\n",
        "\n",
        "    Use like:\n",
        "    patch_cached_labels(\"Data/cache/distilbert_embeddings_regular.pkl\")\n",
        "    '''\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        blob = pickle.load(f)\n",
        "\n",
        "    labels = blob[\"labels\"]\n",
        "    if not torch.is_tensor(labels):\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Fix labels that are outside expected range\n",
        "    corrected_labels = []\n",
        "    for lbl in labels:\n",
        "        lbl_int = int(lbl)\n",
        "        if lbl_int in LABELS_ENCODER.values():\n",
        "            corrected_labels.append(lbl_int)\n",
        "        else:\n",
        "            print(f\"[Warning] Skipping invalid label: {lbl_int}\")\n",
        "            corrected_labels.append(-1)  # or raise error if strict\n",
        "\n",
        "    blob[\"labels\"] = torch.tensor(corrected_labels, dtype=torch.long)\n",
        "\n",
        "    with open(pkl_path, 'wb') as f:\n",
        "        pickle.dump(blob, f)\n",
        "\n",
        "    print(f\"[Patch] Labels fixed and saved back to: {pkl_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12KdDN4H2xEX"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-86-D0Yk2wWK",
        "outputId": "0ed91eda-2cf7-41c1-8d8e-587e46e93f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 1] Loading merged embeddings from disk...\n",
            "[Step 2] Loading pretrained SVM model...\n",
            "[Model Load Status]: Loaded pretrained svm from C:\\Users\\yonat\\CodeProjects\\Israel-Palestine-Political-Affiliation-Text-Classification\\Checkpoint\\best_svm.pkl\n",
            "[Step 3] Predicting on full dataset (chunked)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 2684/2684 [13:42:27<00:00, 18.39s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 4] Merging predictions into full dataset...\n",
            "[Step 5] Saving labeled dataset...\n",
            "✅ Merged + labeled dataset saved locally to: classified_comment_stance.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Constants\n",
        "FULL_DATA_PATH = \"C:\\\\Users\\\\yonat\\\\CodeProjects\\\\Israel-Palestine-Political-Affiliation-Text-Classification\\\\Data\\\\cache\\\\reddit_opinion_PSE_ISR_cleaned.csv\"\n",
        "EMBEDDING_CACHE = \"C:\\\\Users\\\\yonat\\\\CodeProjects\\\\Israel-Palestine-Political-Affiliation-Text-Classification\\\\Data\\\\cache\\\\distilbert_FULL.pkl\"\n",
        "CHECKPOINT_PATH = \"C:\\\\Users\\\\yonat\\\\CodeProjects\\\\Israel-Palestine-Political-Affiliation-Text-Classification\\\\Checkpoint\\\\best_svm.pkl\"\n",
        "\n",
        "LOCAL_SAVE_PATH = \"classified_comment_stance.csv\"\n",
        "\n",
        "# Dataset column indices\n",
        "ID_COLUMN_IDX = 0\n",
        "COMMENT_COLUMN_IDX = 1\n",
        "LABEL_COLUMN_IDX = 2  # dummy\n",
        "SUBSET_COLUMN_IDX = 3\n",
        "\n",
        "# 1. Load embeddings\n",
        "print(\"[Step 1] Loading merged embeddings from disk...\")\n",
        "if not os.path.exists(EMBEDDING_CACHE):\n",
        "    raise FileNotFoundError(f\"Missing merged embedding file: {EMBEDDING_CACHE}\")\n",
        "\n",
        "with open(EMBEDDING_CACHE, \"rb\") as f:\n",
        "    ids, X = pickle.load(f)  # tuple: (comment_ids, embeddings)\n",
        "\n",
        "# 2. Load pretrained SVM\n",
        "print(\"[Step 2] Loading pretrained SVM model...\")\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    raise FileNotFoundError(f\"SVM checkpoint not found: {CHECKPOINT_PATH}\")\n",
        "\n",
        "svm_clf = Classifier({}, model_type=\"svm\", log=True, init_model=False)\n",
        "svm_clf.load(CHECKPOINT_PATH)\n",
        "\n",
        "# 3. Predict\n",
        "print(\"[Step 3] Predicting on full dataset (chunked)...\")\n",
        "chunk_size = 1_000\n",
        "y_pred = []\n",
        "\n",
        "for i in tqdm(range(0, len(X), chunk_size), desc=\"Predicting\"):\n",
        "    X_chunk = X[i: i + chunk_size]\n",
        "    preds = svm_clf.model.predict(X_chunk)\n",
        "    y_pred.extend(preds)\n",
        "\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# 4. Merge predictions into full dataset\n",
        "print(\"[Step 4] Merging predictions into full dataset...\")\n",
        "full_df = pd.read_csv(FULL_DATA_PATH)\n",
        "pred_df = pd.DataFrame({\"comment_id\": ids, \"predicted_label\": y_pred})\n",
        "merged_df = full_df.merge(pred_df, on=\"comment_id\", how=\"left\")\n",
        "\n",
        "# 5. Save locally and to Drive\n",
        "print(\"[Step 5] Saving labeled dataset...\")\n",
        "\n",
        "LABELS_ENCODER = {\n",
        "    \"Pro-Palestine\": 0,\n",
        "    \"Pro-Israel\": 1,\n",
        "    \"Undefined\": 2\n",
        "}\n",
        "\n",
        "# Reverse the label encoder\n",
        "LABELS_DECODER = {v: k for k, v in LABELS_ENCODER.items()}\n",
        "\n",
        "# Map numeric predicted_label to text\n",
        "merged_df[\"predicted_label\"] = merged_df[\"predicted_label\"].map(LABELS_DECODER)\n",
        "\n",
        "# Save to CSV\n",
        "# Drop rows with NaN in predicted_label\n",
        "merged_df = merged_df.dropna(subset=[\"predicted_label\"])\n",
        "merged_df.to_csv(LOCAL_SAVE_PATH, index=False)\n",
        "\n",
        "print(f\"✅ Merged + labeled dataset saved locally to: {LOCAL_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 'predicted_label' column exists.\n",
            "\n",
            "Value counts:\n",
            "predicted_label\n",
            "Undefined        1835508\n",
            "Pro-Israel        440587\n",
            "Pro-Palestine     407135\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Column type:\n",
            "object\n"
          ]
        }
      ],
      "source": [
        "# Check if the column exists\n",
        "if \"predicted_label\" in merged_df.columns:\n",
        "    print(\"✅ 'predicted_label' column exists.\")\n",
        "\n",
        "    # Show basic info about the column\n",
        "    print(\"\\nValue counts:\")\n",
        "    print(merged_df[\"predicted_label\"].value_counts(dropna=False))\n",
        "\n",
        "    print(\"\\nColumn type:\")\n",
        "    print(merged_df[\"predicted_label\"].dtype)\n",
        "else:\n",
        "    print(\"❌ 'predicted_label' column is missing from merged_df.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>self_text</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Again though, his actions are forced by member...</td>\n",
              "      <td>Undefined</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The German police arresting a Jewish man for a...</td>\n",
              "      <td>Undefined</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We're already there. The killing just isn't en...</td>\n",
              "      <td>Pro-Palestine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thats not the definition of tokenism. That is ...</td>\n",
              "      <td>Pro-Palestine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I dont know if youre being serious but Im goin...</td>\n",
              "      <td>Pro-Palestine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           self_text predicted_label\n",
              "0  Again though, his actions are forced by member...       Undefined\n",
              "1  The German police arresting a Jewish man for a...       Undefined\n",
              "3  We're already there. The killing just isn't en...   Pro-Palestine\n",
              "4  Thats not the definition of tokenism. That is ...   Pro-Palestine\n",
              "5  I dont know if youre being serious but Im goin...   Pro-Palestine"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(merged_df[[\"self_text\", \"predicted_label\"]].head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
